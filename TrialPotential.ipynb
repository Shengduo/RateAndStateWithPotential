{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef0a63ca-e115-4af7-bb71-1ed80dda452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ï¼©mport necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import xitorch\n",
    "from xitorch.optimize import rootfinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8052a83-487a-4a75-8e8f-3c37ee64d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP for potentials\n",
    "class PP(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, NNs, input_dim = 1, output_dim = 1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, NNs[0]), \n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        for i in range(len(NNs) - 1):\n",
    "            self.fc.append(nn.Linear(NNs[i], NNs[i + 1]))\n",
    "            self.fc.append(nn.ELU())\n",
    "        \n",
    "        self.fc.append(nn.Linear(NNs[-1], output_dim))\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb5723-d9ad-4030-8d84-80c062bd6bb3",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b937e0a-4ac4-46b2-97e4-dbe3bf833408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from DataGeneration import generateSamples, genVVtt\n",
    "import os\n",
    "\n",
    "generating_flag = False\n",
    "kwgs = {\n",
    "    \"beta\" : [0.011, 0.016, 1. / 1.e1, 0.58], \n",
    "    \"totalNofSeqs\" : 1024 * 16, \n",
    "    \"NofIntervalsRange\" : [5, 11], \n",
    "    \"VVRange\" : [-10, 3], \n",
    "    \"VVLenRange\" : [8, 9], \n",
    "    \"theta0\" : 1., \n",
    "    \"prefix\" : \"Trial1003\", \n",
    "    \"NofVVSteps\" : 400, \n",
    "}\n",
    "\n",
    "# Generate / load data\n",
    "dataFile = \"./data/\" + kwgs[\"prefix\"] + \".pt\"\n",
    "\n",
    "if generating_flag or not(os.path.isfile(dataFile)):\n",
    "    print(\"Generating data\")\n",
    "    generateSamples(kwgs)\n",
    "\n",
    "shit = torch.load(dataFile)\n",
    "Vs = shit[\"Vs\"]\n",
    "thetas = shit[\"thetas\"]\n",
    "fs = shit[\"fs\"]\n",
    "\n",
    "# Stack data as\n",
    "Vs = torch.stack(Vs)[:1000, :500]\n",
    "thetas = torch.stack(thetas)[:1000, :500]\n",
    "fs = torch.stack(fs)[:1000, :500]\n",
    "ts = shit[\"ts\"][:1000, :500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "05c7667c-56dc-4c49-8163-8ea6c7f15afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs.shape:  torch.Size([1000, 500])\n",
      "thetas.shape:  torch.Size([1000, 500])\n",
      "fs.shape:  torch.Size([1000, 500])\n",
      "ts.shape:  torch.Size([1000, 500])\n"
     ]
    }
   ],
   "source": [
    "# Now Vs and ts have fixed length\n",
    "print(\"Vs.shape: \", Vs.shape)\n",
    "print(\"thetas.shape: \", thetas.shape)\n",
    "print(\"fs.shape: \", fs.shape)\n",
    "print(\"ts.shape: \", ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "440950e0-b282-4704-9118-4b775588700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs.shape:  torch.Size([1000, 500])\n"
     ]
    }
   ],
   "source": [
    "# Calculate Xs\n",
    "Xs = torch.zeros(Vs.shape)\n",
    "Xs[:, 1:] = torch.cumulative_trapezoid(Xs, ts)\n",
    "print(\"Xs.shape: \", Xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335115d-6483-4292-8f2a-ff9f42d2bdb6",
   "metadata": {},
   "source": [
    "# Defining NNs, for $W (V, \\xi)$ and $D (V, \\xi, \\dot{\\xi})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843efa3b-dc41-45d7-a4be-8264da84fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify dimension of xi\n",
    "dim_xi = 4\n",
    "\n",
    "# Specify NNs for W and D\n",
    "NNs_W = [128, 128]\n",
    "NNs_D = [256, 256]\n",
    "\n",
    "kwgsPot = {\n",
    "    \"dim_xi\" : dim_xi, \n",
    "    \"NNs_W\" : NNs_W, \n",
    "    \"NNs_D\" : NNs_D, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99e5c-55e4-420b-8ff7-0630e102610b",
   "metadata": {},
   "source": [
    "# Calculate $f = \\partial W / \\partial V$, $\\xi_{n+1}$ such that $\\partial D / \\partial \\dot{\\xi} + \\partial W / \\partial \\xi = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2a2c3a7e-5cb7-45fa-abaa-a20cd24a9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for training and calculating f\n",
    "# Optimizer Adams\n",
    "import torch.optim as optim\n",
    "\n",
    "class PotentialsFric:\n",
    "    # Initialization of W and D\n",
    "    def __init__(self, kwgsPot):\n",
    "        self.dim_xi = kwgsPot[\"dim_xi\"]\n",
    "        self.NNs_W = kwgsPot[\"NNs_W\"]\n",
    "        self.NNs_D = kwgsPot[\"NNs_D\"]\n",
    "        self.W = PP(NNs_W, input_dim = 1 + dim_xi, output_dim = 1)\n",
    "        # self.D = PP(NNs_D, input_dim = 1 + 2 * dim_xi, output_dim = 1)\n",
    "        self.optim_W = optim.Adam(self.W.parameters(), lr=0.001)\n",
    "        # self.optim_D = optim.Adam(self.D.parameters(), lr=0.001)\n",
    "    \n",
    "    # Calculate f \n",
    "    def calf(self, x, t):\n",
    "        # Initialize Vs\n",
    "        batch_size = x.shape[0]\n",
    "        time_steps = x.shape[1]\n",
    "        xi0 = torch.zeros([batch_size, self.dim_xi], requires_grad=True)\n",
    "        # xis[:, :, :] = 1. \n",
    "        \n",
    "        # List of fs\n",
    "        list_fs = []\n",
    "        list_xis = [xi0]\n",
    "        \n",
    "        # Loop through time steps\n",
    "        for idx in range(x.shape[1]):\n",
    "            # f = \\partial W / \\partial V\n",
    "            X_W = torch.concat([x[:, idx].reshape([-1, 1]), list_xis[-1]], dim = 1).requires_grad_()\n",
    "            W = torch.sum(self.W(X_W))\n",
    "            \n",
    "            this_piece = torch.autograd.grad(outputs=W, inputs=X_W, create_graph=True)[0]\n",
    "            list_fs.append(this_piece[:, 0:1])\n",
    "            \n",
    "            # Solve for \\dot{\\xi} + \\partial W / \\partial \\xi = 0\n",
    "            dWdXi = this_piece[:, 1:]\n",
    "            \n",
    "            # XiDot = -dWdXi\n",
    "            if idx < x.shape[1] - 1:\n",
    "                xiNext = list_xis[-1] - dWdXi * (t[:, idx + 1:idx + 2] - t[:, idx:idx + 1]) \n",
    "                list_xis.append(xiNext)\n",
    "        self.fs = torch.concat(list_fs, dim=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b8d8-d4d0-4647-8caa-fd4052416cff",
   "metadata": {},
   "source": [
    "# Define Loss function, training function, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cad54c66-4bbf-40d1-aa92-32527308a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions given fs_targ, fs. \n",
    "def Loss(fs_targ, fs, ts, p = 2):\n",
    "    return torch.sum(torch.pow(torch.trapz((fs_targ - fs) ** p, ts, dim = 1), 1. / p))\n",
    "\n",
    "# Training for one epoch\n",
    "def train1Epoch(data_loader, loss_fn, myPot):\n",
    "    # Record of losses for each batch\n",
    "    Losses = []\n",
    "    \n",
    "    # Enumerate over data_loader\n",
    "    for idx, (Vs, ts, fs_targ) in enumerate(data_loader):\n",
    "        # Refresh the optimizers\n",
    "        myPot.optim_W.zero_grad()\n",
    "        myPot.optim_D.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        myPot.calf(Vs, ts)\n",
    "        loss = loss_fn(fs_targ, myPot.fs, ts)\n",
    "        Losses.append(loss)\n",
    "        \n",
    "        # Update the model parameters\n",
    "        loss.backward()\n",
    "        myPot.optim_W.step()\n",
    "        \n",
    "        if hasattr(myPot, 'optim_D'):\n",
    "            myPot.optim_D.step()\n",
    "        \n",
    "    return sum(Losses) / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2b9cb476-1098-4e8e-9243-cd1b0b7caaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataloaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "AllData = TensorDataset(\n",
    "    Xs, \n",
    "    ts, \n",
    "    fs\n",
    ")\n",
    "\n",
    "train_len = int(len(Vs) * 0.8)\n",
    "test_len = len(Vs) - train_len\n",
    "trainDataset, testDataset = torch.utils.data.random_split(AllData, [train_len, test_len])\n",
    "\n",
    "# Training data loader\n",
    "training_batch_size = 64 #1024\n",
    "trainDataLoader = DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size = training_batch_size,\n",
    "    shuffle = True,\n",
    "#    num_workers = 16,\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    ")\n",
    "\n",
    "# Testing data loader\n",
    "testing_batch_size = 16 # 256\n",
    "testDataLoader = DataLoader(\n",
    "    testDataset,\n",
    "    batch_size = testing_batch_size,\n",
    "    shuffle = True,\n",
    "#    num_workers = 16,\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c2140542-20f0-4029-8003-8dd0e17c6002",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [187]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m myWD \u001b[38;5;241m=\u001b[39m PotentialsFric(kwgsPot)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain1Epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDataLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyWD\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [185]\u001b[0m, in \u001b[0;36mtrain1Epoch\u001b[0;34m(data_loader, loss_fn, myPot)\u001b[0m\n\u001b[1;32m     14\u001b[0m myPot\u001b[38;5;241m.\u001b[39moptim_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmyPot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(fs_targ, myPot\u001b[38;5;241m.\u001b[39mfs, ts)\n\u001b[1;32m     19\u001b[0m Losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Input \u001b[0;32mIn [184]\u001b[0m, in \u001b[0;36mPotentialsFric.calf\u001b[0;34m(self, V, t)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dWdXi \u001b[38;5;241m+\u001b[39m dDdXiDot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshitAssXiDot)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# that_shit = dWdXiPlusdDdXiDot(XiDot)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# print(\"That shit requires grad: \", that_shit.requires_grad)\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXiDot \u001b[38;5;241m=\u001b[39m \u001b[43mrootfinder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdWdXiPlusdDdXiDot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXiDot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# print(\"this_piece.requires_grad: \", this_piece.requires_grad)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(list_fs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xitorch/optimize/rootfinder.py:93\u001b[0m, in \u001b[0;36mrootfinder\u001b[0;34m(fcn, y0, params, bck_options, method, **fwd_options)\u001b[0m\n\u001b[1;32m     91\u001b[0m pfunc \u001b[38;5;241m=\u001b[39m get_pure_function(fcn)\n\u001b[1;32m     92\u001b[0m fwd_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_rootfinder_default_method(method)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_RootFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbck_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xitorch/optimize/rootfinder.py:302\u001b[0m, in \u001b[0;36m_RootFinder.forward\u001b[0;34m(ctx, fcn, y0, fwd_fcn, is_opt_method, options, bck_options, nparams, *allparams)\u001b[0m\n\u001b[1;32m    300\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrootfinder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_opt_method \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     method_fcn \u001b[38;5;241m=\u001b[39m get_method(name, methods, method)\n\u001b[0;32m--> 302\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_fcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfwd_fcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m ctx\u001b[38;5;241m.\u001b[39mfcn \u001b[38;5;241m=\u001b[39m fcn\n\u001b[1;32m    305\u001b[0m ctx\u001b[38;5;241m.\u001b[39mis_opt_method \u001b[38;5;241m=\u001b[39m is_opt_method\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xitorch/_impls/optimize/root/rootsolver.py:181\u001b[0m, in \u001b[0;36mbroyden1\u001b[0;34m(fcn, x0, params, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(_nonlin_solver, assigned\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m'\u001b[39m,))  \u001b[38;5;66;03m# takes only the signature\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroyden1\u001b[39m(fcn, x0, params\u001b[38;5;241m=\u001b[39m(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Solve the root finder or linear equation using the first Broyden method [1]_.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    It can be used to solve minimization by finding the root of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nonlin_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbroyden1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xitorch/_impls/optimize/root/rootsolver.py:92\u001b[0m, in \u001b[0;36m_nonlin_solver\u001b[0;34m(fcn, x0, params, method, alpha, uv0, max_rank, maxiter, f_tol, f_rtol, x_tol, x_rtol, line_search, verbose, **unused)\u001b[0m\n\u001b[1;32m     89\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: _ravel(fcn(_pack(x), \u001b[38;5;241m*\u001b[39mparams))\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m _ravel(x0)\n\u001b[0;32m---> 92\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m y_norm \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mnorm()\n\u001b[1;32m     94\u001b[0m stop_cond \u001b[38;5;241m=\u001b[39m TerminationCondition(f_tol, f_rtol, y_norm, x_tol, x_rtol)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xitorch/_impls/optimize/root/rootsolver.py:89\u001b[0m, in \u001b[0;36m_nonlin_solver.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# shorthand for the function\u001b[39;00m\n\u001b[1;32m     88\u001b[0m xshape \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 89\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: _ravel(\u001b[43mfcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_pack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m _ravel(x0)\n\u001b[1;32m     92\u001b[0m y \u001b[38;5;241m=\u001b[39m func(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xitorch/_core/pure_function.py:34\u001b[0m, in \u001b[0;36mPureFunction.__call__\u001b[0;34m(self, *params)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fcntocall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [184]\u001b[0m, in \u001b[0;36mPotentialsFric.calf.<locals>.dWdXiPlusdDdXiDot\u001b[0;34m(XiDot)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshitAssXiDot\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# XiDot.requires_grad=True\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Debug Line\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(\"dWdXiPlusdDdXiDot XiDot.requires_grad: \", XiDot.requires_grad)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dWdXi \u001b[38;5;241m+\u001b[39m \u001b[43mdDdXiDot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshitAssXiDot\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [184]\u001b[0m, in \u001b[0;36mPotentialsFric.calf.<locals>.dDdXiDot\u001b[0;34m(XiDot)\u001b[0m\n\u001b[1;32m     62\u001b[0m temp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([X_W, XiDot], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD(temp))\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m---> 64\u001b[0m this_piece \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(\"dDdXiDot this_piece.requires_grad: \", this_piece.requires_grad)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis_piece: \u001b[39m\u001b[38;5;124m\"\u001b[39m, this_piece)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "# Get a test case for potentials\n",
    "myWD = PotentialsFric(kwgsPot)\n",
    "\n",
    "# Train for one epoch\n",
    "train1Epoch(trainDataLoader, Loss, myWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "57cee3a8-c83c-466a-8995-7fd420c5d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myWD.shitAssXiDot.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0eebd-bf5c-46fc-86e5-19ee7424db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros([5,5])\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c184242-9b52-4504-94f3-681077781dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0361, 0.0361, 0.0361,  ..., 0.0359, 0.0359, 0.0359],\n",
       "        [0.0338, 0.0338, 0.0338,  ..., 0.0362, 0.0362, 0.0362],\n",
       "        [0.0362, 0.0362, 0.0362,  ..., 0.0362, 0.0362, 0.0362],\n",
       "        ...,\n",
       "        [0.0362, 0.0362, 0.0362,  ..., 0.0362, 0.0362, 0.0362],\n",
       "        [0.0362, 0.0362, 0.0362,  ..., 0.0362, 0.0362, 0.0362],\n",
       "        [0.0362, 0.0362, 0.0362,  ..., 0.0362, 0.0362, 0.0362]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWD.fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fa1989a-4eb7-42f7-b3e5-5d26bdbf64fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 4000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWD.fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e09309-d069-4d1a-be9a-f5927980f267",
   "metadata": {},
   "source": [
    "# Demo: Try taking gradients w.r.t. inputs, this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bdab64-0708-4afc-8040-2a937b9c28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct my NN with initialized default parameters \n",
    "NNs = [16, 16]\n",
    "input_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "myPP = PP(NNs, input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "070c9e16-b938-4d6d-af2b-5f7229866fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor(-0.1882, grad_fn=<SumBackward0>)\n",
      "dydx:  tensor([[ 0.0650, -0.0497, -0.0758, -0.0635]])\n"
     ]
    }
   ],
   "source": [
    "# Try taking gradients w.r.t. the inputs\n",
    "x = torch.tensor([[1., 2., -1., 3.]], requires_grad=True)\n",
    "y = torch.sum(myPP(x))\n",
    "dydx = torch.autograd.grad(outputs=y, inputs=x, retain_graph=True)[0]\n",
    "\n",
    "# Show values and gradients\n",
    "print(\"y: \", y)\n",
    "print(\"dydx: \", dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c58661-929b-499c-bf23-4b4151906779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor(-0.3059, grad_fn=<SumBackward0>)\n",
      "dydx:  tensor([[ 0.0650, -0.0497, -0.0758, -0.0635],\n",
      "        [ 0.0623, -0.0945, -0.0193, -0.0369]])\n"
     ]
    }
   ],
   "source": [
    "# Try taking gradients w.r.t. the inputs\n",
    "x = torch.tensor([[1., 2., -1., 3.], [-1., -3., 2., 5.]], requires_grad=True)\n",
    "y = torch.sum(myPP(x))\n",
    "dydx = torch.autograd.grad(outputs=y, inputs=x, retain_graph=True)[0]\n",
    "\n",
    "# Show values and gradients\n",
    "print(\"y: \", y)\n",
    "print(\"dydx: \", dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec0d169-8be9-4adb-8abe-a3eeb98df35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0650, -0.0497, -0.0758, -0.0635],\n",
       "        [ 0.0623, -0.0945, -0.0193, -0.0369]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dydx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "496545f8-039c-4cb8-be32-7bac4cc1f2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3059, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a95a-0ce3-4031-9f61-5400e0d1a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
