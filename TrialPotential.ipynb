{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0a63ca-e115-4af7-bb71-1ed80dda452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ï¼©mport necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8052a83-487a-4a75-8e8f-3c37ee64d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP for potentials\n",
    "class PP(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, NNs, input_dim = 1, output_dim = 1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, NNs[0]), \n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        for i in range(len(NNs) - 1):\n",
    "            self.fc.append(nn.Linear(NNs[i], NNs[i + 1]))\n",
    "            self.fc.append(nn.ELU())\n",
    "        \n",
    "        self.fc.append(nn.Linear(NNs[-1], output_dim))\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb5723-d9ad-4030-8d84-80c062bd6bb3",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b937e0a-4ac4-46b2-97e4-dbe3bf833408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataGeneration import generateSamples, genVVtt\n",
    "import os\n",
    "\n",
    "generating_flag = False\n",
    "kwgs = {\n",
    "    \"beta\" : [0.011, 0.016, 1. / 1.e1, 0.58], \n",
    "    \"totalNofSeqs\" : 1024 * 16, \n",
    "    \"NofIntervalsRange\" : [5, 11], \n",
    "    \"VVRange\" : [-10, 3], \n",
    "    \"VVLenRange\" : [8, 9], \n",
    "    \"theta0\" : 1., \n",
    "    \"prefix\" : \"Trial1003\", \n",
    "    \"NofVVSteps\" : 400, \n",
    "}\n",
    "\n",
    "# Generate / load data\n",
    "dataFile = \"./data/\" + kwgs[\"prefix\"] + \".pt\"\n",
    "\n",
    "if generating_flag or not(os.path.isfile(dataFile)):\n",
    "    print(\"Generating data\")\n",
    "    generateSamples(kwgs)\n",
    "\n",
    "shit = torch.load(dataFile)\n",
    "Vs = shit[\"Vs\"]\n",
    "thetas = shit[\"thetas\"]\n",
    "fs = shit[\"fs\"]\n",
    "\n",
    "# Stack data as\n",
    "Vs = torch.stack(Vs)\n",
    "thetas = torch.stack(thetas)\n",
    "fs = torch.stack(fs)\n",
    "ts = shit[\"ts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05c7667c-56dc-4c49-8163-8ea6c7f15afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs.shape:  torch.Size([16384, 4000])\n",
      "thetas.shape:  torch.Size([16384, 4000])\n",
      "fs.shape:  torch.Size([16384, 4000])\n",
      "ts.shape:  torch.Size([16384, 4000])\n"
     ]
    }
   ],
   "source": [
    "# Now Vs and ts have fixed length\n",
    "print(\"Vs.shape: \", Vs.shape)\n",
    "print(\"thetas.shape: \", thetas.shape)\n",
    "print(\"fs.shape: \", fs.shape)\n",
    "print(\"ts.shape: \", ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335115d-6483-4292-8f2a-ff9f42d2bdb6",
   "metadata": {},
   "source": [
    "# Defining NNs, for $W (V, \\xi)$ and $D (V, \\xi, \\dot{\\xi})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843efa3b-dc41-45d7-a4be-8264da84fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify dimension of xi\n",
    "dim_xi = 4\n",
    "\n",
    "# Specify NNs for W and D\n",
    "NNs_W = [128, 128]\n",
    "NNs_D = [256, 256]\n",
    "\n",
    "kwgsPot = {\n",
    "    \"dim_xi\" : dim_xi, \n",
    "    \"NNs_W\" : NNs_W, \n",
    "    \"NNs_D\" : NNs_D, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99e5c-55e4-420b-8ff7-0630e102610b",
   "metadata": {},
   "source": [
    "# Calculate $f = \\partial W / \\partial V$, $\\xi_{n+1}$ such that $\\partial D / \\partial \\dot{\\xi} + \\partial W / \\partial \\xi = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a2c3a7e-5cb7-45fa-abaa-a20cd24a9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for training and calculating f\n",
    "# Optimizer Adams\n",
    "import torch.optim as optim\n",
    "\n",
    "class PotentialsFric:\n",
    "    # Initialization of W and D\n",
    "    def __init__(self, kwgsPot):\n",
    "        self.dim_xi = kwgsPot[\"dim_xi\"]\n",
    "        self.NNs_W = kwgsPot[\"NNs_W\"]\n",
    "        self.NNs_D = kwgsPot[\"NNs_D\"]\n",
    "        self.W = PP(NNs_W, input_dim = 1 + dim_xi, output_dim = 1)\n",
    "        self.D = PP(NNs_D, input_dim = 1 + 2 * dim_xi, output_dim = 1)\n",
    "        self.optim_W = optim.Adam(self.W.parameters(), lr=0.001)\n",
    "        self.optim_D = optim.Adam(self.D.parameters(), lr=0.001)\n",
    "    \n",
    "    # Calculate f \n",
    "    def calf(self, V, t):\n",
    "        # Initialize Vs\n",
    "        batch_size = V.shape[0]\n",
    "        time_steps = V.shape[1]\n",
    "        xis = torch.zeros([batch_size, self.dim_xi, time_steps])\n",
    "        xis[:, :, :] = 1. \n",
    "                           \n",
    "        self.fs = torch.zeros(V.shape)\n",
    "                           \n",
    "        # Loop through time steps\n",
    "        for idx in range(V.shape[1]):\n",
    "            X_W = torch.concat([V[:, idx].reshape([-1, 1]), xis[:, :, idx]], dim = 1).clone().detach().requires_grad_(True)\n",
    "            W = torch.sum(self.W(X_W))\n",
    "            self.fs[:, idx] = torch.autograd.grad(outputs=W, inputs=X_W, retain_graph=True)[0][:, 0]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b8d8-d4d0-4647-8caa-fd4052416cff",
   "metadata": {},
   "source": [
    "# Define Loss function, training function, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cad54c66-4bbf-40d1-aa92-32527308a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions given fs_targ, fs. \n",
    "def Loss(fs_targ, fs, ts, p = 2):\n",
    "    return torch.sum(torch.pow(torch.trapz((fs_targ - fs) ** p, ts, dim = 1), 1. / p))\n",
    "\n",
    "# Training for one epoch\n",
    "def train1Epoch(data_loader, loss_fn, myPot):\n",
    "    # Record of losses for each batch\n",
    "    Losses = []\n",
    "    \n",
    "    # Enumerate over data_loader\n",
    "    for idx, (Vs, ts, fs_targ) in enumerate(data_loader):\n",
    "        # Refresh the optimizers\n",
    "        myPot.optim_W.zero_grad()\n",
    "        myPot.optim_D.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        myPot.calf(Vs, ts)\n",
    "        loss = loss_fn(fs_targ, myPot.fs, ts)\n",
    "        Losses.append(loss)\n",
    "        \n",
    "        # Update the model parameters\n",
    "        loss.backward()\n",
    "        myPot.optim_W.step()\n",
    "        myPot.optim_D.step()\n",
    "        \n",
    "    return sum(Losses) / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b9cb476-1098-4e8e-9243-cd1b0b7caaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataloaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "AllData = TensorDataset(\n",
    "    Vs, \n",
    "    ts, \n",
    "    fs\n",
    ")\n",
    "\n",
    "train_len = int(len(Vs) * 0.8)\n",
    "test_len = len(Vs) - train_len\n",
    "trainDataset, testDataset = torch.utils.data.random_split(AllData, [train_len, test_len])\n",
    "\n",
    "# Training data loader\n",
    "training_batch_size = 1024\n",
    "trainDataLoader = DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size = training_batch_size,\n",
    "    shuffle = True,\n",
    "#    num_workers = 16,\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    ")\n",
    "\n",
    "# Testing data loader\n",
    "testing_batch_size = 256\n",
    "testDataLoader = DataLoader(\n",
    "    testDataset,\n",
    "    batch_size = testing_batch_size,\n",
    "    shuffle = True,\n",
    "#    num_workers = 16,\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2140542-20f0-4029-8003-8dd0e17c6002",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m myWD \u001b[38;5;241m=\u001b[39m PotentialsFric(kwgsPot)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain1Epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDataLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyWD\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mtrain1Epoch\u001b[0;34m(data_loader, loss_fn, myPot)\u001b[0m\n\u001b[1;32m     19\u001b[0m Losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Update the model parameters\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m myPot\u001b[38;5;241m.\u001b[39moptim_W\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m myPot\u001b[38;5;241m.\u001b[39moptim_D\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Get a test case for potentials\n",
    "myWD = PotentialsFric(kwgsPot)\n",
    "\n",
    "# Train for one epoch\n",
    "train1Epoch(trainDataLoader, Loss, myWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fa1989a-4eb7-42f7-b3e5-5d26bdbf64fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 4000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWD.fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e09309-d069-4d1a-be9a-f5927980f267",
   "metadata": {},
   "source": [
    "# Demo: Try taking gradients w.r.t. inputs, this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bdab64-0708-4afc-8040-2a937b9c28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct my NN with initialized default parameters \n",
    "NNs = [16, 16]\n",
    "input_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "myPP = PP(NNs, input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "070c9e16-b938-4d6d-af2b-5f7229866fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor(-0.1882, grad_fn=<SumBackward0>)\n",
      "dydx:  tensor([[ 0.0650, -0.0497, -0.0758, -0.0635]])\n"
     ]
    }
   ],
   "source": [
    "# Try taking gradients w.r.t. the inputs\n",
    "x = torch.tensor([[1., 2., -1., 3.]], requires_grad=True)\n",
    "y = torch.sum(myPP(x))\n",
    "dydx = torch.autograd.grad(outputs=y, inputs=x, retain_graph=True)[0]\n",
    "\n",
    "# Show values and gradients\n",
    "print(\"y: \", y)\n",
    "print(\"dydx: \", dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c58661-929b-499c-bf23-4b4151906779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor(-0.3059, grad_fn=<SumBackward0>)\n",
      "dydx:  tensor([[ 0.0650, -0.0497, -0.0758, -0.0635],\n",
      "        [ 0.0623, -0.0945, -0.0193, -0.0369]])\n"
     ]
    }
   ],
   "source": [
    "# Try taking gradients w.r.t. the inputs\n",
    "x = torch.tensor([[1., 2., -1., 3.], [-1., -3., 2., 5.]], requires_grad=True)\n",
    "y = torch.sum(myPP(x))\n",
    "dydx = torch.autograd.grad(outputs=y, inputs=x, retain_graph=True)[0]\n",
    "\n",
    "# Show values and gradients\n",
    "print(\"y: \", y)\n",
    "print(\"dydx: \", dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec0d169-8be9-4adb-8abe-a3eeb98df35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0650, -0.0497, -0.0758, -0.0635],\n",
       "        [ 0.0623, -0.0945, -0.0193, -0.0369]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dydx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "496545f8-039c-4cb8-be32-7bac4cc1f2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3059, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a95a-0ce3-4031-9f61-5400e0d1a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
