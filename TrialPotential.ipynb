{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ef0a63ca-e115-4af7-bb71-1ed80dda452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available:  True\n",
      "Device is:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "#ï¼©mport necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import xitorch\n",
    "from xitorch.optimize import rootfinder\n",
    "\n",
    "# Testify whether GPU is available\n",
    "print(\"Cuda is available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b8052a83-487a-4a75-8e8f-3c37ee64d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP for potentials\n",
    "class PP(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, NNs, input_dim = 1, output_dim = 1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, NNs[0]), \n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        for i in range(len(NNs) - 1):\n",
    "            self.fc.append(nn.Linear(NNs[i], NNs[i + 1]))\n",
    "            self.fc.append(nn.ELU())\n",
    "        \n",
    "        self.fc.append(nn.Linear(NNs[-1], output_dim))\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb5723-d9ad-4030-8d84-80c062bd6bb3",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0b937e0a-4ac4-46b2-97e4-dbe3bf833408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataGeneration import generateSamples, genVVtt\n",
    "import os\n",
    "\n",
    "generating_flag = False\n",
    "kwgs = {\n",
    "    \"beta\" : [0.011, 0.016, 1. / 1.e1, 0.58], \n",
    "    \"totalNofSeqs\" : 1024 * 16, \n",
    "    \"NofIntervalsRange\" : [5, 11], \n",
    "    \"VVRange\" : [-10, 3], \n",
    "    \"VVLenRange\" : [8, 9], \n",
    "    \"theta0\" : 1., \n",
    "    \"prefix\" : \"Trial1003\", \n",
    "    \"NofVVSteps\" : 400, \n",
    "}\n",
    "\n",
    "# Generate / load data\n",
    "dataFile = \"./data/\" + kwgs[\"prefix\"] + \".pt\"\n",
    "\n",
    "if generating_flag or not(os.path.isfile(dataFile)):\n",
    "    print(\"Generating data\")\n",
    "    generateSamples(kwgs)\n",
    "\n",
    "shit = torch.load(dataFile)\n",
    "Vs = shit[\"Vs\"]\n",
    "thetas = shit[\"thetas\"]\n",
    "fs = shit[\"fs\"]\n",
    "\n",
    "# Stack data as\n",
    "Vs = torch.stack(Vs)[:1000, :500]\n",
    "thetas = torch.stack(thetas)[:1000, :500]\n",
    "fs = torch.stack(fs)[:1000, :500]\n",
    "ts = shit[\"ts\"][:1000, :500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "05c7667c-56dc-4c49-8163-8ea6c7f15afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs.shape:  torch.Size([1000, 500])\n",
      "thetas.shape:  torch.Size([1000, 500])\n",
      "fs.shape:  torch.Size([1000, 500])\n",
      "ts.shape:  torch.Size([1000, 500])\n"
     ]
    }
   ],
   "source": [
    "# Now Vs and ts have fixed length\n",
    "print(\"Vs.shape: \", Vs.shape)\n",
    "print(\"thetas.shape: \", thetas.shape)\n",
    "print(\"fs.shape: \", fs.shape)\n",
    "print(\"ts.shape: \", ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "440950e0-b282-4704-9118-4b775588700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs.shape:  torch.Size([1000, 500])\n"
     ]
    }
   ],
   "source": [
    "# Calculate Xs\n",
    "Xs = torch.zeros(Vs.shape)\n",
    "Xs[:, 1:] = torch.cumulative_trapezoid(Xs, ts)\n",
    "print(\"Xs.shape: \", Xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335115d-6483-4292-8f2a-ff9f42d2bdb6",
   "metadata": {},
   "source": [
    "# Defining NNs, for $W (V, \\xi)$ and $D (V, \\xi, \\dot{\\xi})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "843efa3b-dc41-45d7-a4be-8264da84fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify dimension of xi\n",
    "dim_xi = 4\n",
    "\n",
    "# Specify NNs for W and D\n",
    "NNs_W = [128, 128]\n",
    "NNs_D = [256, 256]\n",
    "\n",
    "kwgsPot = {\n",
    "    \"dim_xi\" : dim_xi, \n",
    "    \"NNs_W\" : NNs_W, \n",
    "    \"NNs_D\" : NNs_D, \n",
    "    \"device\" : device, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99e5c-55e4-420b-8ff7-0630e102610b",
   "metadata": {},
   "source": [
    "# Calculate $f = \\partial W / \\partial V$, $\\xi_{n+1}$ such that $\\partial D / \\partial \\dot{\\xi} + \\partial W / \\partial \\xi = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2a2c3a7e-5cb7-45fa-abaa-a20cd24a9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for training and calculating f\n",
    "# Optimizer Adams\n",
    "import torch.optim as optim\n",
    "\n",
    "class PotentialsFric:\n",
    "    # Initialization of W and D\n",
    "    def __init__(self, kwgsPot):\n",
    "        self.dim_xi = kwgsPot[\"dim_xi\"]\n",
    "        self.NNs_W = kwgsPot[\"NNs_W\"]\n",
    "        self.NNs_D = kwgsPot[\"NNs_D\"]\n",
    "        self.W = PP(NNs_W, input_dim = 1 + dim_xi, output_dim = 1)\n",
    "        # self.D = PP(NNs_D, input_dim = 1 + 2 * dim_xi, output_dim = 1)\n",
    "        self.optim_W = optim.Adam(self.W.parameters(), lr=0.001)\n",
    "        # self.optim_D = optim.Adam(self.D.parameters(), lr=0.001)\n",
    "        \n",
    "        # Device\n",
    "        self.device = kwgsPot[\"device\"]\n",
    "        self.W.to(self.device)\n",
    "        \n",
    "    # Calculate f \n",
    "    def calf(self, x, t):\n",
    "        # Initialize Vs\n",
    "        batch_size = x.shape[0]\n",
    "        time_steps = x.shape[1]\n",
    "        xi0 = torch.zeros([batch_size, self.dim_xi], requires_grad=True, device=self.device)\n",
    "        # xis[:, :, :] = 1. \n",
    "        \n",
    "        # List of fs\n",
    "        list_fs = []\n",
    "        list_xis = [xi0]\n",
    "        \n",
    "        # Loop through time steps\n",
    "        for idx in range(x.shape[1]):\n",
    "            # f = \\partial W / \\partial V\n",
    "            X_W = torch.concat([x[:, idx:idx + 1], list_xis[-1]], dim = 1).requires_grad_()\n",
    "            # X_W.to(self.device)\n",
    "            W = torch.sum(self.W(X_W))\n",
    "            \n",
    "            this_piece = torch.autograd.grad(outputs=W, inputs=X_W, create_graph=True)[0]\n",
    "            list_fs.append(this_piece[:, 0:1])\n",
    "            \n",
    "            # Solve for \\dot{\\xi} + \\partial W / \\partial \\xi = 0\n",
    "            dWdXi = this_piece[:, 1:]\n",
    "            \n",
    "            # XiDot = -dWdXi\n",
    "            if idx < x.shape[1] - 1:\n",
    "                xiNext = list_xis[-1] - dWdXi * (t[:, idx + 1:idx + 2] - t[:, idx:idx + 1]) \n",
    "                list_xis.append(xiNext)\n",
    "        self.fs = torch.concat(list_fs, dim=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b8d8-d4d0-4647-8caa-fd4052416cff",
   "metadata": {},
   "source": [
    "# Define Loss function, training function, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "cad54c66-4bbf-40d1-aa92-32527308a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions given fs_targ, fs. \n",
    "def Loss(fs_targ, fs, ts, p = 2):\n",
    "    return torch.sum(torch.pow(torch.trapz((fs_targ - fs) ** p, ts, dim = 1), 1. / p))\n",
    "\n",
    "# Training for one epoch\n",
    "def train1Epoch(data_loader, loss_fn, myPot):\n",
    "    # Record of losses for each batch\n",
    "    Losses = []\n",
    "    device=myPot.device\n",
    "    \n",
    "    # Enumerate over data_loader\n",
    "    for idx, (Xs, ts, fs_targ) in enumerate(data_loader):\n",
    "        # Send shits to GPU\n",
    "        Xs = Xs.to(device)\n",
    "        ts = ts.to(device)\n",
    "        fs_targ = fs_targ.to(device)\n",
    "        \n",
    "        # Refresh the optimizers\n",
    "        myPot.optim_W.zero_grad()\n",
    "        \n",
    "        if hasattr(myPot, 'optim_D'):\n",
    "            myPot.optim_D.zero_grad()\n",
    "        \n",
    "        ## DEBUG LINE CHECK DEVICES\n",
    "        # print(\"Xs.device: \", Xs.device)\n",
    "        # print(\"Xs[:, 0:1].device: \", Xs[:, 0:1].device)\n",
    "        \n",
    "        # Compute loss\n",
    "        myPot.calf(Xs, ts)\n",
    "        loss = loss_fn(fs_targ, myPot.fs, ts)\n",
    "        Losses.append(loss)\n",
    "        \n",
    "        # Update the model parameters\n",
    "        loss.backward()\n",
    "        myPot.optim_W.step()\n",
    "        \n",
    "        if hasattr(myPot, 'optim_D'):\n",
    "            myPot.optim_D.step()\n",
    "        \n",
    "    return sum(Losses) / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2b9cb476-1098-4e8e-9243-cd1b0b7caaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataloaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "AllData = TensorDataset(\n",
    "    Xs, \n",
    "    ts, \n",
    "    fs\n",
    ")\n",
    "\n",
    "dataloader_kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_len = int(len(Vs) * 0.8)\n",
    "test_len = len(Vs) - train_len\n",
    "trainDataset, testDataset = torch.utils.data.random_split(AllData, [train_len, test_len])\n",
    "\n",
    "# Training data loader\n",
    "training_batch_size = 64 #1024\n",
    "trainDataLoader = DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size = training_batch_size,\n",
    "    shuffle = True,\n",
    "#    num_workers = 16,\n",
    "    collate_fn = None,\n",
    "    **dataloader_kwargs, \n",
    ")\n",
    "\n",
    "# Testing data loader\n",
    "testing_batch_size = 16 # 256\n",
    "testDataLoader = DataLoader(\n",
    "    testDataset,\n",
    "    batch_size = testing_batch_size,\n",
    "    shuffle = True,\n",
    "#    num_workers = 16,\n",
    "    collate_fn = None,\n",
    "    **dataloader_kwargs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c2140542-20f0-4029-8003-8dd0e17c6002",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shit is:  tensor(0.5297, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "shit is:  tensor(0.2945, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get a test case for potentials\n",
    "myWD = PotentialsFric(kwgsPot)\n",
    "\n",
    "# Train for one epoch\n",
    "for i in range(2):\n",
    "    shit = train1Epoch(trainDataLoader, Loss, myWD)\n",
    "    print(\"shit is: \", shit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3b34e1cb-5e94-44bc-b69a-4ba6988fea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "68834829-31f8-4756-891d-7280fa433eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testify whether GPU is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e09309-d069-4d1a-be9a-f5927980f267",
   "metadata": {},
   "source": [
    "# Demo: Try taking gradients w.r.t. inputs, this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bdab64-0708-4afc-8040-2a937b9c28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct my NN with initialized default parameters \n",
    "NNs = [16, 16]\n",
    "input_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "myPP = PP(NNs, input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "070c9e16-b938-4d6d-af2b-5f7229866fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor(-0.1882, grad_fn=<SumBackward0>)\n",
      "dydx:  tensor([[ 0.0650, -0.0497, -0.0758, -0.0635]])\n"
     ]
    }
   ],
   "source": [
    "# Try taking gradients w.r.t. the inputs\n",
    "x = torch.tensor([[1., 2., -1., 3.]], requires_grad=True)\n",
    "y = torch.sum(myPP(x))\n",
    "dydx = torch.autograd.grad(outputs=y, inputs=x, retain_graph=True)[0]\n",
    "\n",
    "# Show values and gradients\n",
    "print(\"y: \", y)\n",
    "print(\"dydx: \", dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c58661-929b-499c-bf23-4b4151906779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor(-0.3059, grad_fn=<SumBackward0>)\n",
      "dydx:  tensor([[ 0.0650, -0.0497, -0.0758, -0.0635],\n",
      "        [ 0.0623, -0.0945, -0.0193, -0.0369]])\n"
     ]
    }
   ],
   "source": [
    "# Try taking gradients w.r.t. the inputs\n",
    "x = torch.tensor([[1., 2., -1., 3.], [-1., -3., 2., 5.]], requires_grad=True)\n",
    "y = torch.sum(myPP(x))\n",
    "dydx = torch.autograd.grad(outputs=y, inputs=x, retain_graph=True)[0]\n",
    "\n",
    "# Show values and gradients\n",
    "print(\"y: \", y)\n",
    "print(\"dydx: \", dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec0d169-8be9-4adb-8abe-a3eeb98df35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0650, -0.0497, -0.0758, -0.0635],\n",
       "        [ 0.0623, -0.0945, -0.0193, -0.0369]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dydx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "496545f8-039c-4cb8-be32-7bac4cc1f2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3059, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a95a-0ce3-4031-9f61-5400e0d1a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
