/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[I 2024-03-11 15:40:41,162] Using an existing study with name 'my_study1' instead of creating a new one.
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_smallDRS_smallA_400.pt
Vs.shape:  torch.Size([400, 100])
thetas.shape:  torch.Size([400, 100])
fs.shape:  torch.Size([400, 100])
ts.shape:  torch.Size([400, 100])
Xs.shape:  torch.Size([400, 100])
No pruned database has been founded.
--------------------  Trial  5   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.993615262389122, 'log_learning_rate_D': -3.0178538113971602, 'log_learning_rate_D_dagger': -1.424912001071907, 'training_batch_size': 11, 'training_p': 8}
	 epoch  0 training error:  tensor(417.1308, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  10 training error:  tensor(110.4253, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  20 training error:  tensor(4.3152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.2639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(2.9496, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  50 training error:  tensor(1.4664, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  60 training error:  tensor(1.4236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.9912, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.8619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.7348, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.3994140625
Memory cached:  6.0
[I 2024-03-11 15:42:21,170] Trial 5 finished with value: 0.7055261731147766 and parameters: {'log_learning_rate': -3.993615262389122, 'log_learning_rate_D': -3.0178538113971602, 'log_learning_rate_D_dagger': -1.424912001071907, 'training_batch_size': 11, 'training_p': 8}. Best is trial 5 with value: 0.7055261731147766.
res:  tensor(0.7055, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  99.76649904251099
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  6   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.6032084277497662, 'log_learning_rate_D': -2.9725587977350663, 'log_learning_rate_D_dagger': -3.1423020397583046, 'training_batch_size': 12, 'training_p': 5}
	 epoch  0 training error:  tensor(61.6345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  10 training error:  tensor(56.6756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(51.7989, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(47.0119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(42.3179, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(37.7163, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(33.2042, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(28.7768, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(24.4284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(20.1522, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-11 15:44:01,961] Trial 6 finished with value: 10.433277130126953 and parameters: {'log_learning_rate': -3.6032084277497662, 'log_learning_rate_D': -2.9725587977350663, 'log_learning_rate_D_dagger': -3.1423020397583046, 'training_batch_size': 12, 'training_p': 5}. Best is trial 5 with value: 0.7055261731147766.
Time for this trial:  100.60220694541931
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  7   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.3814791705071965, 'log_learning_rate_D': -2.2519402804356528, 'log_learning_rate_D_dagger': -4.914276909530178, 'training_batch_size': 7, 'training_p': 3}
	 epoch  0 training error:  tensor(110.1376, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(109.9405, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(109.7397, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(109.5380, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(109.3376, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(109.1360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(108.9339, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(108.7334, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(108.5311, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(108.3304, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 15:48:36,723] Trial 7 finished with value: 76.702880859375 and parameters: {'log_learning_rate': -4.3814791705071965, 'log_learning_rate_D': -2.2519402804356528, 'log_learning_rate_D_dagger': -4.914276909530178, 'training_batch_size': 7, 'training_p': 3}. Best is trial 5 with value: 0.7055261731147766.
Time for this trial:  274.5438632965088
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  8   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.9822486628392912, 'log_learning_rate_D': -2.479970679259301, 'log_learning_rate_D_dagger': -4.379485043082489, 'training_batch_size': 11, 'training_p': 2}
	 epoch  0 training error:  tensor(75.6680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  10 training error:  tensor(75.4677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(75.2675, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(75.0676, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(74.8680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(74.6686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(74.4695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(74.2707, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(74.0722, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(73.8739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-11 15:50:18,693] Trial 8 finished with value: 66.53350067138672 and parameters: {'log_learning_rate': -2.9822486628392912, 'log_learning_rate_D': -2.479970679259301, 'log_learning_rate_D_dagger': -4.379485043082489, 'training_batch_size': 11, 'training_p': 2}. Best is trial 5 with value: 0.7055261731147766.
Time for this trial:  101.77692866325378
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  9   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.539411352199628, 'log_learning_rate_D': -4.804863820970217, 'log_learning_rate_D_dagger': -4.00162185520995, 'training_batch_size': 10, 'training_p': 6}
	 epoch  0 training error:  tensor(222.9628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  10 training error:  tensor(221.8929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(220.8252, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(219.7619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(218.7100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(217.6861, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(216.7070, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(215.7711, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(214.8670, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(213.9859, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-11 15:52:00,741] Trial 9 finished with value: 125.29304504394531 and parameters: {'log_learning_rate': -1.539411352199628, 'log_learning_rate_D': -4.804863820970217, 'log_learning_rate_D_dagger': -4.00162185520995, 'training_batch_size': 10, 'training_p': 6}. Best is trial 5 with value: 0.7055261731147766.
Time for this trial:  101.84232330322266
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  10   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.838183823299353, 'log_learning_rate_D': -4.337167162507627, 'log_learning_rate_D_dagger': -4.832860088263093, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(155.7326, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(154.5678, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(153.4177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(152.3650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(151.4984, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(150.7577, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(150.0809, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(149.4451, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(148.8217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(148.2029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 15:59:30,991] Trial 10 finished with value: 91.22071075439453 and parameters: {'log_learning_rate': -1.838183823299353, 'log_learning_rate_D': -4.337167162507627, 'log_learning_rate_D_dagger': -4.832860088263093, 'training_batch_size': 6, 'training_p': 5}. Best is trial 5 with value: 0.7055261731147766.
Time for this trial:  450.0279428958893
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  11   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8201583027026569, 'log_learning_rate_D': -3.94569135116793, 'log_learning_rate_D_dagger': -2.4052766960014584, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(8.8775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7661, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8043, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5747, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5535, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5155, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:02:41,037] Trial 11 finished with value: 0.46003055572509766 and parameters: {'log_learning_rate': -1.8201583027026569, 'log_learning_rate_D': -3.94569135116793, 'log_learning_rate_D_dagger': -2.4052766960014584, 'training_batch_size': 8, 'training_p': 3}. Best is trial 11 with value: 0.46003055572509766.
res:  tensor(0.4600, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.7055, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  189.83395552635193
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  12   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.41947521692401, 'log_learning_rate_D': -3.8088109694297194, 'log_learning_rate_D_dagger': -3.3199546269615245, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(48.7781, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(47.3242, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(45.8954, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(44.4941, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(43.1220, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(41.7799, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(40.4679, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(39.1858, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(37.9334, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  90 training error:  tensor(36.7101, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-11 16:04:24,807] Trial 12 finished with value: 31.676660537719727 and parameters: {'log_learning_rate': -4.41947521692401, 'log_learning_rate_D': -3.8088109694297194, 'log_learning_rate_D_dagger': -3.3199546269615245, 'training_batch_size': 9, 'training_p': 2}. Best is trial 11 with value: 0.46003055572509766.
Time for this trial:  103.56575417518616
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  13   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2634421050497067, 'log_learning_rate_D': -4.23728207811169, 'log_learning_rate_D_dagger': -2.0481981157665965, 'training_batch_size': 10, 'training_p': 5}
	 epoch  0 training error:  tensor(143.9256, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(93.1700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(55.1207, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(29.7906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(14.4211, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(6.1086, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(2.0540, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.8586, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.8211, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.6041, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-11 16:06:08,915] Trial 13 finished with value: 0.5509872436523438 and parameters: {'log_learning_rate': -1.2634421050497067, 'log_learning_rate_D': -4.23728207811169, 'log_learning_rate_D_dagger': -2.0481981157665965, 'training_batch_size': 10, 'training_p': 5}. Best is trial 11 with value: 0.46003055572509766.
Time for this trial:  103.89694261550903
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  14   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.585491401571747, 'log_learning_rate_D': -3.3280838605364127, 'log_learning_rate_D_dagger': -3.656976963065609, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(51.1892, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(45.8832, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(40.8133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(36.0025, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(31.3906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(26.9966, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(22.7087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(18.6500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(14.7505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(11.0146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 16:13:41,326] Trial 14 finished with value: 5.708029270172119 and parameters: {'log_learning_rate': -2.585491401571747, 'log_learning_rate_D': -3.3280838605364127, 'log_learning_rate_D_dagger': -3.656976963065609, 'training_batch_size': 6, 'training_p': 6}. Best is trial 11 with value: 0.46003055572509766.
Time for this trial:  452.1668984889984
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  15   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.187068991856652, 'log_learning_rate_D': -1.1827893295810785, 'log_learning_rate_D_dagger': -2.492255019973957, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(215.7519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(177.3168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(143.9854, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(115.8465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(92.0301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(72.2623, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(55.9258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(42.7190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(32.0398, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(23.5984, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:16:51,608] Trial 15 finished with value: 12.247414588928223 and parameters: {'log_learning_rate': -2.187068991856652, 'log_learning_rate_D': -1.1827893295810785, 'log_learning_rate_D_dagger': -2.492255019973957, 'training_batch_size': 8, 'training_p': 3}. Best is trial 11 with value: 0.46003055572509766.
Time for this trial:  190.046236038208
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  16   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.019999029945883, 'log_learning_rate_D': -4.180606695278626, 'log_learning_rate_D_dagger': -2.246635749034576, 'training_batch_size': 9, 'training_p': 4}
	 epoch  0 training error:  tensor(70.4365, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(43.7750, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(22.0853, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(5.6417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(5.9000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(3.4572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(3.3882, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(2.9167, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(2.6420, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  90 training error:  tensor(2.5261, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-11 16:18:35,549] Trial 16 finished with value: 2.182781219482422 and parameters: {'log_learning_rate': -1.019999029945883, 'log_learning_rate_D': -4.180606695278626, 'log_learning_rate_D_dagger': -2.246635749034576, 'training_batch_size': 9, 'training_p': 4}. Best is trial 11 with value: 0.46003055572509766.
Time for this trial:  103.72771549224854
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  17   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.104821924376985, 'log_learning_rate_D': -4.9675909815726715, 'log_learning_rate_D_dagger': -1.7610139227068804, 'training_batch_size': 8, 'training_p': 7}
	 epoch  0 training error:  tensor(20.2550, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5531, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5454, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5349, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5309, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5291, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:21:46,237] Trial 17 finished with value: 0.4166421592235565 and parameters: {'log_learning_rate': -1.104821924376985, 'log_learning_rate_D': -4.9675909815726715, 'log_learning_rate_D_dagger': -1.7610139227068804, 'training_batch_size': 8, 'training_p': 7}. Best is trial 17 with value: 0.4166421592235565.
res:  tensor(0.4166, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.4600, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  190.48303389549255
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  18   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9425318202890725, 'log_learning_rate_D': -4.991529360816454, 'log_learning_rate_D_dagger': -1.0127001114880727, 'training_batch_size': 8, 'training_p': 8}
	 epoch  0 training error:  tensor(259.6285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.8684, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(6.4769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5773, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5620, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5530, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[W 2024-03-11 16:24:41,849] Trial 18 failed with parameters: {'log_learning_rate': -1.9425318202890725, 'log_learning_rate_D': -4.991529360816454, 'log_learning_rate_D_dagger': -1.0127001114880727, 'training_batch_size': 8, 'training_p': 8} because of the following error: The value nan is not acceptable.
[W 2024-03-11 16:24:41,849] Trial 18 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  175.4095482826233
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  19   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.060921724965495, 'log_learning_rate_D': -4.885827620423704, 'log_learning_rate_D_dagger': -1.2697534964648471, 'training_batch_size': 8, 'training_p': 8}
	 epoch  0 training error:  tensor(52.8822, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.7300, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7955, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6730, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6143, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5892, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5742, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5672, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5635, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5600, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:27:52,423] Trial 19 finished with value: 0.45212194323539734 and parameters: {'log_learning_rate': -2.060921724965495, 'log_learning_rate_D': -4.885827620423704, 'log_learning_rate_D_dagger': -1.2697534964648471, 'training_batch_size': 8, 'training_p': 8}. Best is trial 17 with value: 0.4166421592235565.
Time for this trial:  190.36562156677246
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  20   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0546616981877268, 'log_learning_rate_D': -4.939090024507162, 'log_learning_rate_D_dagger': -1.1483671452802544, 'training_batch_size': 8, 'training_p': 8}
	 epoch  0 training error:  tensor(2.1735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5894, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5517, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5485, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5442, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5451, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5437, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5460, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:31:02,334] Trial 20 finished with value: 0.4040220379829407 and parameters: {'log_learning_rate': -1.0546616981877268, 'log_learning_rate_D': -4.939090024507162, 'log_learning_rate_D_dagger': -1.1483671452802544, 'training_batch_size': 8, 'training_p': 8}. Best is trial 20 with value: 0.4040220379829407.
res:  tensor(0.4040, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.4166, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  189.68829822540283
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  6.0
--------------------  Trial  21   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0766670469125703, 'log_learning_rate_D': -4.931396874276357, 'log_learning_rate_D_dagger': -1.023028670208166, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(128.1887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.6959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.6376, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5429, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5370, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5262, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5381, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5221, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-11 16:35:39,297] Trial 21 finished with value: 0.39170145988464355 and parameters: {'log_learning_rate': -1.0766670469125703, 'log_learning_rate_D': -4.931396874276357, 'log_learning_rate_D_dagger': -1.023028670208166, 'training_batch_size': 7, 'training_p': 7}. Best is trial 21 with value: 0.39170145988464355.
res:  tensor(0.3917, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.4040, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  276.7614321708679
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  22   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5200687381469944, 'log_learning_rate_D': -4.592463410215662, 'log_learning_rate_D_dagger': -1.1809624224158002, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(360.8520, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.5134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5622, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5521, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5429, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:40:15,679] Trial 22 finished with value: 0.4210711419582367 and parameters: {'log_learning_rate': -1.5200687381469944, 'log_learning_rate_D': -4.592463410215662, 'log_learning_rate_D_dagger': -1.1809624224158002, 'training_batch_size': 7, 'training_p': 7}. Best is trial 21 with value: 0.39170145988464355.
Time for this trial:  276.14809131622314
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  23   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.5743029430049105, 'log_learning_rate_D': -4.950185924233199, 'log_learning_rate_D_dagger': -1.0444694325066908, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(43.9612, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.2951, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5644, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:44:51,271] Trial 23 finished with value: 0.3983392119407654 and parameters: {'log_learning_rate': -2.5743029430049105, 'log_learning_rate_D': -4.950185924233199, 'log_learning_rate_D_dagger': -1.0444694325066908, 'training_batch_size': 7, 'training_p': 7}. Best is trial 21 with value: 0.39170145988464355.
Time for this trial:  275.3506259918213
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  24   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.566380361924838, 'log_learning_rate_D': -4.502369721014388, 'log_learning_rate_D_dagger': -1.7517206682409863, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(174.1091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.3651, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5852, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5467, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5449, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5430, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5411, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5396, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5378, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 16:49:28,169] Trial 24 finished with value: 0.45206767320632935 and parameters: {'log_learning_rate': -2.566380361924838, 'log_learning_rate_D': -4.502369721014388, 'log_learning_rate_D_dagger': -1.7517206682409863, 'training_batch_size': 7, 'training_p': 7}. Best is trial 21 with value: 0.39170145988464355.
Time for this trial:  276.6644802093506
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  25   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.971624430130946, 'log_learning_rate_D': -3.752219784370463, 'log_learning_rate_D_dagger': -2.71538951916071, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(131.6061, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(74.9270, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(33.1014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.8806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.9697, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.9236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.9088, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.9018, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.8980, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.8886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 16:56:59,691] Trial 25 finished with value: 0.8780091404914856 and parameters: {'log_learning_rate': -4.971624430130946, 'log_learning_rate_D': -3.752219784370463, 'log_learning_rate_D_dagger': -2.71538951916071, 'training_batch_size': 6, 'training_p': 6}. Best is trial 21 with value: 0.39170145988464355.
Time for this trial:  451.25148916244507
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  26   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.9715777556435814, 'log_learning_rate_D': -4.616083203062128, 'log_learning_rate_D_dagger': -1.631422792630099, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(87.0444, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.6666, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6093, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5472, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5267, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5232, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5223, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5219, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 17:01:37,985] Trial 26 finished with value: 0.3941173255443573 and parameters: {'log_learning_rate': -2.9715777556435814, 'log_learning_rate_D': -4.616083203062128, 'log_learning_rate_D_dagger': -1.631422792630099, 'training_batch_size': 7, 'training_p': 7}. Best is trial 21 with value: 0.39170145988464355.
Time for this trial:  278.07730865478516
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  27   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.1152760650496187, 'log_learning_rate_D': -4.597185840684282, 'log_learning_rate_D_dagger': -1.0221511621240882, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(5.9354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5457, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5356, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5608, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5238, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5234, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5225, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 17:06:14,871] Trial 27 finished with value: 0.3914168179035187 and parameters: {'log_learning_rate': -3.1152760650496187, 'log_learning_rate_D': -4.597185840684282, 'log_learning_rate_D_dagger': -1.0221511621240882, 'training_batch_size': 7, 'training_p': 7}. Best is trial 27 with value: 0.3914168179035187.
res:  tensor(0.3914, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3917, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  276.6456391811371
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  28   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.4261353931224403, 'log_learning_rate_D': -4.523034419805349, 'log_learning_rate_D_dagger': -1.546785259751029, 'training_batch_size': 7, 'training_p': 6}
	 epoch  0 training error:  tensor(27.6745, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6658, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6081, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5366, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5069, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5045, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5047, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5046, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 17:10:51,682] Trial 28 finished with value: 0.40433913469314575 and parameters: {'log_learning_rate': -3.4261353931224403, 'log_learning_rate_D': -4.523034419805349, 'log_learning_rate_D_dagger': -1.546785259751029, 'training_batch_size': 7, 'training_p': 6}. Best is trial 27 with value: 0.3914168179035187.
Time for this trial:  276.57550835609436
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  29   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.1225530877518373, 'log_learning_rate_D': -4.54307075810165, 'log_learning_rate_D_dagger': -1.7983343390820847, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(67.7753, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5915, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5844, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5702, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5658, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5625, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 17:18:24,137] Trial 29 finished with value: 0.4696151316165924 and parameters: {'log_learning_rate': -3.1225530877518373, 'log_learning_rate_D': -4.54307075810165, 'log_learning_rate_D_dagger': -1.7983343390820847, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.3914168179035187.
Time for this trial:  452.1917905807495
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  30   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.0394354077446866, 'log_learning_rate_D': -4.0804333043869745, 'log_learning_rate_D_dagger': -1.4522954149496303, 'training_batch_size': 7, 'training_p': 8}
	 epoch  0 training error:  tensor(149.6924, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.6494, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0507, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5850, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5580, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5555, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 17:23:02,694] Trial 30 finished with value: 0.4485325813293457 and parameters: {'log_learning_rate': -3.0394354077446866, 'log_learning_rate_D': -4.0804333043869745, 'log_learning_rate_D_dagger': -1.4522954149496303, 'training_batch_size': 7, 'training_p': 8}. Best is trial 27 with value: 0.3914168179035187.
Time for this trial:  278.3249752521515
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  31   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3190666413514984, 'log_learning_rate_D': -4.650343313089484, 'log_learning_rate_D_dagger': -1.019573187576182, 'training_batch_size': 9, 'training_p': 6}
	 epoch  0 training error:  tensor(96.8635, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(7.9122, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(2.5144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(1.1537, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(3.8911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.9129, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.6115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.5339, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.5243, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.5123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 17:24:47,864] Trial 31 finished with value: 0.4131704866886139 and parameters: {'log_learning_rate': -2.3190666413514984, 'log_learning_rate_D': -4.650343313089484, 'log_learning_rate_D_dagger': -1.019573187576182, 'training_batch_size': 9, 'training_p': 6}. Best is trial 27 with value: 0.3914168179035187.
Time for this trial:  104.94842147827148
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  32   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.77460362411, 'log_learning_rate_D': -4.296926982491112, 'log_learning_rate_D_dagger': -1.9479650970142814, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(7.9174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5338, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5266, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5228, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5219, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 17:32:20,128] Trial 32 finished with value: 0.39062610268592834 and parameters: {'log_learning_rate': -2.77460362411, 'log_learning_rate_D': -4.296926982491112, 'log_learning_rate_D_dagger': -1.9479650970142814, 'training_batch_size': 6, 'training_p': 7}. Best is trial 32 with value: 0.39062610268592834.
res:  tensor(0.3906, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3914, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  451.99704933166504
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  33   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4377998943554706, 'log_learning_rate_D': -4.274706290103099, 'log_learning_rate_D_dagger': -1.9273246796701693, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(32.9075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.6436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.1842, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6768, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5593, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[W 2024-03-11 17:36:56,303] Trial 33 failed with parameters: {'log_learning_rate': -1.4377998943554706, 'log_learning_rate_D': -4.274706290103099, 'log_learning_rate_D_dagger': -1.9273246796701693, 'training_batch_size': 6, 'training_p': 8} because of the following error: The value nan is not acceptable.
[W 2024-03-11 17:36:56,303] Trial 33 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  275.93646574020386
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  34   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5742364353046647, 'log_learning_rate_D': -4.1379551674244155, 'log_learning_rate_D_dagger': -2.0318608901793094, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(457.6498, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(105.6428, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(12.0796, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.1527, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.9336, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.8250, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.7378, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.6729, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.6291, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5996, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 17:44:22,752] Trial 34 finished with value: 0.4353291690349579 and parameters: {'log_learning_rate': -1.5742364353046647, 'log_learning_rate_D': -4.1379551674244155, 'log_learning_rate_D_dagger': -2.0318608901793094, 'training_batch_size': 6, 'training_p': 8}. Best is trial 32 with value: 0.39062610268592834.
Time for this trial:  446.1818015575409
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  35   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3402704197278164, 'log_learning_rate_D': -4.340489755983908, 'log_learning_rate_D_dagger': -1.2816003799595013, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(135.8521, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4490, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4328, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4270, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4273, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 17:51:55,590] Trial 35 finished with value: 0.3358282148838043 and parameters: {'log_learning_rate': -2.3402704197278164, 'log_learning_rate_D': -4.340489755983908, 'log_learning_rate_D_dagger': -1.2816003799595013, 'training_batch_size': 6, 'training_p': 4}. Best is trial 35 with value: 0.3358282148838043.
res:  tensor(0.3358, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3906, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  452.5809733867645
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  36   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.739463668146268, 'log_learning_rate_D': -3.75270114672174, 'log_learning_rate_D_dagger': -1.3843883386410023, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(93.6418, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[W 2024-03-11 17:52:05,587] Trial 36 failed with parameters: {'log_learning_rate': -2.739463668146268, 'log_learning_rate_D': -3.75270114672174, 'log_learning_rate_D_dagger': -1.3843883386410023, 'training_batch_size': 6, 'training_p': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-11 17:52:05,588] Trial 36 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  9.770554065704346
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  37   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.7279359050210488, 'log_learning_rate_D': -3.6496371557799927, 'log_learning_rate_D_dagger': -1.2945092218984113, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(302.3839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7835, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6687, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4636, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4339, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4266, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 17:59:37,678] Trial 37 finished with value: 0.3365418612957001 and parameters: {'log_learning_rate': -2.7279359050210488, 'log_learning_rate_D': -3.6496371557799927, 'log_learning_rate_D_dagger': -1.2945092218984113, 'training_batch_size': 6, 'training_p': 4}. Best is trial 35 with value: 0.3358282148838043.
Time for this trial:  451.84047651290894
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  38   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.722404241788715, 'log_learning_rate_D': -3.6313642169180396, 'log_learning_rate_D_dagger': -1.439580360741779, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(73.9873, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4424, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4338, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4286, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4287, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:07:10,144] Trial 38 finished with value: 0.34215304255485535 and parameters: {'log_learning_rate': -2.722404241788715, 'log_learning_rate_D': -3.6313642169180396, 'log_learning_rate_D_dagger': -1.439580360741779, 'training_batch_size': 6, 'training_p': 4}. Best is trial 35 with value: 0.3358282148838043.
Time for this trial:  452.19729351997375
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  39   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.742925167015912, 'log_learning_rate_D': -3.620190708729651, 'log_learning_rate_D_dagger': -1.5804891907936873, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(108.0655, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6899, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5544, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5262, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5017, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4837, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4723, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4636, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4565, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:14:40,127] Trial 39 finished with value: 0.37098580598831177 and parameters: {'log_learning_rate': -2.742925167015912, 'log_learning_rate_D': -3.620190708729651, 'log_learning_rate_D_dagger': -1.5804891907936873, 'training_batch_size': 6, 'training_p': 4}. Best is trial 35 with value: 0.3358282148838043.
Time for this trial:  449.75603318214417
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  40   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3981285734071083, 'log_learning_rate_D': -3.5806143513660817, 'log_learning_rate_D_dagger': -1.4120492020219704, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(127.3958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.4021, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4320, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4283, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4278, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:22:12,488] Trial 40 finished with value: 0.33477750420570374 and parameters: {'log_learning_rate': -2.3981285734071083, 'log_learning_rate_D': -3.5806143513660817, 'log_learning_rate_D_dagger': -1.4120492020219704, 'training_batch_size': 6, 'training_p': 4}. Best is trial 40 with value: 0.33477750420570374.
res:  tensor(0.3348, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3358, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  452.13468861579895
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  41   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3853371792010036, 'log_learning_rate_D': -3.557051436396864, 'log_learning_rate_D_dagger': -1.3693521532004669, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(17.9488, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4793, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4279, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4281, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:29:43,912] Trial 41 finished with value: 0.3397747576236725 and parameters: {'log_learning_rate': -2.3853371792010036, 'log_learning_rate_D': -3.557051436396864, 'log_learning_rate_D_dagger': -1.3693521532004669, 'training_batch_size': 6, 'training_p': 4}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  451.18332171440125
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  42   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2841600534777937, 'log_learning_rate_D': -3.4581590030827165, 'log_learning_rate_D_dagger': -1.319168033851596, 'training_batch_size': 12, 'training_p': 4}
	 epoch  0 training error:  tensor(43.1311, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(1.8612, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(3.1384, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(1.1144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.5798, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5083, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4736, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4652, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4621, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 18:31:30,831] Trial 42 finished with value: 0.4036531448364258 and parameters: {'log_learning_rate': -2.2841600534777937, 'log_learning_rate_D': -3.4581590030827165, 'log_learning_rate_D_dagger': -1.319168033851596, 'training_batch_size': 12, 'training_p': 4}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  106.68160462379456
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  8.0
--------------------  Trial  43   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.349611483356278, 'log_learning_rate_D': -3.237219379059377, 'log_learning_rate_D_dagger': -1.4489114895485975, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(13.0063, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4801, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4387, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4007, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3808, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3783, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3823, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3791, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3783, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:39:01,108] Trial 43 finished with value: 0.3352231979370117 and parameters: {'log_learning_rate': -2.349611483356278, 'log_learning_rate_D': -3.237219379059377, 'log_learning_rate_D_dagger': -1.4489114895485975, 'training_batch_size': 6, 'training_p': 3}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  450.03138613700867
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  44   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0227427304267183, 'log_learning_rate_D': -3.146222619194213, 'log_learning_rate_D_dagger': -1.455648317273079, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(124.0978, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9684, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4362, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4207, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4189, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4193, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4191, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:46:32,611] Trial 44 finished with value: 0.3757674992084503 and parameters: {'log_learning_rate': -2.0227427304267183, 'log_learning_rate_D': -3.146222619194213, 'log_learning_rate_D_dagger': -1.455648317273079, 'training_batch_size': 6, 'training_p': 3}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  451.2350661754608
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  45   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.43781560739038, 'log_learning_rate_D': -2.789410870832372, 'log_learning_rate_D_dagger': -1.885990713381179, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(92.5973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.0526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5158, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4885, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4779, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4681, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4587, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4489, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4390, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4286, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 18:54:03,403] Trial 45 finished with value: 0.39724379777908325 and parameters: {'log_learning_rate': -2.43781560739038, 'log_learning_rate_D': -2.789410870832372, 'log_learning_rate_D_dagger': -1.885990713381179, 'training_batch_size': 6, 'training_p': 3}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  450.539155960083
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  46   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.10457386777639, 'log_learning_rate_D': -3.9408662772942824, 'log_learning_rate_D_dagger': -1.313859637231583, 'training_batch_size': 10, 'training_p': 2}
	 epoch  0 training error:  tensor(35.0938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(4.6505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.5939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.9542, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.6901, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.4154, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3790, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3561, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3402, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 18:55:50,050] Trial 46 finished with value: 0.3430614173412323 and parameters: {'log_learning_rate': -2.10457386777639, 'log_learning_rate_D': -3.9408662772942824, 'log_learning_rate_D_dagger': -1.313859637231583, 'training_batch_size': 10, 'training_p': 2}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  106.4015748500824
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  8.0
--------------------  Trial  47   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.3501886094464366, 'log_learning_rate_D': -3.296097992071961, 'log_learning_rate_D_dagger': -2.192551968407244, 'training_batch_size': 11, 'training_p': 5}
	 epoch  0 training error:  tensor(200.5021, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(151.6372, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(109.1676, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(73.2740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(43.4724, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(18.6814, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(3.9090, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(5.4481, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(3.2964, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(2.2305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 18:57:36,587] Trial 47 finished with value: 1.2193896770477295 and parameters: {'log_learning_rate': -3.3501886094464366, 'log_learning_rate_D': -3.296097992071961, 'log_learning_rate_D_dagger': -2.192551968407244, 'training_batch_size': 11, 'training_p': 5}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  106.30418968200684
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  8.0
--------------------  Trial  48   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.4289026408398042, 'log_learning_rate_D': -2.9877209931509037, 'log_learning_rate_D_dagger': -1.590602530275294, 'training_batch_size': 7, 'training_p': 5}
	 epoch  0 training error:  tensor(31.6417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.8512, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7687, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5604, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5721, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.6785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4933, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4846, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4821, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 19:02:14,477] Trial 48 finished with value: 0.3922583758831024 and parameters: {'log_learning_rate': -2.4289026408398042, 'log_learning_rate_D': -2.9877209931509037, 'log_learning_rate_D_dagger': -1.590602530275294, 'training_batch_size': 7, 'training_p': 5}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  277.64936804771423
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  49   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.901175280070854, 'log_learning_rate_D': -3.4882396343020687, 'log_learning_rate_D_dagger': -1.290235713202962, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(43.8804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5183, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4818, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4630, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4550, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4671, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4481, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4512, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 19:09:38,232] Trial 49 finished with value: 0.40784788131713867 and parameters: {'log_learning_rate': -1.901175280070854, 'log_learning_rate_D': -3.4882396343020687, 'log_learning_rate_D_dagger': -1.290235713202962, 'training_batch_size': 6, 'training_p': 4}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  443.4949486255646
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  50   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2839959574859794, 'log_learning_rate_D': -3.857845076852324, 'log_learning_rate_D_dagger': -1.7096632964238632, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(52.0239, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4817, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4600, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4473, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4444, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4408, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4414, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4377, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4343, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 19:17:09,302] Trial 50 finished with value: 0.3736273944377899 and parameters: {'log_learning_rate': -2.2839959574859794, 'log_learning_rate_D': -3.857845076852324, 'log_learning_rate_D_dagger': -1.7096632964238632, 'training_batch_size': 6, 'training_p': 4}. Best is trial 40 with value: 0.33477750420570374.
Time for this trial:  450.8188543319702
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  51   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3724934527055317, 'log_learning_rate_D': -3.685865507603495, 'log_learning_rate_D_dagger': -1.3346304660671147, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(208.6775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6963, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4227, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3907, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3720, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3709, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3705, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3705, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3703, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 19:24:40,605] Trial 51 finished with value: 0.3182363510131836 and parameters: {'log_learning_rate': -2.3724934527055317, 'log_learning_rate_D': -3.685865507603495, 'log_learning_rate_D_dagger': -1.3346304660671147, 'training_batch_size': 6, 'training_p': 3}. Best is trial 51 with value: 0.3182363510131836.
res:  tensor(0.3182, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3348, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  451.042462348938
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  52   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.17319905853492, 'log_learning_rate_D': -3.824681652334957, 'log_learning_rate_D_dagger': -1.181892136914724, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(26.9689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4232, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4367, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4001, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3898, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3881, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 19:32:02,671] Trial 52 finished with value: 0.32601800560951233 and parameters: {'log_learning_rate': -2.17319905853492, 'log_learning_rate_D': -3.824681652334957, 'log_learning_rate_D_dagger': -1.181892136914724, 'training_batch_size': 6, 'training_p': 3}. Best is trial 51 with value: 0.3182363510131836.
Time for this trial:  441.81554198265076
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  53   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7569955656447456, 'log_learning_rate_D': -4.058047367082951, 'log_learning_rate_D_dagger': -1.5587232132062607, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(87.6556, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.8756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8642, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5053, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3567, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4705, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3601, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3191, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3039, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 19:36:41,363] Trial 53 finished with value: 0.3570233881473541 and parameters: {'log_learning_rate': -1.7569955656447456, 'log_learning_rate_D': -4.058047367082951, 'log_learning_rate_D_dagger': -1.5587232132062607, 'training_batch_size': 7, 'training_p': 2}. Best is trial 51 with value: 0.3182363510131836.
Time for this trial:  278.4588477611542
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  54   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1168957850789503, 'log_learning_rate_D': -3.8424774490982716, 'log_learning_rate_D_dagger': -1.9005221632051645, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(10.2651, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5095, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4327, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3984, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3761, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3768, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 19:44:12,102] Trial 54 finished with value: 0.33228859305381775 and parameters: {'log_learning_rate': -2.1168957850789503, 'log_learning_rate_D': -3.8424774490982716, 'log_learning_rate_D_dagger': -1.9005221632051645, 'training_batch_size': 6, 'training_p': 3}. Best is trial 51 with value: 0.3182363510131836.
Time for this trial:  450.48983430862427
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  55   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1101592522571986, 'log_learning_rate_D': -3.8286237043785305, 'log_learning_rate_D_dagger': -1.8755164033592968, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(67.4984, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.6582, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(3.1712, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.7977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.1999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.8961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.6598, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4919, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4222, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-11 19:47:25,321] Trial 55 finished with value: 0.36791449785232544 and parameters: {'log_learning_rate': -2.1101592522571986, 'log_learning_rate_D': -3.8286237043785305, 'log_learning_rate_D_dagger': -1.8755164033592968, 'training_batch_size': 8, 'training_p': 3}. Best is trial 51 with value: 0.3182363510131836.
Time for this trial:  192.98932600021362
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  56   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0021223620201747, 'log_learning_rate_D': -3.3277149817157654, 'log_learning_rate_D_dagger': -2.0562877687482213, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(67.0406, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.8987, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(2.3582, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.4869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.8178, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4332, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3821, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3774, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3744, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3726, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 19:54:58,386] Trial 56 finished with value: 0.31449568271636963 and parameters: {'log_learning_rate': -2.0021223620201747, 'log_learning_rate_D': -3.3277149817157654, 'log_learning_rate_D_dagger': -2.0562877687482213, 'training_batch_size': 6, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
res:  tensor(0.3145, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3182, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  452.81680822372437
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  57   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8540101968019402, 'log_learning_rate_D': -3.39793686953779, 'log_learning_rate_D_dagger': -2.130067311577202, 'training_batch_size': 11, 'training_p': 2}
	 epoch  0 training error:  tensor(141.5102, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(105.6805, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(75.7014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  30 training error:  tensor(51.7602, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(33.4961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(20.1312, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(10.8588, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(4.3345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(1.4433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(1.2932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 19:56:44,984] Trial 57 finished with value: 0.8579934239387512 and parameters: {'log_learning_rate': -1.8540101968019402, 'log_learning_rate_D': -3.39793686953779, 'log_learning_rate_D_dagger': -2.130067311577202, 'training_batch_size': 11, 'training_p': 2}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  106.39912438392639
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  8.0
--------------------  Trial  58   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.171183814882855, 'log_learning_rate_D': -3.7393732708942755, 'log_learning_rate_D_dagger': -2.4794985414589137, 'training_batch_size': 7, 'training_p': 3}
	 epoch  0 training error:  tensor(374.4917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(291.6989, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(222.9344, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(167.1800, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(123.3974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(89.2353, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(63.4093, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(44.1707, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(29.8653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(19.6622, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 20:01:24,791] Trial 58 finished with value: 8.657221794128418 and parameters: {'log_learning_rate': -2.171183814882855, 'log_learning_rate_D': -3.7393732708942755, 'log_learning_rate_D_dagger': -2.4794985414589137, 'training_batch_size': 7, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  279.5704753398895
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  59   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.998728449105906, 'log_learning_rate_D': -3.224500470395161, 'log_learning_rate_D_dagger': -1.7252513719089744, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(405.0505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.4784, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.2125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.8824, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6862, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5069, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4773, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4607, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 20:08:57,731] Trial 59 finished with value: 0.4120454490184784 and parameters: {'log_learning_rate': -1.998728449105906, 'log_learning_rate_D': -3.224500470395161, 'log_learning_rate_D_dagger': -1.7252513719089744, 'training_batch_size': 6, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  452.69800996780396
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  60   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6957296686624463, 'log_learning_rate_D': -3.120924094606534, 'log_learning_rate_D_dagger': -1.1534146950801234, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(50.0579, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5333, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4262, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4054, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3738, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3747, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3768, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3707, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3703, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 20:16:28,951] Trial 60 finished with value: 0.31814059615135193 and parameters: {'log_learning_rate': -1.6957296686624463, 'log_learning_rate_D': -3.120924094606534, 'log_learning_rate_D_dagger': -1.1534146950801234, 'training_batch_size': 6, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  450.97016954421997
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  61   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7017627415794137, 'log_learning_rate_D': -3.4590164579874476, 'log_learning_rate_D_dagger': -1.152297714209444, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(10.3354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4608, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3749, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3723, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3731, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3734, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 20:24:01,514] Trial 61 finished with value: 0.3167746067047119 and parameters: {'log_learning_rate': -1.7017627415794137, 'log_learning_rate_D': -3.4590164579874476, 'log_learning_rate_D_dagger': -1.152297714209444, 'training_batch_size': 6, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  452.3217787742615
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  62   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5986952172149993, 'log_learning_rate_D': -3.0899747045955612, 'log_learning_rate_D_dagger': -1.1732529791005653, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(79.8968, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.4384, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4723, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3445, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3145, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 20:28:41,747] Trial 62 finished with value: 0.3358171284198761 and parameters: {'log_learning_rate': -1.5986952172149993, 'log_learning_rate_D': -3.0899747045955612, 'log_learning_rate_D_dagger': -1.1732529791005653, 'training_batch_size': 7, 'training_p': 2}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  279.99615144729614
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  63   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3611403066795669, 'log_learning_rate_D': -3.3791456915751326, 'log_learning_rate_D_dagger': -1.1804221525422531, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(153.5794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7017, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4341, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4037, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3933, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3850, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3805, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3820, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3787, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 20:36:14,415] Trial 63 finished with value: 0.3395638167858124 and parameters: {'log_learning_rate': -1.3611403066795669, 'log_learning_rate_D': -3.3791456915751326, 'log_learning_rate_D_dagger': -1.1804221525422531, 'training_batch_size': 6, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  452.3884553909302
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  64   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7896098042660453, 'log_learning_rate_D': -2.835703600052691, 'log_learning_rate_D_dagger': -2.015701627248456, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(32.3287, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(5.5715, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(3.2055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.9915, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(1.4426, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.0259, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.7504, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5864, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4521, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 20:40:54,569] Trial 64 finished with value: 0.45583483576774597 and parameters: {'log_learning_rate': -1.7896098042660453, 'log_learning_rate_D': -2.835703600052691, 'log_learning_rate_D_dagger': -2.015701627248456, 'training_batch_size': 7, 'training_p': 2}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  279.8866910934448
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  65   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9550582934597656, 'log_learning_rate_D': -3.9959306038375932, 'log_learning_rate_D_dagger': -2.3664760089691272, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(126.5157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(27.1268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.7337, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6045, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4854, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4669, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4820, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4487, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 20:48:24,597] Trial 65 finished with value: 0.41454440355300903 and parameters: {'log_learning_rate': -1.9550582934597656, 'log_learning_rate_D': -3.9959306038375932, 'log_learning_rate_D_dagger': -2.3664760089691272, 'training_batch_size': 6, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  449.7615969181061
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  66   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6770465781855455, 'log_learning_rate_D': -3.9198017494547646, 'log_learning_rate_D_dagger': -1.1333819541170211, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(46.0228, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.4425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5229, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4883, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4542, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4248, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4191, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4365, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 20:51:38,152] Trial 66 finished with value: 0.3681422472000122 and parameters: {'log_learning_rate': -1.6770465781855455, 'log_learning_rate_D': -3.9198017494547646, 'log_learning_rate_D_dagger': -1.1333819541170211, 'training_batch_size': 8, 'training_p': 3}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  193.33264636993408
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  67   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4724698109538832, 'log_learning_rate_D': -3.7327212528920453, 'log_learning_rate_D_dagger': -1.0086092624190897, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(111.6213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.9313, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8199, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3849, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3659, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4054, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3636, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3332, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 20:56:17,744] Trial 67 finished with value: 0.32625728845596313 and parameters: {'log_learning_rate': -1.4724698109538832, 'log_learning_rate_D': -3.7327212528920453, 'log_learning_rate_D_dagger': -1.0086092624190897, 'training_batch_size': 7, 'training_p': 2}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  279.3387327194214
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  68   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.439931087793139, 'log_learning_rate_D': -3.426547063008482, 'log_learning_rate_D_dagger': -1.1100848271595023, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(16.4849, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.3684, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.2920, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2978, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2936, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2954, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:00:56,705] Trial 68 finished with value: 0.3147924542427063 and parameters: {'log_learning_rate': -1.439931087793139, 'log_learning_rate_D': -3.426547063008482, 'log_learning_rate_D_dagger': -1.1100848271595023, 'training_batch_size': 7, 'training_p': 2}. Best is trial 56 with value: 0.31449568271636963.
Time for this trial:  278.73255038261414
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  69   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4248129353565597, 'log_learning_rate_D': -3.4035575084183343, 'log_learning_rate_D_dagger': -1.031857236321552, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(22.6970, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3661, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3161, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.2996, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2968, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2949, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:05:35,469] Trial 69 finished with value: 0.31162887811660767 and parameters: {'log_learning_rate': -1.4248129353565597, 'log_learning_rate_D': -3.4035575084183343, 'log_learning_rate_D_dagger': -1.031857236321552, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
res:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3145, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  278.5385670661926
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  70   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3105119809198051, 'log_learning_rate_D': -3.4101078486822507, 'log_learning_rate_D_dagger': -1.1334707873752212, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(7.1174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4281, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3903, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3306, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3195, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2972, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:08:47,196] Trial 70 finished with value: 0.31833598017692566 and parameters: {'log_learning_rate': -1.3105119809198051, 'log_learning_rate_D': -3.4101078486822507, 'log_learning_rate_D_dagger': -1.1334707873752212, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.5279893875122
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  71   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.229527100625497, 'log_learning_rate_D': -3.35130338440464, 'log_learning_rate_D_dagger': -1.1138576294660747, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(57.2189, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.1287, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3930, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3920, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3764, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3727, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3745, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3648, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:11:59,713] Trial 71 finished with value: 0.3751066327095032 and parameters: {'log_learning_rate': -1.229527100625497, 'log_learning_rate_D': -3.35130338440464, 'log_learning_rate_D_dagger': -1.1138576294660747, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  192.29854822158813
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  72   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3976172639227837, 'log_learning_rate_D': -3.4736635946344383, 'log_learning_rate_D_dagger': -1.1888382455385125, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(36.1412, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(3.3530, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.5724, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(1.0303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.6606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.4911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4149, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3819, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3435, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3234, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 21:13:45,186] Trial 72 finished with value: 0.33173468708992004 and parameters: {'log_learning_rate': -1.3976172639227837, 'log_learning_rate_D': -3.4736635946344383, 'log_learning_rate_D_dagger': -1.1888382455385125, 'training_batch_size': 9, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  105.23557209968567
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  73   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.692104758346507, 'log_learning_rate_D': -3.140275402732709, 'log_learning_rate_D_dagger': -1.0372401143285883, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(137.6992, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.8157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.3514, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4454, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4039, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3736, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3503, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:16:57,232] Trial 73 finished with value: 0.36517807841300964 and parameters: {'log_learning_rate': -1.692104758346507, 'log_learning_rate_D': -3.140275402732709, 'log_learning_rate_D_dagger': -1.0372401143285883, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.82358074188232
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  74   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2306042171310574, 'log_learning_rate_D': -3.3020827205429035, 'log_learning_rate_D_dagger': -1.5089764609598553, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(166.3827, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.2993, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3461, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3381, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3233, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3182, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:21:34,946] Trial 74 finished with value: 0.3318440616130829 and parameters: {'log_learning_rate': -1.2306042171310574, 'log_learning_rate_D': -3.3020827205429035, 'log_learning_rate_D_dagger': -1.5089764609598553, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  277.4659559726715
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  75   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.510028864105747, 'log_learning_rate_D': -2.922985533156522, 'log_learning_rate_D_dagger': -1.6377718322153205, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(114.9974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(34.1714, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(3.4187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.9983, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.3906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.7908, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3840, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 21:23:20,807] Trial 75 finished with value: 0.39411646127700806 and parameters: {'log_learning_rate': -1.510028864105747, 'log_learning_rate_D': -2.922985533156522, 'log_learning_rate_D_dagger': -1.6377718322153205, 'training_batch_size': 9, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  105.64837431907654
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  76   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3405885668167856, 'log_learning_rate_D': -3.5258610367659307, 'log_learning_rate_D_dagger': -1.3784638245534522, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(2.6145, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.3995, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3537, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3448, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2945, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:27:58,653] Trial 76 finished with value: 0.3136882483959198 and parameters: {'log_learning_rate': -1.3405885668167856, 'log_learning_rate_D': -3.5258610367659307, 'log_learning_rate_D_dagger': -1.3784638245534522, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  277.6040539741516
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  77   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6333432935232877, 'log_learning_rate_D': -3.58498590258405, 'log_learning_rate_D_dagger': -1.3574257630764903, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(194.6452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.9015, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3808, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3525, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:32:35,612] Trial 77 finished with value: 0.33112701773643494 and parameters: {'log_learning_rate': -1.6333432935232877, 'log_learning_rate_D': -3.58498590258405, 'log_learning_rate_D_dagger': -1.3574257630764903, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  276.7143933773041
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  78   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4536494614926656, 'log_learning_rate_D': -3.216646140840513, 'log_learning_rate_D_dagger': -1.6668509312791828, 'training_batch_size': 7, 'training_p': 3}
	 epoch  0 training error:  tensor(190.4473, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(12.7679, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(2.0825, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.3701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.9443, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.6581, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4849, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4329, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4061, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:37:12,011] Trial 78 finished with value: 0.34756147861480713 and parameters: {'log_learning_rate': -1.4536494614926656, 'log_learning_rate_D': -3.216646140840513, 'log_learning_rate_D_dagger': -1.6668509312791828, 'training_batch_size': 7, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  276.1516568660736
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  79   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3334959171697984, 'log_learning_rate_D': -3.5337958213241163, 'log_learning_rate_D_dagger': -1.2520784258227937, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(62.2017, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5951, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2998, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:40:24,570] Trial 79 finished with value: 0.3131522238254547 and parameters: {'log_learning_rate': -1.3334959171697984, 'log_learning_rate_D': -3.5337958213241163, 'log_learning_rate_D_dagger': -1.2520784258227937, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  192.33791851997375
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  80   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0110744787022052, 'log_learning_rate_D': -3.510586460983025, 'log_learning_rate_D_dagger': -1.250118336020054, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(83.7811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.3267, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5825, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3358, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:43:36,607] Trial 80 finished with value: 0.3148610293865204 and parameters: {'log_learning_rate': -1.0110744787022052, 'log_learning_rate_D': -3.510586460983025, 'log_learning_rate_D_dagger': -1.250118336020054, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.81200313568115
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  81   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.147953037245149, 'log_learning_rate_D': -3.545138873427924, 'log_learning_rate_D_dagger': -1.0019521065168515, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(31.9319, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3799, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3626, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3374, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3537, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2990, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:46:48,870] Trial 81 finished with value: 0.31809931993484497 and parameters: {'log_learning_rate': -1.147953037245149, 'log_learning_rate_D': -3.545138873427924, 'log_learning_rate_D_dagger': -1.0019521065168515, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  192.01981806755066
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  82   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1374371543060178, 'log_learning_rate_D': -3.5040108259576725, 'log_learning_rate_D_dagger': -1.4760828639321584, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(23.3839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5335, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3252, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3137, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3132, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3167, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:50:01,418] Trial 82 finished with value: 0.3300793766975403 and parameters: {'log_learning_rate': -1.1374371543060178, 'log_learning_rate_D': -3.5040108259576725, 'log_learning_rate_D_dagger': -1.4760828639321584, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  192.32953882217407
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  83   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0180859984790134, 'log_learning_rate_D': -3.5578901616183956, 'log_learning_rate_D_dagger': -1.2536458510944413, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(23.0230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(3.8838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.8465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.7364, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4337, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.4028, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3583, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3385, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3235, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 21:51:46,920] Trial 83 finished with value: 0.3349458873271942 and parameters: {'log_learning_rate': -1.0180859984790134, 'log_learning_rate_D': -3.5578901616183956, 'log_learning_rate_D_dagger': -1.2536458510944413, 'training_batch_size': 9, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  105.29773187637329
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  84   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1437831550426827, 'log_learning_rate_D': -3.67181716469894, 'log_learning_rate_D_dagger': -1.0085593534095392, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(68.8012, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[W 2024-03-11 21:51:53,612] Trial 84 failed with parameters: {'log_learning_rate': -1.1437831550426827, 'log_learning_rate_D': -3.67181716469894, 'log_learning_rate_D_dagger': -1.0085593534095392, 'training_batch_size': 8, 'training_p': 2} because of the following error: The value nan is not acceptable.
[W 2024-03-11 21:51:53,613] Trial 84 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  6.46256685256958
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  85   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.134086105954415, 'log_learning_rate_D': -3.660928256380414, 'log_learning_rate_D_dagger': -1.021175532010007, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(121.9474, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7719, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4607, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4183, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3884, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3719, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3796, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3774, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 21:55:05,503] Trial 85 finished with value: 0.3976915776729584 and parameters: {'log_learning_rate': -1.134086105954415, 'log_learning_rate_D': -3.660928256380414, 'log_learning_rate_D_dagger': -1.021175532010007, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.64624547958374
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  86   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3108868315281312, 'log_learning_rate_D': -3.3076513079336816, 'log_learning_rate_D_dagger': -1.3885829243197219, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(60.9018, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(11.5645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(2.0949, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(1.3498, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.6843, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4983, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4713, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4544, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 21:56:50,596] Trial 86 finished with value: 0.4799099862575531 and parameters: {'log_learning_rate': -1.3108868315281312, 'log_learning_rate_D': -3.3076513079336816, 'log_learning_rate_D_dagger': -1.3885829243197219, 'training_batch_size': 9, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  104.88605046272278
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  87   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.001686561667444, 'log_learning_rate_D': -3.529741337649431, 'log_learning_rate_D_dagger': -1.2596952620505684, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(132.2779, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.6861, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.6162, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.8243, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3949, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3924, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3834, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3688, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:00:02,092] Trial 87 finished with value: 0.37084004282951355 and parameters: {'log_learning_rate': -1.001686561667444, 'log_learning_rate_D': -3.529741337649431, 'log_learning_rate_D_dagger': -1.2596952620505684, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.27430725097656
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  88   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.187412636510212, 'log_learning_rate_D': -3.722781611890358, 'log_learning_rate_D_dagger': -1.0025218139960894, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(170.7424, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(8.5202, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5034, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5113, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3750, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3675, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3612, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3547, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:03:14,285] Trial 88 finished with value: 0.36087122559547424 and parameters: {'log_learning_rate': -1.187412636510212, 'log_learning_rate_D': -3.722781611890358, 'log_learning_rate_D_dagger': -1.0025218139960894, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.97017002105713
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  89   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4081301561707016, 'log_learning_rate_D': -3.432237198519356, 'log_learning_rate_D_dagger': -1.5354132493658685, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(181.6203, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.0731, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5101, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3646, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3585, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3476, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3422, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3364, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:07:53,386] Trial 89 finished with value: 0.34672239422798157 and parameters: {'log_learning_rate': -1.4081301561707016, 'log_learning_rate_D': -3.432237198519356, 'log_learning_rate_D_dagger': -1.5354132493658685, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  278.85294818878174
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  90   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2825118791610761, 'log_learning_rate_D': -3.0845150771718584, 'log_learning_rate_D_dagger': -1.2301031396467959, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(165.4796, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.4501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4183, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3884, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3781, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3648, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:12:30,957] Trial 90 finished with value: 0.36392268538475037 and parameters: {'log_learning_rate': -1.2825118791610761, 'log_learning_rate_D': -3.0845150771718584, 'log_learning_rate_D_dagger': -1.2301031396467959, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  277.31430530548096
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  91   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5345617340126065, 'log_learning_rate_D': -3.195057410331968, 'log_learning_rate_D_dagger': -1.110359324047542, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(23.7941, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.1852, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4474, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4003, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:15:42,937] Trial 91 finished with value: 0.3599422872066498 and parameters: {'log_learning_rate': -1.5345617340126065, 'log_learning_rate_D': -3.195057410331968, 'log_learning_rate_D_dagger': -1.110359324047542, 'training_batch_size': 8, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.7449233531952
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  92   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1280485911345899, 'log_learning_rate_D': -3.2961349637566952, 'log_learning_rate_D_dagger': -1.4000617796065815, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(28.5423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(2.0537, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.6522, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.7715, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5052, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.4802, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4484, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4314, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4067, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 22:17:28,474] Trial 92 finished with value: 0.408684104681015 and parameters: {'log_learning_rate': -1.1280485911345899, 'log_learning_rate_D': -3.2961349637566952, 'log_learning_rate_D_dagger': -1.4000617796065815, 'training_batch_size': 9, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  105.30683064460754
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  93   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.375395723184206, 'log_learning_rate_D': -3.4201743875371307, 'log_learning_rate_D_dagger': -1.0947766415552762, 'training_batch_size': 7, 'training_p': 3}
	 epoch  0 training error:  tensor(97.1592, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.3506, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4810, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4343, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4440, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4200, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4420, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4145, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4025, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4010, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:22:06,994] Trial 93 finished with value: 0.350801944732666 and parameters: {'log_learning_rate': -1.375395723184206, 'log_learning_rate_D': -3.4201743875371307, 'log_learning_rate_D_dagger': -1.0947766415552762, 'training_batch_size': 7, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  278.28103590011597
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  94   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5455938798936044, 'log_learning_rate_D': -3.040967236152389, 'log_learning_rate_D_dagger': -1.2388167869091473, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(26.8646, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.8251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3319, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3106, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3102, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:26:45,216] Trial 94 finished with value: 0.3296448290348053 and parameters: {'log_learning_rate': -1.5455938798936044, 'log_learning_rate_D': -3.040967236152389, 'log_learning_rate_D_dagger': -1.2388167869091473, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  277.9795820713043
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  95   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7286671183628897, 'log_learning_rate_D': -3.6668463757503025, 'log_learning_rate_D_dagger': -1.3305553454567653, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(169.6197, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(10.8227, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.8595, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5407, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5228, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5068, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4924, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4789, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4665, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:29:57,157] Trial 95 finished with value: 0.43585506081581116 and parameters: {'log_learning_rate': -1.7286671183628897, 'log_learning_rate_D': -3.6668463757503025, 'log_learning_rate_D_dagger': -1.3305553454567653, 'training_batch_size': 8, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  191.70055675506592
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  96   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.449042878332812, 'log_learning_rate_D': -3.5125603201298947, 'log_learning_rate_D_dagger': -1.7887817700282471, 'training_batch_size': 10, 'training_p': 5}
	 epoch  0 training error:  tensor(32.1900, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(2.4567, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(2.1884, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.8668, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5627, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4862, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4764, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4702, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-11 22:31:42,338] Trial 96 finished with value: 0.36822327971458435 and parameters: {'log_learning_rate': -1.449042878332812, 'log_learning_rate_D': -3.5125603201298947, 'log_learning_rate_D_dagger': -1.7887817700282471, 'training_batch_size': 10, 'training_p': 5}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  104.96849155426025
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  97   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2206626334446185, 'log_learning_rate_D': -3.178087004628481, 'log_learning_rate_D_dagger': -1.1040559343469376, 'training_batch_size': 7, 'training_p': 3}
	 epoch  0 training error:  tensor(130.1949, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.3591, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5141, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4249, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4148, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:36:20,418] Trial 97 finished with value: 0.3802759647369385 and parameters: {'log_learning_rate': -1.2206626334446185, 'log_learning_rate_D': -3.178087004628481, 'log_learning_rate_D_dagger': -1.1040559343469376, 'training_batch_size': 7, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  277.83967661857605
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  98   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6249723816669093, 'log_learning_rate_D': -3.5932188918394714, 'log_learning_rate_D_dagger': -1.5418734369146014, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(147.5202, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.8265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4083, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3819, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3672, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3486, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3273, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3336, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 22:43:52,349] Trial 98 finished with value: 0.38969311118125916 and parameters: {'log_learning_rate': -1.6249723816669093, 'log_learning_rate_D': -3.5932188918394714, 'log_learning_rate_D_dagger': -1.5418734369146014, 'training_batch_size': 6, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  451.6648449897766
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  99   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.84381620387561, 'log_learning_rate_D': -3.3514841030750158, 'log_learning_rate_D_dagger': -1.4541675916616708, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(44.3790, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.3108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8256, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3842, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3153, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 22:47:04,689] Trial 99 finished with value: 0.33494189381599426 and parameters: {'log_learning_rate': -1.84381620387561, 'log_learning_rate_D': -3.3514841030750158, 'log_learning_rate_D_dagger': -1.4541675916616708, 'training_batch_size': 8, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  192.11330008506775
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  100   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.316385977746676, 'log_learning_rate_D': -3.7511357199886355, 'log_learning_rate_D_dagger': -1.3245467673726719, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(332.1997, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.2303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4231, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3960, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3795, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3724, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3715, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 22:54:35,163] Trial 100 finished with value: 0.31988897919654846 and parameters: {'log_learning_rate': -1.316385977746676, 'log_learning_rate_D': -3.7511357199886355, 'log_learning_rate_D_dagger': -1.3245467673726719, 'training_batch_size': 6, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  450.20192551612854
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  101   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0796330288234417, 'log_learning_rate_D': -3.4680878982431214, 'log_learning_rate_D_dagger': -1.2296044001138664, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(12.1663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4206, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3836, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3825, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3868, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3822, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3882, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:01:56,899] Trial 101 finished with value: 0.33389315009117126 and parameters: {'log_learning_rate': -1.0796330288234417, 'log_learning_rate_D': -3.4680878982431214, 'log_learning_rate_D_dagger': -1.2296044001138664, 'training_batch_size': 6, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  441.49864077568054
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  102   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7841021075311505, 'log_learning_rate_D': -3.6617023235507222, 'log_learning_rate_D_dagger': -1.3600201428369119, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(117.8657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5921, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4584, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4300, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4061, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3874, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3774, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:09:28,091] Trial 102 finished with value: 0.33388254046440125 and parameters: {'log_learning_rate': -1.7841021075311505, 'log_learning_rate_D': -3.6617023235507222, 'log_learning_rate_D_dagger': -1.3600201428369119, 'training_batch_size': 6, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  450.9432535171509
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  103   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5775739308368322, 'log_learning_rate_D': -3.255385662748387, 'log_learning_rate_D_dagger': -1.087594056332183, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(308.2663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.0993, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3883, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3734, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3753, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3726, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3750, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:16:59,748] Trial 103 finished with value: 0.31852251291275024 and parameters: {'log_learning_rate': -1.5775739308368322, 'log_learning_rate_D': -3.255385662748387, 'log_learning_rate_D_dagger': -1.087594056332183, 'training_batch_size': 6, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  451.39711260795593
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  104   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9057903937185159, 'log_learning_rate_D': -3.3707381271409287, 'log_learning_rate_D_dagger': -1.1851891536807715, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(114.7165, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.2853, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4074, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3734, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3135, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3128, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:24:31,128] Trial 104 finished with value: 0.3347878158092499 and parameters: {'log_learning_rate': -1.9057903937185159, 'log_learning_rate_D': -3.3707381271409287, 'log_learning_rate_D_dagger': -1.1851891536807715, 'training_batch_size': 6, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  451.1294629573822
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  105   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.715431662013839, 'log_learning_rate_D': -3.8162890104957543, 'log_learning_rate_D_dagger': -1.6616710956746703, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(163.3883, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.2811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5044, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4853, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4678, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4512, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4359, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4215, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4084, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 23:29:08,854] Trial 105 finished with value: 0.40605735778808594 and parameters: {'log_learning_rate': -1.715431662013839, 'log_learning_rate_D': -3.8162890104957543, 'log_learning_rate_D_dagger': -1.6616710956746703, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  277.4810788631439
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  106   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4548468374759782, 'log_learning_rate_D': -3.597427223349158, 'log_learning_rate_D_dagger': -1.4251511267266568, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(46.3133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5368, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4328, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4857, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4629, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4143, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4050, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:36:39,264] Trial 106 finished with value: 0.3598218858242035 and parameters: {'log_learning_rate': -1.4548468374759782, 'log_learning_rate_D': -3.597427223349158, 'log_learning_rate_D_dagger': -1.4251511267266568, 'training_batch_size': 6, 'training_p': 3}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  450.1567060947418
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  107   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2046243814978719, 'log_learning_rate_D': -3.143539175429765, 'log_learning_rate_D_dagger': -1.0018433870158763, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(98.3554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.3414, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7575, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4558, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3761, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4221, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3726, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-11 23:41:18,019] Trial 107 finished with value: 0.3647417426109314 and parameters: {'log_learning_rate': -1.2046243814978719, 'log_learning_rate_D': -3.143539175429765, 'log_learning_rate_D_dagger': -1.0018433870158763, 'training_batch_size': 7, 'training_p': 2}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  278.4930684566498
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  108   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3156256066762422, 'log_learning_rate_D': -3.4709687149476856, 'log_learning_rate_D_dagger': -1.3011345255986222, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(202.0875, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.4913, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4798, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4430, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4353, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4331, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4321, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4330, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4316, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:48:50,141] Trial 108 finished with value: 0.334237277507782 and parameters: {'log_learning_rate': -1.3156256066762422, 'log_learning_rate_D': -3.4709687149476856, 'log_learning_rate_D_dagger': -1.3011345255986222, 'training_batch_size': 6, 'training_p': 4}. Best is trial 69 with value: 0.31162887811660767.
Time for this trial:  451.8540675640106
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  109   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9616655834207477, 'log_learning_rate_D': -3.277146946987623, 'log_learning_rate_D_dagger': -1.6089969122377734, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(37.9891, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5630, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3755, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3019, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.2917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2921, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-11 23:56:22,377] Trial 109 finished with value: 0.31147700548171997 and parameters: {'log_learning_rate': -1.9616655834207477, 'log_learning_rate_D': -3.277146946987623, 'log_learning_rate_D_dagger': -1.6089969122377734, 'training_batch_size': 6, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
res:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  451.97583508491516
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  110   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9564167641529635, 'log_learning_rate_D': -3.2469431704074787, 'log_learning_rate_D_dagger': -1.17491998157812, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(10.1319, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4248, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3363, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3135, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3109, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3122, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 00:03:55,218] Trial 110 finished with value: 0.33248645067214966 and parameters: {'log_learning_rate': -1.9564167641529635, 'log_learning_rate_D': -3.2469431704074787, 'log_learning_rate_D_dagger': -1.17491998157812, 'training_batch_size': 6, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  452.5997722148895
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  111   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0417410757882384, 'log_learning_rate_D': -3.3652349494387654, 'log_learning_rate_D_dagger': -1.5114829273493522, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(113.5280, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9450, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4007, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3632, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3456, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3407, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3550, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 00:11:28,577] Trial 111 finished with value: 0.3390914499759674 and parameters: {'log_learning_rate': -2.0417410757882384, 'log_learning_rate_D': -3.3652349494387654, 'log_learning_rate_D_dagger': -1.5114829273493522, 'training_batch_size': 6, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  453.08448219299316
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  112   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6842266225532607, 'log_learning_rate_D': -3.549408864446087, 'log_learning_rate_D_dagger': -1.5919441568060824, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(24.9121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5238, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3727, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3106, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3128, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3136, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3104, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 00:19:00,781] Trial 112 finished with value: 0.3298272490501404 and parameters: {'log_learning_rate': -1.6842266225532607, 'log_learning_rate_D': -3.549408864446087, 'log_learning_rate_D_dagger': -1.5919441568060824, 'training_batch_size': 6, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  451.9417304992676
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  113   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5324442188911394, 'log_learning_rate_D': -3.705562409338596, 'log_learning_rate_D_dagger': -1.2538449496404194, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(163.1564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.0897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3477, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.2929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2923, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2925, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2921, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 00:26:32,865] Trial 113 finished with value: 0.31211820244789124 and parameters: {'log_learning_rate': -1.5324442188911394, 'log_learning_rate_D': -3.705562409338596, 'log_learning_rate_D_dagger': -1.2538449496404194, 'training_batch_size': 6, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  451.82674956321716
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  114   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5111834137048645, 'log_learning_rate_D': -3.014515255219152, 'log_learning_rate_D_dagger': -1.1133689237970579, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(155.2517, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.8759, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.2414, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.2965, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2962, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3225, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2981, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 00:31:05,521] Trial 114 finished with value: 0.31896764039993286 and parameters: {'log_learning_rate': -1.5111834137048645, 'log_learning_rate_D': -3.014515255219152, 'log_learning_rate_D_dagger': -1.1133689237970579, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  272.42390155792236
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  115   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.376212840279642, 'log_learning_rate_D': -3.8971598816448187, 'log_learning_rate_D_dagger': -1.2402502758500833, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(62.9402, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.8081, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3373, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3229, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3156, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3158, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 00:34:17,435] Trial 115 finished with value: 0.3350342810153961 and parameters: {'log_learning_rate': -1.376212840279642, 'log_learning_rate_D': -3.8971598816448187, 'log_learning_rate_D_dagger': -1.2402502758500833, 'training_batch_size': 8, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  191.6850163936615
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  116   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.63162156270907, 'log_learning_rate_D': -3.4128837025793484, 'log_learning_rate_D_dagger': -1.404700210415753, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(169.9560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4319, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3893, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3561, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3411, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 00:41:49,880] Trial 116 finished with value: 0.35603615641593933 and parameters: {'log_learning_rate': -1.63162156270907, 'log_learning_rate_D': -3.4128837025793484, 'log_learning_rate_D_dagger': -1.404700210415753, 'training_batch_size': 6, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  452.19125485420227
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  117   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1007132410725202, 'log_learning_rate_D': -3.7755905718513656, 'log_learning_rate_D_dagger': -1.0607667200760416, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(6.7978, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5909, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2989, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3089, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2927, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 00:46:28,803] Trial 117 finished with value: 0.313819020986557 and parameters: {'log_learning_rate': -1.1007132410725202, 'log_learning_rate_D': -3.7755905718513656, 'log_learning_rate_D_dagger': -1.0607667200760416, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  278.70228838920593
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  118   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0292256307785292, 'log_learning_rate_D': -3.7193173289731605, 'log_learning_rate_D_dagger': -1.2796456779104521, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(94.3644, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.9739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3435, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3184, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3140, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3141, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 00:51:07,850] Trial 118 finished with value: 0.33707648515701294 and parameters: {'log_learning_rate': -1.0292256307785292, 'log_learning_rate_D': -3.7193173289731605, 'log_learning_rate_D_dagger': -1.2796456779104521, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  278.79684376716614
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  119   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1411625126196017, 'log_learning_rate_D': -3.7991498838502835, 'log_learning_rate_D_dagger': -1.0878657486886472, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(78.5765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6275, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3898, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3447, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3010, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3008, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2979, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 00:55:47,341] Trial 119 finished with value: 0.3161143958568573 and parameters: {'log_learning_rate': -1.1411625126196017, 'log_learning_rate_D': -3.7991498838502835, 'log_learning_rate_D_dagger': -1.0878657486886472, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.2456958293915
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  120   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1429484202215932, 'log_learning_rate_D': -3.6242335802522945, 'log_learning_rate_D_dagger': -1.0677510350514725, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(195.2591, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.7092, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7348, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4254, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3697, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3654, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3895, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:00:26,664] Trial 120 finished with value: 0.3766930103302002 and parameters: {'log_learning_rate': -1.1429484202215932, 'log_learning_rate_D': -3.6242335802522945, 'log_learning_rate_D_dagger': -1.0677510350514725, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.09565806388855
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  121   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.250980133648323, 'log_learning_rate_D': -3.7981731061081865, 'log_learning_rate_D_dagger': -1.079046487920502, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(81.4594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.4487, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3474, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3180, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3037, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2941, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2947, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:05:06,207] Trial 121 finished with value: 0.31372928619384766 and parameters: {'log_learning_rate': -1.250980133648323, 'log_learning_rate_D': -3.7981731061081865, 'log_learning_rate_D_dagger': -1.079046487920502, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.292010307312
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  122   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2593238484803315, 'log_learning_rate_D': -3.7810639445706795, 'log_learning_rate_D_dagger': -1.1845724330513303, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(44.3334, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.4275, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4991, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3065, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3147, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2960, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2936, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:09:45,932] Trial 122 finished with value: 0.31170332431793213 and parameters: {'log_learning_rate': -1.2593238484803315, 'log_learning_rate_D': -3.7810639445706795, 'log_learning_rate_D_dagger': -1.1845724330513303, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.4736726284027
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  123   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2778562941824372, 'log_learning_rate_D': -3.9870171477900516, 'log_learning_rate_D_dagger': -1.3692099209646411, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(7.9956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6344, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3378, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3102, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3158, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3104, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:14:25,000] Trial 123 finished with value: 0.33050188422203064 and parameters: {'log_learning_rate': -1.2778562941824372, 'log_learning_rate_D': -3.9870171477900516, 'log_learning_rate_D_dagger': -1.3692099209646411, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  278.82966804504395
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  124   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0804601486077532, 'log_learning_rate_D': -3.7855894595692985, 'log_learning_rate_D_dagger': -1.214161852236808, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(295.6201, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.8598, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.1194, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5057, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.7834, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3563, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3441, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3446, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3250, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:19:04,716] Trial 124 finished with value: 0.3383323848247528 and parameters: {'log_learning_rate': -1.0804601486077532, 'log_learning_rate_D': -3.7855894595692985, 'log_learning_rate_D_dagger': -1.214161852236808, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.4808475971222
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  125   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2052851967753166, 'log_learning_rate_D': -3.8651963382948034, 'log_learning_rate_D_dagger': -1.4761532159969106, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(47.1683, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.8190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4156, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3370, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:23:44,679] Trial 125 finished with value: 0.3282491862773895 and parameters: {'log_learning_rate': -1.2052851967753166, 'log_learning_rate_D': -3.8651963382948034, 'log_learning_rate_D_dagger': -1.4761532159969106, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.73938393592834
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  126   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.376742878972662, 'log_learning_rate_D': -4.05944045848015, 'log_learning_rate_D_dagger': -1.07227193658316, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(28.0427, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[W 2024-03-12 01:24:02,304] Trial 126 failed with parameters: {'log_learning_rate': -1.376742878972662, 'log_learning_rate_D': -4.05944045848015, 'log_learning_rate_D_dagger': -1.07227193658316, 'training_batch_size': 7, 'training_p': 2} because of the following error: The value nan is not acceptable.
[W 2024-03-12 01:24:02,305] Trial 126 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  17.376457452774048
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  127   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3863042302534996, 'log_learning_rate_D': -4.088331724559789, 'log_learning_rate_D_dagger': -1.0739503429577142, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(46.3145, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.3706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3693, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3094, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3094, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3089, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:28:41,808] Trial 127 finished with value: 0.32839447259902954 and parameters: {'log_learning_rate': -1.3863042302534996, 'log_learning_rate_D': -4.088331724559789, 'log_learning_rate_D_dagger': -1.0739503429577142, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  279.2681312561035
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  128   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2475547931320243, 'log_learning_rate_D': -3.942936006182817, 'log_learning_rate_D_dagger': -1.2888012122778327, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(122.3830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.2386, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5824, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3913, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3520, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3302, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3149, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:33:19,618] Trial 128 finished with value: 0.33542537689208984 and parameters: {'log_learning_rate': -1.2475547931320243, 'log_learning_rate_D': -3.942936006182817, 'log_learning_rate_D_dagger': -1.2888012122778327, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  277.5683114528656
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  129   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.076260480950442, 'log_learning_rate_D': -3.772743433403159, 'log_learning_rate_D_dagger': -1.1769623446328774, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(95.3958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.2813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4084, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3796, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3605, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3141, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3141, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3129, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:37:58,677] Trial 129 finished with value: 0.33023160696029663 and parameters: {'log_learning_rate': -1.076260480950442, 'log_learning_rate_D': -3.772743433403159, 'log_learning_rate_D_dagger': -1.1769623446328774, 'training_batch_size': 7, 'training_p': 2}. Best is trial 109 with value: 0.31147700548171997.
Time for this trial:  278.83700799942017
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  130   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.466647377946538, 'log_learning_rate_D': -3.665435063791336, 'log_learning_rate_D_dagger': -1.5927007234260881, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(93.7745, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.0259, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.3879, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5849, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3215, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3093, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3037, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2962, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:42:36,473] Trial 130 finished with value: 0.3114100396633148 and parameters: {'log_learning_rate': -1.466647377946538, 'log_learning_rate_D': -3.665435063791336, 'log_learning_rate_D_dagger': -1.5927007234260881, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
res:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  277.5486087799072
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  131   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4509835605281305, 'log_learning_rate_D': -3.69055552245537, 'log_learning_rate_D_dagger': -1.6077244472102714, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(178.7198, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.2295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6928, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3603, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3431, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3330, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3227, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3192, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:47:13,336] Trial 131 finished with value: 0.3378088176250458 and parameters: {'log_learning_rate': -1.4509835605281305, 'log_learning_rate_D': -3.69055552245537, 'log_learning_rate_D_dagger': -1.6077244472102714, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  276.6166479587555
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  132   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0003571639466042, 'log_learning_rate_D': -3.871977368940878, 'log_learning_rate_D_dagger': -1.8339787782745594, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(42.9761, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.8757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(2.0293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.2038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.7271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4602, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3529, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4660, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3603, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:51:52,316] Trial 132 finished with value: 0.3883141875267029 and parameters: {'log_learning_rate': -1.0003571639466042, 'log_learning_rate_D': -3.871977368940878, 'log_learning_rate_D_dagger': -1.8339787782745594, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.73751950263977
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  133   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.355420101504624, 'log_learning_rate_D': -3.642172275791376, 'log_learning_rate_D_dagger': -1.761494249929459, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(56.7987, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.1249, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.9410, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5620, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4325, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4077, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3688, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 01:56:30,790] Trial 133 finished with value: 0.3598538041114807 and parameters: {'log_learning_rate': -1.355420101504624, 'log_learning_rate_D': -3.642172275791376, 'log_learning_rate_D_dagger': -1.761494249929459, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.22347617149353
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  134   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2641360636875854, 'log_learning_rate_D': -3.8196486764661906, 'log_learning_rate_D_dagger': -1.7114215419123835, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(188.1483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.5192, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.1185, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.8601, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5530, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4161, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4023, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:01:09,070] Trial 134 finished with value: 0.4276810586452484 and parameters: {'log_learning_rate': -1.2641360636875854, 'log_learning_rate_D': -3.8196486764661906, 'log_learning_rate_D_dagger': -1.7114215419123835, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.03909134864807
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  135   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5290919661324265, 'log_learning_rate_D': -3.990397121590071, 'log_learning_rate_D_dagger': -1.3657511805544644, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(122.8151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.1582, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3457, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3336, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3246, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3180, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3122, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:05:47,495] Trial 135 finished with value: 0.3323976695537567 and parameters: {'log_learning_rate': -1.5290919661324265, 'log_learning_rate_D': -3.990397121590071, 'log_learning_rate_D_dagger': -1.3657511805544644, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.1664915084839
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  136   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.104917945378181, 'log_learning_rate_D': -3.520402949134542, 'log_learning_rate_D_dagger': -1.0700798403546965, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(213.6632, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(5.8980, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0188, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4883, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4051, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3658, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3593, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3541, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:10:26,138] Trial 136 finished with value: 0.7202440500259399 and parameters: {'log_learning_rate': -1.104917945378181, 'log_learning_rate_D': -3.520402949134542, 'log_learning_rate_D_dagger': -1.0700798403546965, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.411500453949
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  137   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.170235312830897, 'log_learning_rate_D': -3.770645219241134, 'log_learning_rate_D_dagger': -1.4804624241731383, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(45.9755, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5530, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3877, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3197, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3098, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3109, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3148, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:15:04,519] Trial 137 finished with value: 0.32953861355781555 and parameters: {'log_learning_rate': -1.170235312830897, 'log_learning_rate_D': -3.770645219241134, 'log_learning_rate_D_dagger': -1.4804624241731383, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.1393804550171
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  138   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4399264972205787, 'log_learning_rate_D': -3.5803380883724887, 'log_learning_rate_D_dagger': -1.2413858394933912, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(107.5117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7379, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3288, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3158, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:19:43,317] Trial 138 finished with value: 0.33353495597839355 and parameters: {'log_learning_rate': -1.4399264972205787, 'log_learning_rate_D': -3.5803380883724887, 'log_learning_rate_D_dagger': -1.2413858394933912, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.572461605072
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  139   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3295637794955069, 'log_learning_rate_D': -3.6858852012925714, 'log_learning_rate_D_dagger': -1.135778523728165, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(90.8065, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.3279, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0250, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4003, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3832, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3688, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3430, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:22:53,550] Trial 139 finished with value: 0.34579014778137207 and parameters: {'log_learning_rate': -1.3295637794955069, 'log_learning_rate_D': -3.6858852012925714, 'log_learning_rate_D_dagger': -1.135778523728165, 'training_batch_size': 8, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  190.01349329948425
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  140   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.572550276097914, 'log_learning_rate_D': -3.2861333207920604, 'log_learning_rate_D_dagger': -1.3147622506982986, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(63.1613, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7747, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3586, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3109, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3109, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3106, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:27:32,166] Trial 140 finished with value: 0.329994261264801 and parameters: {'log_learning_rate': -1.572550276097914, 'log_learning_rate_D': -3.2861333207920604, 'log_learning_rate_D_dagger': -1.3147622506982986, 'training_batch_size': 7, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  278.3862030506134
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  141   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2274752366066093, 'log_learning_rate_D': -3.4336845275517422, 'log_learning_rate_D_dagger': -1.151914959578458, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(4.9817, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.3400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3090, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2980, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3041, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3031, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 02:35:04,376] Trial 141 finished with value: 0.3176330029964447 and parameters: {'log_learning_rate': -1.2274752366066093, 'log_learning_rate_D': -3.4336845275517422, 'log_learning_rate_D_dagger': -1.151914959578458, 'training_batch_size': 6, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  451.9563090801239
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  142   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4901682465520336, 'log_learning_rate_D': -3.4903851662299137, 'log_learning_rate_D_dagger': -1.0591864341590087, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(6.0756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3164, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3180, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3165, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3223, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3422, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3180, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3112, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 02:42:37,115] Trial 142 finished with value: 0.32838091254234314 and parameters: {'log_learning_rate': -1.4901682465520336, 'log_learning_rate_D': -3.4903851662299137, 'log_learning_rate_D_dagger': -1.0591864341590087, 'training_batch_size': 6, 'training_p': 2}. Best is trial 130 with value: 0.3114100396633148.
Time for this trial:  452.47886323928833
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  143   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8243130144478552, 'log_learning_rate_D': -3.613438354568236, 'log_learning_rate_D_dagger': -1.2178818173241277, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(9.5190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4652, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3611, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3188, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.2952, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2926, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2923, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:47:16,304] Trial 143 finished with value: 0.3113675117492676 and parameters: {'log_learning_rate': -1.8243130144478552, 'log_learning_rate_D': -3.613438354568236, 'log_learning_rate_D_dagger': -1.2178818173241277, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
res:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  278.96631622314453
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  4.0
--------------------  Trial  144   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.780677148001645, 'log_learning_rate_D': -3.624028905338091, 'log_learning_rate_D_dagger': -1.2266126209453039, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(92.7450, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.0268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3310, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3160, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:51:54,383] Trial 144 finished with value: 0.33472558856010437 and parameters: {'log_learning_rate': -1.780677148001645, 'log_learning_rate_D': -3.624028905338091, 'log_learning_rate_D_dagger': -1.2266126209453039, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.8505036830902
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  145   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3706861504400634, 'log_learning_rate_D': -3.89985046416424, 'log_learning_rate_D_dagger': -1.4234937648437267, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(123.3142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.3738, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6994, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4541, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3890, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3796, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3712, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 02:56:32,570] Trial 145 finished with value: 0.3754786550998688 and parameters: {'log_learning_rate': -1.3706861504400634, 'log_learning_rate_D': -3.89985046416424, 'log_learning_rate_D_dagger': -1.4234937648437267, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.94105553627014
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  146   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2794576540893874, 'log_learning_rate_D': -3.7495469220438746, 'log_learning_rate_D_dagger': -1.0013607038777006, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(28.7933, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3429, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3160, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3179, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3126, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3147, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:01:10,055] Trial 146 finished with value: 0.33037281036376953 and parameters: {'log_learning_rate': -1.2794576540893874, 'log_learning_rate_D': -3.7495469220438746, 'log_learning_rate_D_dagger': -1.0013607038777006, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.24794697761536
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  147   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6256432823619238, 'log_learning_rate_D': -4.1638731257711665, 'log_learning_rate_D_dagger': -1.5535094116057033, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(3.4408, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3848, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3681, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3399, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3153, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:05:48,600] Trial 147 finished with value: 0.33282455801963806 and parameters: {'log_learning_rate': -1.6256432823619238, 'log_learning_rate_D': -4.1638731257711665, 'log_learning_rate_D_dagger': -1.5535094116057033, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.27981996536255
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  148   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1740242075432377, 'log_learning_rate_D': -3.364827998682819, 'log_learning_rate_D_dagger': -1.3147561501258425, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(316.1046, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.1894, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.3455, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.7205, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5172, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4240, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:10:27,181] Trial 148 finished with value: 0.370085746049881 and parameters: {'log_learning_rate': -1.1740242075432377, 'log_learning_rate_D': -3.364827998682819, 'log_learning_rate_D_dagger': -1.3147561501258425, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.352205991745
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  149   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4337033517315974, 'log_learning_rate_D': -3.5593455227926847, 'log_learning_rate_D_dagger': -1.183096377510202, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(41.5543, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5913, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4525, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4243, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4074, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4073, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3727, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3722, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:15:05,442] Trial 149 finished with value: 0.3714078366756439 and parameters: {'log_learning_rate': -1.4337033517315974, 'log_learning_rate_D': -3.5593455227926847, 'log_learning_rate_D_dagger': -1.183096377510202, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.02072954177856
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  150   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8308910891611452, 'log_learning_rate_D': -4.039410877316364, 'log_learning_rate_D_dagger': -1.6518396432343228, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(45.7343, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(7.2154, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(2.9745, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.5650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.8079, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4673, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4275, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3904, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3627, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3411, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:18:17,234] Trial 150 finished with value: 0.3435964584350586 and parameters: {'log_learning_rate': -1.8308910891611452, 'log_learning_rate_D': -4.039410877316364, 'log_learning_rate_D_dagger': -1.6518396432343228, 'training_batch_size': 8, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  191.56020545959473
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  151   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5826282771632385, 'log_learning_rate_D': -3.460680967722889, 'log_learning_rate_D_dagger': -1.1135394717656144, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(83.9857, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4947, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3113, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3109, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 03:25:50,728] Trial 151 finished with value: 0.33220115303993225 and parameters: {'log_learning_rate': -1.5826282771632385, 'log_learning_rate_D': -3.460680967722889, 'log_learning_rate_D_dagger': -1.1135394717656144, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  453.25114274024963
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  152   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6801297867646756, 'log_learning_rate_D': -3.673466533493406, 'log_learning_rate_D_dagger': -1.2695639349194316, 'training_batch_size': 12, 'training_p': 2}
	 epoch  0 training error:  tensor(36.6174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(1.0247, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.7959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.8782, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.3772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3471, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3296, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3196, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 03:27:36,464] Trial 152 finished with value: 0.33493369817733765 and parameters: {'log_learning_rate': -1.6801297867646756, 'log_learning_rate_D': -3.673466533493406, 'log_learning_rate_D_dagger': -1.2695639349194316, 'training_batch_size': 12, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  105.50202798843384
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  153   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9058895208306708, 'log_learning_rate_D': -3.3288925082693153, 'log_learning_rate_D_dagger': -1.1499874301526714, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(119.4071, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4755, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4725, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4718, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4730, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 03:35:07,087] Trial 153 finished with value: 0.35572493076324463 and parameters: {'log_learning_rate': -1.9058895208306708, 'log_learning_rate_D': -3.3288925082693153, 'log_learning_rate_D_dagger': -1.1499874301526714, 'training_batch_size': 6, 'training_p': 5}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  450.34896969795227
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  154   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0916796882323239, 'log_learning_rate_D': -3.5102671645993264, 'log_learning_rate_D_dagger': -1.4068947221387829, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(89.0310, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.3885, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5534, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3667, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3325, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2957, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:39:45,876] Trial 154 finished with value: 0.3164287805557251 and parameters: {'log_learning_rate': -1.0916796882323239, 'log_learning_rate_D': -3.5102671645993264, 'log_learning_rate_D_dagger': -1.4068947221387829, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.56189465522766
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  155   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.142425039180154, 'log_learning_rate_D': -3.611522480469878, 'log_learning_rate_D_dagger': -1.4125598089749463, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(57.2811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.3944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5842, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4512, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3723, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3480, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3281, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3160, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3136, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:44:24,770] Trial 155 finished with value: 0.33504626154899597 and parameters: {'log_learning_rate': -1.142425039180154, 'log_learning_rate_D': -3.611522480469878, 'log_learning_rate_D_dagger': -1.4125598089749463, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.63244342803955
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  156   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0894618183345461, 'log_learning_rate_D': -3.818175367252291, 'log_learning_rate_D_dagger': -1.4764780702883735, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(74.1191, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7385, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3310, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3107, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3155, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:49:03,383] Trial 156 finished with value: 0.3380758464336395 and parameters: {'log_learning_rate': -1.0894618183345461, 'log_learning_rate_D': -3.818175367252291, 'log_learning_rate_D_dagger': -1.4764780702883735, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.3895945549011
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  157   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.319076920622936, 'log_learning_rate_D': -3.525149999099945, 'log_learning_rate_D_dagger': -1.3402214245045658, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(68.3742, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.8200, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4153, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3586, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3421, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3186, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3126, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:53:42,413] Trial 157 finished with value: 0.33585217595100403 and parameters: {'log_learning_rate': -1.319076920622936, 'log_learning_rate_D': -3.525149999099945, 'log_learning_rate_D_dagger': -1.3402214245045658, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.7877881526947
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  158   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0120963010647714, 'log_learning_rate_D': -3.722894787033828, 'log_learning_rate_D_dagger': -2.665184640599593, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(15.2055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(5.4578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.6833, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.4508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(1.3356, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.2247, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.1156, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.0128, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.9170, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.8295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 03:58:20,476] Trial 158 finished with value: 0.7945346236228943 and parameters: {'log_learning_rate': -1.0120963010647714, 'log_learning_rate_D': -3.722894787033828, 'log_learning_rate_D_dagger': -2.665184640599593, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.8168523311615
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  159   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.23084306417863, 'log_learning_rate_D': -3.9280476678650027, 'log_learning_rate_D_dagger': -1.5493847071255382, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(301.9269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(7.6416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.9700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4370, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4175, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4004, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3864, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3625, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3528, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:02:58,775] Trial 159 finished with value: 0.3585068881511688 and parameters: {'log_learning_rate': -1.23084306417863, 'log_learning_rate_D': -3.9280476678650027, 'log_learning_rate_D_dagger': -1.5493847071255382, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.04671335220337
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  160   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3866780394763134, 'log_learning_rate_D': -3.4172165124717075, 'log_learning_rate_D_dagger': -1.9536583346640708, 'training_batch_size': 8, 'training_p': 2}
	 epoch  0 training error:  tensor(207.3478, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(99.6700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(32.6479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(6.3497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(2.5538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.7303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.4851, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.2804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(1.1065, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.9538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:06:11,118] Trial 160 finished with value: 0.8717981576919556 and parameters: {'log_learning_rate': -1.3866780394763134, 'log_learning_rate_D': -3.4172165124717075, 'log_learning_rate_D_dagger': -1.9536583346640708, 'training_batch_size': 8, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  192.09345197677612
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  161   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.518146834807434, 'log_learning_rate_D': -3.472076505285268, 'log_learning_rate_D_dagger': -1.2046671565796123, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(32.9383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[W 2024-03-12 04:06:21,018] Trial 161 failed with parameters: {'log_learning_rate': -1.518146834807434, 'log_learning_rate_D': -3.472076505285268, 'log_learning_rate_D_dagger': -1.2046671565796123, 'training_batch_size': 6, 'training_p': 2} because of the following error: The value nan is not acceptable.
[W 2024-03-12 04:06:21,018] Trial 161 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  9.661046743392944
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  162   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7592535183421418, 'log_learning_rate_D': -3.4719385954115434, 'log_learning_rate_D_dagger': -1.234168823043215, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(69.8299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3359, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3113, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 04:13:53,177] Trial 162 finished with value: 0.33383145928382874 and parameters: {'log_learning_rate': -1.7592535183421418, 'log_learning_rate_D': -3.4719385954115434, 'log_learning_rate_D_dagger': -1.234168823043215, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.9023587703705
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  163   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4990814022290966, 'log_learning_rate_D': -3.209763861072696, 'log_learning_rate_D_dagger': -1.1093421707190712, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(30.6550, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.1440, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3106, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3107, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3135, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3143, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:18:29,820] Trial 163 finished with value: 0.33160200715065 and parameters: {'log_learning_rate': -1.4990814022290966, 'log_learning_rate_D': -3.209763861072696, 'log_learning_rate_D_dagger': -1.1093421707190712, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  276.38467240333557
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  164   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.280473966279468, 'log_learning_rate_D': -3.555565789395832, 'log_learning_rate_D_dagger': -1.0085160877263588, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(200.3495, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.7731, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4341, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3544, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3331, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3035, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3646, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:23:08,391] Trial 164 finished with value: 0.34741729497909546 and parameters: {'log_learning_rate': -1.280473966279468, 'log_learning_rate_D': -3.555565789395832, 'log_learning_rate_D_dagger': -1.0085160877263588, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.316437959671
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  165   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1010362901101232, 'log_learning_rate_D': -3.289659844005058, 'log_learning_rate_D_dagger': -1.1907849612845178, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(15.2051, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4459, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3326, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3137, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3136, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3136, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3191, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:27:46,778] Trial 165 finished with value: 0.34147384762763977 and parameters: {'log_learning_rate': -1.1010362901101232, 'log_learning_rate_D': -3.289659844005058, 'log_learning_rate_D_dagger': -1.1907849612845178, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.1498146057129
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  166   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1929920937550151, 'log_learning_rate_D': -3.635521821256546, 'log_learning_rate_D_dagger': -1.3685775923852748, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(31.9654, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.5578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4367, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4197, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3913, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 04:35:18,485] Trial 166 finished with value: 0.3207894265651703 and parameters: {'log_learning_rate': -1.1929920937550151, 'log_learning_rate_D': -3.635521821256546, 'log_learning_rate_D_dagger': -1.3685775923852748, 'training_batch_size': 6, 'training_p': 3}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.4301595687866
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  167   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0660072328429697, 'log_learning_rate_D': -3.3974154301529076, 'log_learning_rate_D_dagger': -1.2705485021407734, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(149.7281, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.2831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5681, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4807, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4446, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4132, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3945, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3750, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:39:56,378] Trial 167 finished with value: 0.37596550583839417 and parameters: {'log_learning_rate': -2.0660072328429697, 'log_learning_rate_D': -3.3974154301529076, 'log_learning_rate_D_dagger': -1.2705485021407734, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.6312599182129
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  168   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5415914071920354, 'log_learning_rate_D': -3.7833125290330107, 'log_learning_rate_D_dagger': -1.0794453108700854, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(25.9423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6169, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3784, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3734, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3548, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3335, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3113, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3033, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 04:47:27,194] Trial 168 finished with value: 0.31312987208366394 and parameters: {'log_learning_rate': -1.5415914071920354, 'log_learning_rate_D': -3.7833125290330107, 'log_learning_rate_D_dagger': -1.0794453108700854, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  450.5310769081116
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  169   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.410127163658937, 'log_learning_rate_D': -3.776537834475881, 'log_learning_rate_D_dagger': -1.08050827037518, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(35.2615, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5071, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4688, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4010, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4037, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3337, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3005, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3200, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 04:52:06,132] Trial 169 finished with value: 0.31353759765625 and parameters: {'log_learning_rate': -1.410127163658937, 'log_learning_rate_D': -3.776537834475881, 'log_learning_rate_D_dagger': -1.08050827037518, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.702517747879
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  170   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5251543777037826, 'log_learning_rate_D': -3.798756139563972, 'log_learning_rate_D_dagger': -1.0513079404426415, 'training_batch_size': 9, 'training_p': 2}
	 epoch  0 training error:  tensor(71.4132, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(12.5554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(5.4499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.5905, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.6766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.7389, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4007, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4170, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.5517, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 04:53:50,892] Trial 170 finished with value: 0.46471044421195984 and parameters: {'log_learning_rate': -1.5251543777037826, 'log_learning_rate_D': -3.798756139563972, 'log_learning_rate_D_dagger': -1.0513079404426415, 'training_batch_size': 9, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  104.54253458976746
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  171   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4199412221440828, 'log_learning_rate_D': -3.874596862961452, 'log_learning_rate_D_dagger': -1.0743579539361563, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(185.4075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.4114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3694, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3850, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3607, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3525, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 05:01:22,947] Trial 171 finished with value: 0.33437228202819824 and parameters: {'log_learning_rate': -1.4199412221440828, 'log_learning_rate_D': -3.874596862961452, 'log_learning_rate_D_dagger': -1.0743579539361563, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.80129981040955
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  172   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3332498488415083, 'log_learning_rate_D': -3.678427816502559, 'log_learning_rate_D_dagger': -1.1819355472999336, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(59.6519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.0532, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3575, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3163, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3097, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3093, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3072, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3057, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2989, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:06:01,431] Trial 172 finished with value: 0.31301555037498474 and parameters: {'log_learning_rate': -1.3332498488415083, 'log_learning_rate_D': -3.678427816502559, 'log_learning_rate_D_dagger': -1.1819355472999336, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.23046684265137
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  173   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3433902685631969, 'log_learning_rate_D': -3.722837509459308, 'log_learning_rate_D_dagger': -1.1695828332155307, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(54.7906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4201, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3626, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3422, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3081, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2998, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:10:40,324] Trial 173 finished with value: 0.31171324849128723 and parameters: {'log_learning_rate': -1.3433902685631969, 'log_learning_rate_D': -3.722837509459308, 'log_learning_rate_D_dagger': -1.1695828332155307, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.66281819343567
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  174   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3485475082260077, 'log_learning_rate_D': -3.715628652704558, 'log_learning_rate_D_dagger': -1.190710708065865, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(169.5096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.6843, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.9285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3848, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3880, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3309, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3203, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:15:18,499] Trial 174 finished with value: 0.33618178963661194 and parameters: {'log_learning_rate': -1.3485475082260077, 'log_learning_rate_D': -3.715628652704558, 'log_learning_rate_D_dagger': -1.190710708065865, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.952849149704
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  175   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6116887830200206, 'log_learning_rate_D': -3.689056812310277, 'log_learning_rate_D_dagger': -1.2813755176891048, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(94.9000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.3080, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3879, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3430, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3359, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3270, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.6329, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:19:56,752] Trial 175 finished with value: 0.3220035135746002 and parameters: {'log_learning_rate': -1.6116887830200206, 'log_learning_rate_D': -3.689056812310277, 'log_learning_rate_D_dagger': -1.2813755176891048, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.02484798431396
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  176   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.448878151012765, 'log_learning_rate_D': -3.60913790289692, 'log_learning_rate_D_dagger': -1.0037995405335918, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(94.6572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.8453, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4496, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3557, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:24:34,684] Trial 176 finished with value: 0.3297235667705536 and parameters: {'log_learning_rate': -1.448878151012765, 'log_learning_rate_D': -3.60913790289692, 'log_learning_rate_D_dagger': -1.0037995405335918, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.6978313922882
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  177   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5355260621797506, 'log_learning_rate_D': -3.7360863009763285, 'log_learning_rate_D_dagger': -1.1461915586826181, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(145.1354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(4.8404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3440, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.2974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.2961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:29:12,780] Trial 177 finished with value: 0.313369482755661 and parameters: {'log_learning_rate': -1.5355260621797506, 'log_learning_rate_D': -3.7360863009763285, 'log_learning_rate_D_dagger': -1.1461915586826181, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.8567867279053
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  178   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.556620421410414, 'log_learning_rate_D': -3.98805274234091, 'log_learning_rate_D_dagger': -1.1368316480638385, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(212.3151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.0409, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.4775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3895, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3755, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3225, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:33:51,136] Trial 178 finished with value: 0.3350469768047333 and parameters: {'log_learning_rate': -1.556620421410414, 'log_learning_rate_D': -3.98805274234091, 'log_learning_rate_D_dagger': -1.1368316480638385, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.092166185379
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  179   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4873964549185832, 'log_learning_rate_D': -3.8610431599610324, 'log_learning_rate_D_dagger': -1.1873680528070598, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(197.5948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.3573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6026, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4386, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4037, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3743, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3261, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:38:29,086] Trial 179 finished with value: 0.33014509081840515 and parameters: {'log_learning_rate': -1.4873964549185832, 'log_learning_rate_D': -3.8610431599610324, 'log_learning_rate_D_dagger': -1.1873680528070598, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.71401023864746
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  180   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.38059735104346, 'log_learning_rate_D': -3.7595813542240575, 'log_learning_rate_D_dagger': -1.1078110589231966, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(152.4773, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.2529, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3377, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3175, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3126, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 05:43:06,944] Trial 180 finished with value: 0.33100631833076477 and parameters: {'log_learning_rate': -1.38059735104346, 'log_learning_rate_D': -3.7595813542240575, 'log_learning_rate_D_dagger': -1.1078110589231966, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.61987924575806
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  181   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6624450063826735, 'log_learning_rate_D': -3.9369525319821745, 'log_learning_rate_D_dagger': -2.145391374579698, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(4.0681, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.3888, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3676, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3415, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3226, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3148, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2946, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3089, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 05:50:39,039] Trial 181 finished with value: 0.313763827085495 and parameters: {'log_learning_rate': -1.6624450063826735, 'log_learning_rate_D': -3.9369525319821745, 'log_learning_rate_D_dagger': -2.145391374579698, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.84229946136475
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  182   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6581977738661235, 'log_learning_rate_D': -3.9004721997753395, 'log_learning_rate_D_dagger': -1.9608072873348972, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(129.1245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.1265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4126, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3600, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3373, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 05:58:10,228] Trial 182 finished with value: 0.3190380036830902 and parameters: {'log_learning_rate': -1.6581977738661235, 'log_learning_rate_D': -3.9004721997753395, 'log_learning_rate_D_dagger': -1.9608072873348972, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  450.9445044994354
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  183   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5494888007839867, 'log_learning_rate_D': -4.060395361960671, 'log_learning_rate_D_dagger': -2.294608039343729, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(93.8204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(12.7756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(3.1348, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(2.7143, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(2.2901, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.8912, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.5346, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.2386, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.9901, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.7919, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:05:41,483] Trial 183 finished with value: 0.6905274391174316 and parameters: {'log_learning_rate': -1.5494888007839867, 'log_learning_rate_D': -4.060395361960671, 'log_learning_rate_D_dagger': -2.294608039343729, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  450.9860153198242
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  184   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4172693140434938, 'log_learning_rate_D': -3.729967916146382, 'log_learning_rate_D_dagger': -2.005979918282725, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(81.9658, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.0735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.6423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4858, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3256, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3010, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2996, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2967, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:13:12,365] Trial 184 finished with value: 0.3137713074684143 and parameters: {'log_learning_rate': -1.4172693140434938, 'log_learning_rate_D': -3.729967916146382, 'log_learning_rate_D_dagger': -2.005979918282725, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  450.62385535240173
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  185   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8273918466593766, 'log_learning_rate_D': -3.748000962419737, 'log_learning_rate_D_dagger': -1.722161915993818, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(77.3853, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.6856, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4389, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3283, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:20:45,034] Trial 185 finished with value: 0.33550092577934265 and parameters: {'log_learning_rate': -1.8273918466593766, 'log_learning_rate_D': -3.748000962419737, 'log_learning_rate_D_dagger': -1.722161915993818, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.40648555755615
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  186   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7487424110158996, 'log_learning_rate_D': -3.949463967289382, 'log_learning_rate_D_dagger': -2.007121506840816, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(251.9448, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(38.9288, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.7638, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3369, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3434, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3649, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3787, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3239, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:28:18,350] Trial 186 finished with value: 0.3418193459510803 and parameters: {'log_learning_rate': -1.7487424110158996, 'log_learning_rate_D': -3.949463967289382, 'log_learning_rate_D_dagger': -2.007121506840816, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  453.0381107330322
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  187   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6068824659689809, 'log_learning_rate_D': -3.829221981255465, 'log_learning_rate_D_dagger': -2.08600989234072, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(7.6097, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.2970, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5169, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5101, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5089, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5207, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:35:49,542] Trial 187 finished with value: 0.4013136029243469 and parameters: {'log_learning_rate': -1.6068824659689809, 'log_learning_rate_D': -3.829221981255465, 'log_learning_rate_D_dagger': -2.08600989234072, 'training_batch_size': 6, 'training_p': 6}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  450.91084575653076
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  188   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2870271856644626, 'log_learning_rate_D': -3.6933854030962867, 'log_learning_rate_D_dagger': -1.8022695093416454, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(16.8000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.3777, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.3483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3381, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3246, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3341, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2942, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3025, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:43:21,459] Trial 188 finished with value: 0.3136453330516815 and parameters: {'log_learning_rate': -1.2870271856644626, 'log_learning_rate_D': -3.6933854030962867, 'log_learning_rate_D_dagger': -1.8022695093416454, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.6334948539734
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  189   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.313042328100729, 'log_learning_rate_D': -3.6665953680643644, 'log_learning_rate_D_dagger': -1.8708400169377826, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(24.6615, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.8967, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.8134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4967, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3670, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3877, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3209, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:50:53,844] Trial 189 finished with value: 0.3668215870857239 and parameters: {'log_learning_rate': -1.313042328100729, 'log_learning_rate_D': -3.6665953680643644, 'log_learning_rate_D_dagger': -1.8708400169377826, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.08053612709045
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  190   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4456588055318118, 'log_learning_rate_D': -3.7827977285879184, 'log_learning_rate_D_dagger': -1.8097535355766536, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(222.1124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.7975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5920, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5164, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4672, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4792, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4278, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3786, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 06:58:26,568] Trial 190 finished with value: 0.5199081301689148 and parameters: {'log_learning_rate': -1.4456588055318118, 'log_learning_rate_D': -3.7827977285879184, 'log_learning_rate_D_dagger': -1.8097535355766536, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.4509496688843
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  191   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.30881655095558, 'log_learning_rate_D': -3.693189734885161, 'log_learning_rate_D_dagger': -1.8758179651328248, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(128.0912, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.6038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.9851, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.1442, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6491, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3649, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3396, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4102, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3388, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3036, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:05:59,283] Trial 191 finished with value: 0.39743655920028687 and parameters: {'log_learning_rate': -1.30881655095558, 'log_learning_rate_D': -3.693189734885161, 'log_learning_rate_D_dagger': -1.8758179651328248, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.45317101478577
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  192   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.946494105513496, 'log_learning_rate_D': -3.8757147005325394, 'log_learning_rate_D_dagger': -2.0796460737822766, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(95.6862, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.5052, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.1124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.8461, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4349, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3829, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3300, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3610, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:13:31,714] Trial 192 finished with value: 0.3220616579055786 and parameters: {'log_learning_rate': -1.946494105513496, 'log_learning_rate_D': -3.8757147005325394, 'log_learning_rate_D_dagger': -2.0796460737822766, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.16779017448425
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  193   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.368465466849895, 'log_learning_rate_D': -3.6145922790014597, 'log_learning_rate_D_dagger': -1.7691858492957648, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(144.4356, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5185, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4472, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4179, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3909, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3696, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3528, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3454, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:21:04,401] Trial 193 finished with value: 0.3291531205177307 and parameters: {'log_learning_rate': -1.368465466849895, 'log_learning_rate_D': -3.6145922790014597, 'log_learning_rate_D_dagger': -1.7691858492957648, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.41443610191345
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  194   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.464557303118619, 'log_learning_rate_D': -3.735092055993765, 'log_learning_rate_D_dagger': -2.2805064692030577, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(181.9635, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(67.0959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(16.4942, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6229, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3924, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3857, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3820, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3747, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:28:37,273] Trial 194 finished with value: 0.39045950770378113 and parameters: {'log_learning_rate': -1.464557303118619, 'log_learning_rate_D': -3.735092055993765, 'log_learning_rate_D_dagger': -2.2805064692030577, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.6586580276489
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  195   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2290961313181008, 'log_learning_rate_D': -3.955338807335936, 'log_learning_rate_D_dagger': -1.6308872836090789, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(89.2883, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.9818, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3713, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3325, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3186, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:36:10,053] Trial 195 finished with value: 0.3422454297542572 and parameters: {'log_learning_rate': -1.2290961313181008, 'log_learning_rate_D': -3.955338807335936, 'log_learning_rate_D_dagger': -1.6308872836090789, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.53187227249146
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  196   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1861600693366823, 'log_learning_rate_D': -3.5963987326122866, 'log_learning_rate_D_dagger': -2.1571600931206882, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(122.0674, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.6124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.4409, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.0339, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.7631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4834, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3259, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:43:40,206] Trial 196 finished with value: 0.3130832016468048 and parameters: {'log_learning_rate': -2.1861600693366823, 'log_learning_rate_D': -3.5963987326122866, 'log_learning_rate_D_dagger': -2.1571600931206882, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  449.908793926239
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  197   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5195672682452368, 'log_learning_rate_D': -3.6281278613192347, 'log_learning_rate_D_dagger': -2.155285002744647, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(64.5095, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.7595, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0661, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6729, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4104, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3516, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4109, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3667, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 07:51:12,276] Trial 197 finished with value: 0.5308778882026672 and parameters: {'log_learning_rate': -1.5195672682452368, 'log_learning_rate_D': -3.6281278613192347, 'log_learning_rate_D_dagger': -2.155285002744647, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.8224391937256
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  198   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1593971309590203, 'log_learning_rate_D': -3.791624032584258, 'log_learning_rate_D_dagger': -2.1134090997121353, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(50.2524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(6.8293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.1897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.6691, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4687, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4169, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3997, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3865, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3744, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 07:55:51,428] Trial 198 finished with value: 0.3879033029079437 and parameters: {'log_learning_rate': -2.1593971309590203, 'log_learning_rate_D': -3.791624032584258, 'log_learning_rate_D_dagger': -2.1134090997121353, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  278.90944743156433
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  199   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6888454446683365, 'log_learning_rate_D': -3.578088052912959, 'log_learning_rate_D_dagger': -3.2783824768936056, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(153.4296, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(138.2833, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(124.1104, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(110.9810, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(98.8248, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(87.6660, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(77.4968, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(67.8770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(58.8264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(50.3056, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 08:03:23,002] Trial 199 finished with value: 38.32297897338867 and parameters: {'log_learning_rate': -1.6888454446683365, 'log_learning_rate_D': -3.578088052912959, 'log_learning_rate_D_dagger': -3.2783824768936056, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.3196222782135
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  200   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.388920640263579, 'log_learning_rate_D': -3.6973661861783236, 'log_learning_rate_D_dagger': -1.2424932609621724, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(274.3675, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(7.6585, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.1006, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4044, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3913, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3751, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3687, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3710, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 08:08:00,812] Trial 200 finished with value: 0.38790398836135864 and parameters: {'log_learning_rate': -1.388920640263579, 'log_learning_rate_D': -3.6973661861783236, 'log_learning_rate_D_dagger': -1.2424932609621724, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.56130480766296
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  201   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2562517718616875, 'log_learning_rate_D': -3.8438111942200357, 'log_learning_rate_D_dagger': -2.1992619459660596, 'training_batch_size': 7, 'training_p': 2}
	 epoch  0 training error:  tensor(77.8114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(19.1766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(3.7360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(2.5481, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(2.2271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.9642, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.7131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.4838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(1.2816, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(1.1064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 08:12:38,736] Trial 201 finished with value: 1.0372536182403564 and parameters: {'log_learning_rate': -1.2562517718616875, 'log_learning_rate_D': -3.8438111942200357, 'log_learning_rate_D_dagger': -2.1992619459660596, 'training_batch_size': 7, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  277.6577105522156
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  4.0
--------------------  Trial  202   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5967837009562613, 'log_learning_rate_D': -3.5327992499031176, 'log_learning_rate_D_dagger': -2.01175435145993, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(68.7929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.0353, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5173, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4373, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4162, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 08:20:10,226] Trial 202 finished with value: 0.39476802945137024 and parameters: {'log_learning_rate': -1.5967837009562613, 'log_learning_rate_D': -3.5327992499031176, 'log_learning_rate_D_dagger': -2.01175435145993, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.2162961959839
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  203   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.040078385119704, 'log_learning_rate_D': -3.671932131916824, 'log_learning_rate_D_dagger': -1.9532848050204448, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(177.6005, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(8.6401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.7064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4852, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.3842, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.2988, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.2941, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.2963, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 08:27:42,212] Trial 203 finished with value: 0.3212301731109619 and parameters: {'log_learning_rate': -2.040078385119704, 'log_learning_rate_D': -3.671932131916824, 'log_learning_rate_D_dagger': -1.9532848050204448, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  451.7421052455902
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
--------------------  Trial  204   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9205282609437593, 'log_learning_rate_D': -3.7491681751564947, 'log_learning_rate_D_dagger': -2.246786952644779, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(36.1594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  10 training error:  tensor(5.0558, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  20 training error:  tensor(2.9956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.9457, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  40 training error:  tensor(1.3108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.8120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3568, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3402, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  4.0
[I 2024-03-12 08:35:14,735] Trial 204 finished with value: 0.33781182765960693 and parameters: {'log_learning_rate': -1.9205282609437593, 'log_learning_rate_D': -3.7491681751564947, 'log_learning_rate_D_dagger': -2.246786952644779, 'training_batch_size': 6, 'training_p': 2}. Best is trial 143 with value: 0.3113675117492676.
Time for this trial:  452.2549033164978
Memory status after this trial: 
Memory allocated:  3.4521484375
Memory cached:  4.0
