/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[I 2024-02-27 17:07:39,709] Using an existing study with name 'my_study' instead of creating a new one.
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
--------------------  Trial  63   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -2.4561951263268527, 'log_learning_rate_D': -4.821982640902222, 'log_learning_rate_D_dagger': -3.231943751821562, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.7170, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  30.62744140625
Memory cached:  386.0
[I 2024-02-27 17:08:17,151] Trial 63 finished with value: 0.3165755271911621 and parameters: {'W_layers': 2, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -2.4561951263268527, 'log_learning_rate_D': -4.821982640902222, 'log_learning_rate_D_dagger': -3.231943751821562, 'training_batch_size': 6, 'training_p': 5}. Best is trial 11 with value: 0.0255799051374197.
[I 2024-02-27 17:08:17,229] Using an existing study with name 'my_study' instead of creating a new one.
res:  tensor(0.3166, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  36.88928461074829
Memory status after this trial: 
Memory allocated:  2014.236328125
Memory cached:  2066.0
--------------------  Trial  2   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 6, 'D_layers': 7, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 5, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 8, 'log_learning_rate': -2.9568734924460074, 'log_learning_rate_D': -1.5335209929088163, 'log_learning_rate_D_dagger': -3.6937834124335724, 'training_batch_size': 12, 'training_p': 5}
/home/shengduo/anaconda3/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
	 epoch  0 training error:  tensor(1.0110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  51.1435546875
Memory cached:  88.0
[I 2024-02-27 17:08:19,450] Trial 2 finished with value: 8.452004432678223 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 6, 'D_layers': 7, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 5, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 8, 'log_learning_rate': -2.9568734924460074, 'log_learning_rate_D': -1.5335209929088163, 'log_learning_rate_D_dagger': -3.6937834124335724, 'training_batch_size': 12, 'training_p': 5}. Best is trial 0 with value: 0.22980225086212158.
res:  tensor(8.4520, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  1.956130027770996
Memory status after this trial: 
Memory allocated:  2821.28125
Memory cached:  2910.0
