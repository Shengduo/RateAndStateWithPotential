/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[I 2024-02-28 11:33:31,884] A new study created in RDB with name: my_study1
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
--------------------  Trial  0   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -1.61874309714424, 'log_learning_rate_D': -1.931815023861323, 'log_learning_rate_D_dagger': -2.3690036615960337, 'training_batch_size': 9, 'training_p': 7}
	 epoch  0 training error:  tensor(9.3246, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  5.61181640625
Memory cached:  100.0
[I 2024-02-28 11:33:46,503] Trial 0 finished with value: 16.19424819946289 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -1.61874309714424, 'log_learning_rate_D': -1.931815023861323, 'log_learning_rate_D_dagger': -2.3690036615960337, 'training_batch_size': 9, 'training_p': 7}. Best is trial 0 with value: 16.19424819946289.
res:  tensor(16.1942, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  14.355716943740845
Memory status after this trial: 
Memory allocated:  1137.099609375
Memory cached:  1146.0
--------------------  Trial  1   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'W_layer_units_exponent_7': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 4, 'D_layer_units_exponent_4': 7, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 8, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -2.6489091655676407, 'log_learning_rate_D': -4.0124313815373505, 'log_learning_rate_D_dagger': -2.0402908421068426, 'training_batch_size': 10, 'training_p': 8}
	 epoch  0 training error:  tensor(1.0082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1147.24169921875
Memory cached:  1278.0
[I 2024-02-28 11:33:56,749] Trial 1 finished with value: 3.2097232341766357 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'W_layer_units_exponent_7': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 4, 'D_layer_units_exponent_4': 7, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 8, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -2.6489091655676407, 'log_learning_rate_D': -4.0124313815373505, 'log_learning_rate_D_dagger': -2.0402908421068426, 'training_batch_size': 10, 'training_p': 8}. Best is trial 1 with value: 3.2097232341766357.
[I 2024-02-28 11:33:56,912] A new study created in RDB with name: my_study1
res:  tensor(3.2097, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(16.1942, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  9.932390213012695
Memory status after this trial: 
Memory allocated:  1914.943359375
Memory cached:  2058.0
--------------------  Trial  0   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 4, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 6, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 4, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -1.9418603911650778, 'log_learning_rate_D': -1.3629056543706262, 'log_learning_rate_D_dagger': -1.345318503839683, 'training_batch_size': 12, 'training_p': 2}
/home/shengduo/anaconda3/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
	 epoch  0 training error:  tensor(0.9916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  12.57568359375
Memory cached:  30.0
[I 2024-02-28 11:33:58,361] Trial 0 finished with value: 37.53935623168945 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 4, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 6, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 4, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -1.9418603911650778, 'log_learning_rate_D': -1.3629056543706262, 'log_learning_rate_D_dagger': -1.345318503839683, 'training_batch_size': 12, 'training_p': 2}. Best is trial 0 with value: 37.53935623168945.
res:  tensor(37.5394, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  1.1467063426971436
Memory status after this trial: 
Memory allocated:  1109.376953125
Memory cached:  1138.0
--------------------  Trial  1   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 5, 'D_layers': 2, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 6, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 8, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -3.3798385637235975, 'log_learning_rate_D': -3.8241580453592787, 'log_learning_rate_D_dagger': -1.4607523329163783, 'training_batch_size': 10, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1116.13330078125
Memory cached:  1142.0
[I 2024-02-28 11:33:59,659] Trial 1 finished with value: 3.132847785949707 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 5, 'D_layers': 2, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 6, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 8, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -3.3798385637235975, 'log_learning_rate_D': -3.8241580453592787, 'log_learning_rate_D_dagger': -1.4607523329163783, 'training_batch_size': 10, 'training_p': 5}. Best is trial 1 with value: 3.132847785949707.
res:  tensor(3.1328, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(37.5394, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  0.9974656105041504
Memory status after this trial: 
Memory allocated:  1257.443359375
Memory cached:  1352.0
