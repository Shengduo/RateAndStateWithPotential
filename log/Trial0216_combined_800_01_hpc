[I 2024-02-28 03:51:44,234] Using an existing study with name 'my_study' instead of creating a new one.
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
--------------------  Trial  64   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 5, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 10, 'D_layers': 5, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 8, 'log_learning_rate': -1.24489062431312, 'log_learning_rate_D': -2.51176889612082, 'log_learning_rate_D_dagger': -2.013880072439033, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  89.11376953125
Memory cached:  700.0
[I 2024-02-28 03:53:27,662] Trial 64 finished with value: 693.7898559570312 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 5, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 10, 'D_layers': 5, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 8, 'log_learning_rate': -1.24489062431312, 'log_learning_rate_D': -2.51176889612082, 'log_learning_rate_D_dagger': -2.013880072439033, 'training_batch_size': 6, 'training_p': 5}. Best is trial 11 with value: 0.0255799051374197.
[I 2024-02-28 03:53:27,849] Using an existing study with name 'my_study' instead of creating a new one.
res:  tensor(693.7899, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  100.12283945083618
Memory status after this trial: 
Memory allocated:  1811.56884765625
Memory cached:  1834.0
--------------------  Trial  3   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'D_layers': 8, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 7, 'D_layer_units_exponent_7': 8, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 5, 'log_learning_rate': -1.079328253416035, 'log_learning_rate_D': -3.2925701951188233, 'log_learning_rate_D_dagger': -2.3816496219304253, 'training_batch_size': 8, 'training_p': 2}
/central/groups/lapusta-group/sliu/conda/miniconda3/lib/python3.11/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
	 epoch  0 training error:  tensor(1263.7389, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  30.1669921875
Memory cached:  48.0
[I 2024-02-28 03:53:29,528] Trial 3 finished with value: 390.643310546875 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'D_layers': 8, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 7, 'D_layer_units_exponent_7': 8, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 5, 'log_learning_rate': -1.079328253416035, 'log_learning_rate_D': -3.2925701951188233, 'log_learning_rate_D_dagger': -2.3816496219304253, 'training_batch_size': 8, 'training_p': 2}. Best is trial 0 with value: 0.22980225086212158.
res:  tensor(390.6433, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  1.266507625579834
Memory status after this trial: 
Memory allocated:  388.7626953125
Memory cached:  412.0
