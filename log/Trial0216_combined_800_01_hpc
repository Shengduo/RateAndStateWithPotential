[I 2024-02-28 11:41:14,336] A new study created in RDB with name: my_study1
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
--------------------  Trial  0   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 4, 'W_layer_units_exponent_7': 8, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 5, 'D_layer_units_exponent_7': 5, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -4.059927623598378, 'log_learning_rate_D': -1.8100431782801123, 'log_learning_rate_D_dagger': -4.903299129075906, 'training_batch_size': 12, 'training_p': 8}
	 epoch  0 training error:  tensor(1.0000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  31.27294921875
Memory cached:  262.0
[I 2024-02-28 11:41:41,895] Trial 0 finished with value: 0.9925511479377747 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 4, 'W_layer_units_exponent_7': 8, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 5, 'D_layer_units_exponent_7': 5, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -4.059927623598378, 'log_learning_rate_D': -1.8100431782801123, 'log_learning_rate_D_dagger': -4.903299129075906, 'training_batch_size': 12, 'training_p': 8}. Best is trial 0 with value: 0.9925511479377747.
res:  tensor(0.9926, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  26.956564903259277
Memory status after this trial: 
Memory allocated:  752.52685546875
Memory cached:  764.0
--------------------  Trial  1   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 9, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 5, 'D_layer_units_exponent_7': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -3.6077200377910947, 'log_learning_rate_D': -1.6748255145403528, 'log_learning_rate_D_dagger': -2.0285983408416897, 'training_batch_size': 12, 'training_p': 8}
	 epoch  0 training error:  tensor(1.1327, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  769.80517578125
Memory cached:  944.0
[I 2024-02-28 11:41:56,547] Trial 1 finished with value: 1.7246769666671753 and parameters: {'W_layers': 2, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 9, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 5, 'D_layer_units_exponent_7': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -3.6077200377910947, 'log_learning_rate_D': -1.6748255145403528, 'log_learning_rate_D_dagger': -2.0285983408416897, 'training_batch_size': 12, 'training_p': 8}. Best is trial 0 with value: 0.9925511479377747.
res:  tensor(1.7247, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.9926, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  14.09190058708191
Memory status after this trial: 
Memory allocated:  1656.828125
Memory cached:  1664.0
--------------------  Trial  2   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 5, 'D_layers': 6, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -3.860580626702042, 'log_learning_rate_D': -2.8784007846568738, 'log_learning_rate_D_dagger': -1.5284904210528447, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(18.3117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  777.19189453125
Memory cached:  1154.0
[I 2024-02-28 11:43:24,264] Trial 2 finished with value: 20.900821685791016 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 5, 'D_layers': 6, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -3.860580626702042, 'log_learning_rate_D': -2.8784007846568738, 'log_learning_rate_D_dagger': -1.5284904210528447, 'training_batch_size': 6, 'training_p': 7}. Best is trial 0 with value: 0.9925511479377747.
res:  tensor(20.9008, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.9926, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  86.49168491363525
Memory status after this trial: 
Memory allocated:  1800.03125
Memory cached:  1816.0
--------------------  Trial  3   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 4, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 6, 'W_layer_units_exponent_7': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 6, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -4.4227741316544185, 'log_learning_rate_D': -2.1379431462671574, 'log_learning_rate_D_dagger': -4.888010300391617, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0163, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  768.8017578125
Memory cached:  1062.0
[I 2024-02-28 11:44:59,867] Trial 3 finished with value: 0.8698424696922302 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 4, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 6, 'W_layer_units_exponent_7': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 6, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -4.4227741316544185, 'log_learning_rate_D': -2.1379431462671574, 'log_learning_rate_D_dagger': -4.888010300391617, 'training_batch_size': 6, 'training_p': 5}. Best is trial 3 with value: 0.8698424696922302.
res:  tensor(0.8698, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.9926, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  94.21131157875061
Memory status after this trial: 
Memory allocated:  1109.58349609375
Memory cached:  1500.0
--------------------  Trial  4   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 7, 'D_layers': 7, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 9, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'log_learning_rate': -2.73697536955737, 'log_learning_rate_D': -4.2086354796928465, 'log_learning_rate_D_dagger': -3.946324993873767, 'training_batch_size': 8, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9516, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1116.431640625
Memory cached:  1486.0
[I 2024-02-28 11:45:32,509] Trial 4 finished with value: 0.8128965497016907 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 7, 'D_layers': 7, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 9, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'log_learning_rate': -2.73697536955737, 'log_learning_rate_D': -4.2086354796928465, 'log_learning_rate_D_dagger': -3.946324993873767, 'training_batch_size': 8, 'training_p': 6}. Best is trial 4 with value: 0.8128965497016907.
res:  tensor(0.8129, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.8698, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  31.806520462036133
Memory status after this trial: 
Memory allocated:  794.58935546875
Memory cached:  1320.0
--------------------  Trial  5   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 4, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 7, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.774749860797642, 'log_learning_rate_D': -2.616423176007947, 'log_learning_rate_D_dagger': -2.127167882352075, 'training_batch_size': 8, 'training_p': 7}
	 epoch  0 training error:  tensor(0.7299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  797.169921875
Memory cached:  1206.0
[I 2024-02-28 11:45:55,293] Trial 5 finished with value: 0.49248820543289185 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 4, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 7, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.774749860797642, 'log_learning_rate_D': -2.616423176007947, 'log_learning_rate_D_dagger': -2.127167882352075, 'training_batch_size': 8, 'training_p': 7}. Best is trial 5 with value: 0.49248820543289185.
res:  tensor(0.4925, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.8129, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  22.12383508682251
Memory status after this trial: 
Memory allocated:  473.52978515625
Memory cached:  928.0
--------------------  Trial  6   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'D_layers': 3, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 7, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -2.2137950274214453, 'log_learning_rate_D': -2.2940705970197155, 'log_learning_rate_D_dagger': -4.031565697142616, 'training_batch_size': 9, 'training_p': 7}
