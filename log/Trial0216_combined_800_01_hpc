[I 2024-03-01 07:32:12,440] A new study created in RDB with name: my_study1
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
--------------------  Trial  0   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 4, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.876329842673198, 'log_learning_rate_D': -1.7263905183340387, 'log_learning_rate_D_dagger': -1.7751587305724579, 'training_batch_size': 10, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0025, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  42.1728515625
Memory cached:  280.0
	 epoch  10 training error:  tensor(0.6478, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  41.9228515625
Memory cached:  572.0
	 epoch  20 training error:  tensor(0.6616, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  42.1728515625
Memory cached:  580.0
	 epoch  30 training error:  tensor(0.6623, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  41.9228515625
Memory cached:  558.0
	 epoch  40 training error:  tensor(0.5714, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  42.1728515625
Memory cached:  552.0
	 epoch  50 training error:  tensor(0.3941, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  41.9228515625
Memory cached:  582.0
	 epoch  60 training error:  tensor(0.3528, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  41.9228515625
Memory cached:  560.0
	 epoch  70 training error:  tensor(0.3801, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  41.9228515625
Memory cached:  562.0
	 epoch  80 training error:  tensor(0.3817, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  42.1728515625
Memory cached:  566.0
	 epoch  90 training error:  tensor(0.3554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  42.1728515625
Memory cached:  548.0
[I 2024-03-01 07:38:56,283] Trial 0 finished with value: 0.23779690265655518 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 4, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.876329842673198, 'log_learning_rate_D': -1.7263905183340387, 'log_learning_rate_D_dagger': -1.7751587305724579, 'training_batch_size': 10, 'training_p': 5}. Best is trial 0 with value: 0.23779690265655518.
res:  tensor(0.2378, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  403.42404341697693
Memory status after this trial: 
Memory allocated:  835.33154296875
Memory cached:  858.0
--------------------  Trial  1   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 6, 'W_layer_units_exponent_5': 4, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 7, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 4, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'log_learning_rate': -3.4039862550030606, 'log_learning_rate_D': -4.766836598218072, 'log_learning_rate_D_dagger': -1.132941513228249, 'training_batch_size': 12, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9840, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1036.0
	 epoch  10 training error:  tensor(2.1214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1198.0
	 epoch  20 training error:  tensor(0.6922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1206.0
	 epoch  30 training error:  tensor(0.6095, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1204.0
	 epoch  40 training error:  tensor(0.5600, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1200.0
	 epoch  50 training error:  tensor(0.3909, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1202.0
	 epoch  60 training error:  tensor(0.4477, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1208.0
	 epoch  70 training error:  tensor(0.3830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1210.0
	 epoch  80 training error:  tensor(0.3577, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1212.0
	 epoch  90 training error:  tensor(0.3606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  840.65673828125
Memory cached:  1200.0
[I 2024-03-01 07:44:26,312] Trial 1 finished with value: 0.21908441185951233 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 6, 'W_layer_units_exponent_5': 4, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 7, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 4, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'log_learning_rate': -3.4039862550030606, 'log_learning_rate_D': -4.766836598218072, 'log_learning_rate_D_dagger': -1.132941513228249, 'training_batch_size': 12, 'training_p': 6}. Best is trial 1 with value: 0.21908441185951233.
res:  tensor(0.2191, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.2378, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  329.63944149017334
Memory status after this trial: 
Memory allocated:  517.49658203125
Memory cached:  1062.0
--------------------  Trial  2   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 5, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 6, 'W_layer_units_exponent_7': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 5, 'D_layer_units_exponent_2': 5, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'log_learning_rate': -1.5610831554111426, 'log_learning_rate_D': -4.3931210401030665, 'log_learning_rate_D_dagger': -4.135949106781026, 'training_batch_size': 10, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0467, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1104.0
	 epoch  10 training error:  tensor(0.5426, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1178.0
	 epoch  20 training error:  tensor(0.7130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1176.0
	 epoch  30 training error:  tensor(0.3650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1166.0
	 epoch  40 training error:  tensor(0.3520, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1176.0
	 epoch  50 training error:  tensor(0.3383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1164.0
	 epoch  60 training error:  tensor(0.3454, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1178.0
	 epoch  70 training error:  tensor(0.3321, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1154.0
	 epoch  80 training error:  tensor(0.3308, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1176.0
	 epoch  90 training error:  tensor(0.3288, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  522.35693359375
Memory cached:  1180.0
[I 2024-03-01 07:50:16,646] Trial 2 finished with value: 0.218912273645401 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 5, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 6, 'W_layer_units_exponent_7': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 5, 'D_layer_units_exponent_2': 5, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'log_learning_rate': -1.5610831554111426, 'log_learning_rate_D': -4.3931210401030665, 'log_learning_rate_D_dagger': -4.135949106781026, 'training_batch_size': 10, 'training_p': 5}. Best is trial 2 with value: 0.218912273645401.
res:  tensor(0.2189, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.2191, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  349.9416825771332
Memory status after this trial: 
Memory allocated:  362.86279296875
Memory cached:  768.0
--------------------  Trial  3   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 8, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 8, 'log_learning_rate': -3.207727754327934, 'log_learning_rate_D': -4.987567753692063, 'log_learning_rate_D_dagger': -4.493450086782318, 'training_batch_size': 11, 'training_p': 2}
	 epoch  0 training error:  tensor(1.0644, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  864.0
	 epoch  10 training error:  tensor(0.5833, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  948.0
	 epoch  20 training error:  tensor(0.5118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  956.0
	 epoch  30 training error:  tensor(0.3989, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  956.0
	 epoch  40 training error:  tensor(0.2670, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  952.0
	 epoch  50 training error:  tensor(0.2417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  952.0
	 epoch  60 training error:  tensor(0.2268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  950.0
	 epoch  70 training error:  tensor(0.2254, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  970.0
	 epoch  80 training error:  tensor(0.2212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  938.0
	 epoch  90 training error:  tensor(0.2196, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  380.30126953125
Memory cached:  960.0
[I 2024-03-01 07:57:11,975] Trial 3 finished with value: 0.22159424424171448 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 8, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 8, 'log_learning_rate': -3.207727754327934, 'log_learning_rate_D': -4.987567753692063, 'log_learning_rate_D_dagger': -4.493450086782318, 'training_batch_size': 11, 'training_p': 2}. Best is trial 2 with value: 0.218912273645401.
Time for this trial:  414.9662911891937
Memory status after this trial: 
Memory allocated:  1077.4794921875
Memory cached:  1104.0
--------------------  Trial  4   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'log_learning_rate': -1.1051842323742243, 'log_learning_rate_D': -4.219172398728812, 'log_learning_rate_D_dagger': -4.427868230452528, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(162.2878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  808.0
	 epoch  10 training error:  tensor(1168.1742, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  802.0
	 epoch  20 training error:  tensor(24.5884, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  814.0
	 epoch  30 training error:  tensor(18.1059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  814.0
	 epoch  40 training error:  tensor(1.2451, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  806.0
	 epoch  50 training error:  tensor(0.6059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  820.0
	 epoch  60 training error:  tensor(0.6335, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  808.0
	 epoch  70 training error:  tensor(0.5858, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  806.0
	 epoch  80 training error:  tensor(0.5438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  812.0
	 epoch  90 training error:  tensor(0.5221, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  376.0166015625
Memory cached:  816.0
[I 2024-03-01 08:12:35,357] Trial 4 finished with value: 0.4579746425151825 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'log_learning_rate': -1.1051842323742243, 'log_learning_rate_D': -4.219172398728812, 'log_learning_rate_D_dagger': -4.427868230452528, 'training_batch_size': 8, 'training_p': 3}. Best is trial 2 with value: 0.218912273645401.
Time for this trial:  922.740453004837
Memory status after this trial: 
Memory allocated:  1038.90771484375
Memory cached:  1070.0
--------------------  Trial  5   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 6, 'D_layers': 8, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 5, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 6, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 10, 'log_learning_rate': -4.310388938897606, 'log_learning_rate_D': -2.4029480824326277, 'log_learning_rate_D_dagger': -2.829419028299656, 'training_batch_size': 12, 'training_p': 4}
	 epoch  0 training error:  tensor(0.9908, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  954.0
	 epoch  10 training error:  tensor(0.3514, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1020.0
	 epoch  20 training error:  tensor(0.3075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1040.0
	 epoch  30 training error:  tensor(0.3019, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1044.0
	 epoch  40 training error:  tensor(0.2976, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1050.0
	 epoch  50 training error:  tensor(0.2934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1034.0
	 epoch  60 training error:  tensor(0.2870, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1036.0
	 epoch  70 training error:  tensor(0.2876, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1032.0
	 epoch  80 training error:  tensor(0.2888, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1062.0
	 epoch  90 training error:  tensor(0.2886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  389.36572265625
Memory cached:  1040.0
[I 2024-03-01 08:19:50,135] Trial 5 finished with value: 0.19881682097911835 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 6, 'D_layers': 8, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 5, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 6, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 10, 'log_learning_rate': -4.310388938897606, 'log_learning_rate_D': -2.4029480824326277, 'log_learning_rate_D_dagger': -2.829419028299656, 'training_batch_size': 12, 'training_p': 4}. Best is trial 5 with value: 0.19881682097911835.
res:  tensor(0.1988, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.2189, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  434.29022336006165
Memory status after this trial: 
Memory allocated:  776.0146484375
Memory cached:  1132.0
--------------------  Trial  6   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 4, 'W_layer_units_exponent_5': 7, 'W_layer_units_exponent_6': 10, 'W_layer_units_exponent_7': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 4, 'D_layer_units_exponent_4': 6, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -4.473830775374605, 'log_learning_rate_D': -3.594800608798616, 'log_learning_rate_D_dagger': -3.8556130775781727, 'training_batch_size': 11, 'training_p': 4}
	 epoch  0 training error:  tensor(0.9846, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  796.7529296875
Memory cached:  1324.0
	 epoch  10 training error:  tensor(0.7295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  797.2529296875
Memory cached:  1564.0
	 epoch  20 training error:  tensor(0.5476, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  797.2529296875
Memory cached:  1568.0
	 epoch  30 training error:  tensor(0.4320, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  796.59228515625
Memory cached:  1570.0
	 epoch  40 training error:  tensor(0.3468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  796.7529296875
Memory cached:  1576.0
	 epoch  50 training error:  tensor(0.3256, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  796.7529296875
Memory cached:  1566.0
	 epoch  60 training error:  tensor(0.3180, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  796.7529296875
Memory cached:  1568.0
	 epoch  70 training error:  tensor(0.3124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  797.2529296875
Memory cached:  1570.0
	 epoch  80 training error:  tensor(0.3091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  797.2529296875
Memory cached:  1562.0
	 epoch  90 training error:  tensor(0.3059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  796.7529296875
Memory cached:  1558.0
[I 2024-03-01 08:27:34,262] Trial 6 finished with value: 0.2205505669116974 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 4, 'W_layer_units_exponent_5': 7, 'W_layer_units_exponent_6': 10, 'W_layer_units_exponent_7': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 4, 'D_layer_units_exponent_4': 6, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -4.473830775374605, 'log_learning_rate_D': -3.594800608798616, 'log_learning_rate_D_dagger': -3.8556130775781727, 'training_batch_size': 11, 'training_p': 4}. Best is trial 5 with value: 0.19881682097911835.
Time for this trial:  463.73887753486633
Memory status after this trial: 
Memory allocated:  1973.74169921875
Memory cached:  1998.0
--------------------  Trial  7   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 4, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 10, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 6, 'log_learning_rate': -2.7158302410125437, 'log_learning_rate_D': -4.0715690982473935, 'log_learning_rate_D_dagger': -1.9165455707029593, 'training_batch_size': 10, 'training_p': 8}
	 epoch  0 training error:  tensor(0.9963, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1272.0
	 epoch  10 training error:  tensor(0.5090, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1432.0
	 epoch  20 training error:  tensor(0.5239, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1434.0
	 epoch  30 training error:  tensor(0.4029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1432.0
	 epoch  40 training error:  tensor(0.4134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1438.0
	 epoch  50 training error:  tensor(0.3891, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1438.0
	 epoch  60 training error:  tensor(0.4020, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1424.0
	 epoch  70 training error:  tensor(0.4016, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1440.0
	 epoch  80 training error:  tensor(0.3898, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1440.0
	 epoch  90 training error:  tensor(0.3958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  788.77099609375
Memory cached:  1434.0
[I 2024-03-01 08:35:28,239] Trial 7 finished with value: 0.22201107442378998 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 4, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 10, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 6, 'log_learning_rate': -2.7158302410125437, 'log_learning_rate_D': -4.0715690982473935, 'log_learning_rate_D_dagger': -1.9165455707029593, 'training_batch_size': 10, 'training_p': 8}. Best is trial 5 with value: 0.19881682097911835.
Time for this trial:  473.483341217041
Memory status after this trial: 
Memory allocated:  1661.63330078125
Memory cached:  1690.0
--------------------  Trial  8   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -4.287580002656933, 'log_learning_rate_D': -1.2496795935609448, 'log_learning_rate_D_dagger': -4.0711929150854935, 'training_batch_size': 8, 'training_p': 3}
	 epoch  0 training error:  tensor(1.1943, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1224.0
	 epoch  10 training error:  tensor(0.6307, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1222.0
	 epoch  20 training error:  tensor(0.6307, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1222.0
	 epoch  30 training error:  tensor(0.6306, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1222.0
	 epoch  40 training error:  tensor(0.6305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1216.0
	 epoch  50 training error:  tensor(0.6305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1232.0
	 epoch  60 training error:  tensor(0.6305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1228.0
	 epoch  70 training error:  tensor(0.6305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1228.0
	 epoch  80 training error:  tensor(0.6305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1228.0
	 epoch  90 training error:  tensor(0.6305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  813.98681640625
Memory cached:  1226.0
[I 2024-03-01 08:53:54,738] Trial 8 finished with value: 0.6089797019958496 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -4.287580002656933, 'log_learning_rate_D': -1.2496795935609448, 'log_learning_rate_D_dagger': -4.0711929150854935, 'training_batch_size': 8, 'training_p': 3}. Best is trial 5 with value: 0.19881682097911835.
Time for this trial:  1106.0067496299744
Memory status after this trial: 
Memory allocated:  1962.904296875
Memory cached:  1990.0
--------------------  Trial  9   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 4, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 4, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.872943474903245, 'log_learning_rate_D': -2.3958967990803157, 'log_learning_rate_D_dagger': -3.621852005690122, 'training_batch_size': 8, 'training_p': 8}
	 epoch  0 training error:  tensor(1.0635, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1152.0
	 epoch  10 training error:  tensor(0.4465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1178.0
	 epoch  20 training error:  tensor(0.4041, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1180.0
	 epoch  30 training error:  tensor(0.3914, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1184.0
	 epoch  40 training error:  tensor(0.3863, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1190.0
	 epoch  50 training error:  tensor(0.3776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1184.0
	 epoch  60 training error:  tensor(0.3716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1178.0
	 epoch  70 training error:  tensor(0.3555, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1182.0
	 epoch  80 training error:  tensor(0.3492, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1190.0
	 epoch  90 training error:  tensor(0.3467, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  787.5068359375
Memory cached:  1176.0
[I 2024-03-01 09:09:56,813] Trial 9 finished with value: 0.20622122287750244 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 4, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 4, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.872943474903245, 'log_learning_rate_D': -2.3958967990803157, 'log_learning_rate_D_dagger': -3.621852005690122, 'training_batch_size': 8, 'training_p': 8}. Best is trial 5 with value: 0.19881682097911835.
Time for this trial:  961.5814335346222
Memory status after this trial: 
Memory allocated:  1351.48779296875
Memory cached:  1378.0
--------------------  Trial  10   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 8, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 4, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.787981079750564, 'log_learning_rate_D': -2.7490072012995754, 'log_learning_rate_D_dagger': -2.6262685479195746, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.6804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1242.0
	 epoch  10 training error:  tensor(0.3495, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1242.0
	 epoch  20 training error:  tensor(0.3269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  30 training error:  tensor(0.3258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  40 training error:  tensor(0.3098, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  50 training error:  tensor(0.3026, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  60 training error:  tensor(0.2804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  70 training error:  tensor(0.2524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  80 training error:  tensor(0.2424, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
	 epoch  90 training error:  tensor(0.2308, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  801.39306640625
Memory cached:  1240.0
[I 2024-03-01 10:07:05,573] Trial 10 finished with value: 0.15435023605823517 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 8, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 4, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.787981079750564, 'log_learning_rate_D': -2.7490072012995754, 'log_learning_rate_D_dagger': -2.6262685479195746, 'training_batch_size': 6, 'training_p': 6}. Best is trial 10 with value: 0.15435023605823517.
res:  tensor(0.1544, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.1988, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3427.7361567020416
Memory status after this trial: 
Memory allocated:  975.60693359375
Memory cached:  1478.0
--------------------  Trial  11   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 8, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 4, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.975219313014927, 'log_learning_rate_D': -2.7897243212031446, 'log_learning_rate_D_dagger': -2.8617234705627963, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.8156, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  10 training error:  tensor(0.3252, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  20 training error:  tensor(0.3157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  30 training error:  tensor(0.3139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  40 training error:  tensor(0.3014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  50 training error:  tensor(0.2489, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  60 training error:  tensor(0.2208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  70 training error:  tensor(0.1927, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  80 training error:  tensor(0.1879, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
	 epoch  90 training error:  tensor(0.1756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1000.736328125
Memory cached:  1482.0
[I 2024-03-01 11:05:07,380] Trial 11 finished with value: 0.1404559463262558 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 8, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 6, 'D_layer_units_exponent_7': 4, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.975219313014927, 'log_learning_rate_D': -2.7897243212031446, 'log_learning_rate_D_dagger': -2.8617234705627963, 'training_batch_size': 6, 'training_p': 6}. Best is trial 11 with value: 0.1404559463262558.
res:  tensor(0.1405, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.1544, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3480.8031237125397
Memory status after this trial: 
Memory allocated:  975.60693359375
Memory cached:  1462.0
--------------------  Trial  12   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 6, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.9534710094007375, 'log_learning_rate_D': -3.105366065311643, 'log_learning_rate_D_dagger': -2.865895373659833, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(0.6356, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1572.0
	 epoch  10 training error:  tensor(0.3686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  20 training error:  tensor(0.3640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  30 training error:  tensor(0.3611, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  40 training error:  tensor(0.3592, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  50 training error:  tensor(0.3574, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  60 training error:  tensor(0.3559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  70 training error:  tensor(0.3251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  80 training error:  tensor(0.3082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
	 epoch  90 training error:  tensor(0.4545, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1010.455078125
Memory cached:  1584.0
[I 2024-03-01 12:00:15,080] Trial 12 finished with value: 0.12159967422485352 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 6, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.9534710094007375, 'log_learning_rate_D': -3.105366065311643, 'log_learning_rate_D_dagger': -2.865895373659833, 'training_batch_size': 6, 'training_p': 7}. Best is trial 12 with value: 0.12159967422485352.
res:  tensor(0.1216, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.1405, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3306.7995312213898
Memory status after this trial: 
Memory allocated:  1134.958984375
Memory cached:  1636.0
--------------------  Trial  13   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 10, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.932021446227949, 'log_learning_rate_D': -3.3257535755644216, 'log_learning_rate_D_dagger': -3.2758434304116637, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(0.6063, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1702.0
	 epoch  10 training error:  tensor(0.3651, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1702.0
	 epoch  20 training error:  tensor(0.2575, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1698.0
	 epoch  30 training error:  tensor(0.2762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1698.0
	 epoch  40 training error:  tensor(0.2760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1700.0
	 epoch  50 training error:  tensor(0.2468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1696.0
	 epoch  60 training error:  tensor(0.2466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1696.0
	 epoch  70 training error:  tensor(0.2467, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1698.0
	 epoch  80 training error:  tensor(0.2469, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1696.0
	 epoch  90 training error:  tensor(0.2466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1182.84423828125
Memory cached:  1698.0
[I 2024-03-01 12:56:52,377] Trial 13 finished with value: 0.15494583547115326 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 4, 'D_layer_units_exponent_6': 10, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'log_learning_rate': -4.932021446227949, 'log_learning_rate_D': -3.3257535755644216, 'log_learning_rate_D_dagger': -3.2758434304116637, 'training_batch_size': 6, 'training_p': 7}. Best is trial 12 with value: 0.12159967422485352.
Time for this trial:  3396.3926515579224
Memory status after this trial: 
Memory allocated:  2834.42333984375
Memory cached:  2876.0
--------------------  Trial  14   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 4, 'D_layers': 7, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 4, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'log_learning_rate': -4.129857282987435, 'log_learning_rate_D': -3.008578502323868, 'log_learning_rate_D_dagger': -2.3844750960053074, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(0.6657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1638.0
	 epoch  10 training error:  tensor(0.3719, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1638.0
	 epoch  20 training error:  tensor(0.3757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1638.0
	 epoch  30 training error:  tensor(0.3621, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1636.0
	 epoch  40 training error:  tensor(0.3540, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1642.0
	 epoch  50 training error:  tensor(0.3556, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1640.0
	 epoch  60 training error:  tensor(0.3257, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1636.0
	 epoch  70 training error:  tensor(0.2146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1638.0
	 epoch  80 training error:  tensor(0.1692, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1638.0
	 epoch  90 training error:  tensor(0.1497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1151.435546875
Memory cached:  1638.0
[I 2024-03-01 13:27:36,887] Trial 14 finished with value: 0.12752662599086761 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 4, 'D_layers': 7, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 7, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 4, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'log_learning_rate': -4.129857282987435, 'log_learning_rate_D': -3.008578502323868, 'log_learning_rate_D_dagger': -2.3844750960053074, 'training_batch_size': 7, 'training_p': 7}. Best is trial 12 with value: 0.12159967422485352.
Time for this trial:  1843.675437450409
Memory status after this trial: 
Memory allocated:  1974.3115234375
Memory cached:  1996.0
--------------------  Trial  15   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 4, 'log_learning_rate': -3.7912767737190762, 'log_learning_rate_D': -3.450572753138181, 'log_learning_rate_D_dagger': -2.2772223218250205, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(1.0231, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1642.0
	 epoch  10 training error:  tensor(0.3760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1650.0
	 epoch  20 training error:  tensor(0.3678, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1646.0
	 epoch  30 training error:  tensor(0.3607, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1644.0
	 epoch  40 training error:  tensor(0.3565, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1646.0
	 epoch  50 training error:  tensor(0.3175, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1644.0
	 epoch  60 training error:  tensor(0.2031, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1650.0
	 epoch  70 training error:  tensor(0.1964, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1644.0
	 epoch  80 training error:  tensor(0.1552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1648.0
	 epoch  90 training error:  tensor(0.1490, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1153.78076171875
Memory cached:  1650.0
[I 2024-03-01 13:57:26,002] Trial 15 finished with value: 0.07262938469648361 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 4, 'log_learning_rate': -3.7912767737190762, 'log_learning_rate_D': -3.450572753138181, 'log_learning_rate_D_dagger': -2.2772223218250205, 'training_batch_size': 7, 'training_p': 7}. Best is trial 15 with value: 0.07262938469648361.
res:  tensor(0.0726, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.1216, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  1785.489902496338
Memory status after this trial: 
Memory allocated:  950.724609375
Memory cached:  1608.0
--------------------  Trial  16   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 7, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'log_learning_rate': -3.7857733571316494, 'log_learning_rate_D': -3.625923163117787, 'log_learning_rate_D_dagger': -2.1080463066915183, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(1.2034, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1528.0
	 epoch  10 training error:  tensor(0.3741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1526.0
	 epoch  20 training error:  tensor(0.3675, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1524.0
	 epoch  30 training error:  tensor(0.3596, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1526.0
	 epoch  40 training error:  tensor(0.3433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1524.0
	 epoch  50 training error:  tensor(0.3445, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1524.0
	 epoch  60 training error:  tensor(0.3453, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1524.0
	 epoch  70 training error:  tensor(0.3308, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1524.0
	 epoch  80 training error:  tensor(0.3163, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1526.0
	 epoch  90 training error:  tensor(0.2712, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  960.8359375
Memory cached:  1524.0
[I 2024-03-01 14:27:22,458] Trial 16 finished with value: 0.11692693084478378 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 7, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'log_learning_rate': -3.7857733571316494, 'log_learning_rate_D': -3.625923163117787, 'log_learning_rate_D_dagger': -2.1080463066915183, 'training_batch_size': 7, 'training_p': 7}. Best is trial 15 with value: 0.07262938469648361.
Time for this trial:  1795.6399774551392
Memory status after this trial: 
Memory allocated:  1754.984375
Memory cached:  1784.0
--------------------  Trial  17   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 7, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 4, 'log_learning_rate': -3.7117061351477894, 'log_learning_rate_D': -3.812663079946188, 'log_learning_rate_D_dagger': -2.0626238288164003, 'training_batch_size': 7, 'training_p': 8}
	 epoch  0 training error:  tensor(0.7479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1524.0
	 epoch  10 training error:  tensor(0.3653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1526.0
	 epoch  20 training error:  tensor(0.3094, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1524.0
	 epoch  30 training error:  tensor(0.1994, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1528.0
	 epoch  40 training error:  tensor(0.1696, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1526.0
	 epoch  50 training error:  tensor(0.1181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1524.0
	 epoch  60 training error:  tensor(0.1106, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1528.0
	 epoch  70 training error:  tensor(0.0986, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1526.0
	 epoch  80 training error:  tensor(0.0945, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1524.0
	 epoch  90 training error:  tensor(0.1194, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  964.021484375
Memory cached:  1528.0
[I 2024-03-01 14:58:55,965] Trial 17 finished with value: 1.8042891025543213 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_layer_units_exponent_5': 7, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 4, 'log_learning_rate': -3.7117061351477894, 'log_learning_rate_D': -3.812663079946188, 'log_learning_rate_D_dagger': -2.0626238288164003, 'training_batch_size': 7, 'training_p': 8}. Best is trial 15 with value: 0.07262938469648361.
Time for this trial:  1892.7325704097748
Memory status after this trial: 
Memory allocated:  1874.64501953125
Memory cached:  1906.0
--------------------  Trial  18   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 7, 'W_layer_units_exponent_6': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'log_learning_rate': -2.260877498776759, 'log_learning_rate_D': -3.684418703455768, 'log_learning_rate_D_dagger': -1.2451771653895825, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(63.8288, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1532.0
	 epoch  10 training error:  tensor(0.4571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1532.0
	 epoch  20 training error:  tensor(0.4508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1532.0
	 epoch  30 training error:  tensor(0.3616, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1528.0
	 epoch  40 training error:  tensor(0.4051, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1532.0
	 epoch  50 training error:  tensor(0.4281, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1534.0
	 epoch  60 training error:  tensor(0.3775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1530.0
	 epoch  70 training error:  tensor(0.3660, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1530.0
	 epoch  80 training error:  tensor(0.3650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1534.0
	 epoch  90 training error:  tensor(0.7461, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.65869140625
Memory cached:  1530.0
[I 2024-03-01 15:28:40,809] Trial 18 finished with value: 0.21994610130786896 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 7, 'W_layer_units_exponent_6': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'log_learning_rate': -2.260877498776759, 'log_learning_rate_D': -3.684418703455768, 'log_learning_rate_D_dagger': -1.2451771653895825, 'training_batch_size': 7, 'training_p': 7}. Best is trial 15 with value: 0.07262938469648361.
Time for this trial:  1784.1232013702393
Memory status after this trial: 
Memory allocated:  1979.4482421875
Memory cached:  2016.0
--------------------  Trial  19   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'W_layer_units_exponent_7': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'log_learning_rate': -3.821110579074639, 'log_learning_rate_D': -2.182451918165141, 'log_learning_rate_D_dagger': -1.4883318167901207, 'training_batch_size': 9, 'training_p': 6}
	 epoch  0 training error:  tensor(1.4223, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  10 training error:  tensor(0.7235, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  20 training error:  tensor(0.6513, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  30 training error:  tensor(0.6512, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  40 training error:  tensor(0.6502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  50 training error:  tensor(0.6507, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  60 training error:  tensor(0.6510, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  70 training error:  tensor(0.6513, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  80 training error:  tensor(0.6527, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
	 epoch  90 training error:  tensor(0.6525, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  955.7705078125
Memory cached:  1522.0
[I 2024-03-01 15:40:45,826] Trial 19 finished with value: 0.6093716025352478 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 4, 'W_layer_units_exponent_7': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'log_learning_rate': -3.821110579074639, 'log_learning_rate_D': -2.182451918165141, 'log_learning_rate_D_dagger': -1.4883318167901207, 'training_batch_size': 9, 'training_p': 6}. Best is trial 15 with value: 0.07262938469648361.
Time for this trial:  724.2828440666199
Memory status after this trial: 
Memory allocated:  1580.16259765625
Memory cached:  1606.0
--------------------  Trial  20   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 8, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'log_learning_rate': -2.1388374442824607, 'log_learning_rate_D': -3.399690413777546, 'log_learning_rate_D_dagger': -2.3199062595229947, 'training_batch_size': 9, 'training_p': 5}
	 epoch  0 training error:  tensor(1.5900, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  10 training error:  tensor(0.4431, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  20 training error:  tensor(0.3773, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  30 training error:  tensor(0.3274, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  40 training error:  tensor(0.3153, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  50 training error:  tensor(0.3166, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  60 training error:  tensor(0.3251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  70 training error:  tensor(0.3187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  80 training error:  tensor(0.2998, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
	 epoch  90 training error:  tensor(0.3421, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  954.5908203125
Memory cached:  1520.0
[I 2024-03-01 15:50:20,604] Trial 20 finished with value: 0.2502809166908264 and parameters: {'W_layers': 2, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 8, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'log_learning_rate': -2.1388374442824607, 'log_learning_rate_D': -3.399690413777546, 'log_learning_rate_D_dagger': -2.3199062595229947, 'training_batch_size': 9, 'training_p': 5}. Best is trial 15 with value: 0.07262938469648361.
Time for this trial:  574.257973909378
Memory status after this trial: 
Memory allocated:  1347.84619140625
Memory cached:  1520.0
--------------------  Trial  21   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 9, 'D_layers': 7, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 5, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 9, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'log_learning_rate': -3.7503270152755324, 'log_learning_rate_D': -3.147309233702836, 'log_learning_rate_D_dagger': -3.412392186805879, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(0.8158, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1582.0
	 epoch  10 training error:  tensor(0.3807, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1584.0
	 epoch  20 training error:  tensor(0.3691, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1584.0
	 epoch  30 training error:  tensor(0.3628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1584.0
	 epoch  40 training error:  tensor(0.3589, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1584.0
	 epoch  50 training error:  tensor(0.3568, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1582.0
	 epoch  60 training error:  tensor(0.3576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1582.0
	 epoch  70 training error:  tensor(0.3846, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1582.0
	 epoch  80 training error:  tensor(0.3290, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1582.0
	 epoch  90 training error:  tensor(0.2872, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  968.01220703125
Memory cached:  1582.0
[I 2024-03-01 16:17:52,164] Trial 21 finished with value: 0.1673801690340042 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 4, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 9, 'D_layers': 7, 'D_layer_units_exponent_0': 4, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 5, 'D_layer_units_exponent_5': 6, 'D_layer_units_exponent_6': 9, 'D_dagger_layers': 2, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 9, 'log_learning_rate': -3.7503270152755324, 'log_learning_rate_D': -3.147309233702836, 'log_learning_rate_D_dagger': -3.412392186805879, 'training_batch_size': 7, 'training_p': 7}. Best is trial 15 with value: 0.07262938469648361.
Time for this trial:  1650.7943630218506
Memory status after this trial: 
Memory allocated:  1791.69775390625
Memory cached:  1818.0
--------------------  Trial  22   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -3.4039401912870453, 'log_learning_rate_D': -3.779396953521222, 'log_learning_rate_D_dagger': -3.144408468685878, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(0.5703, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  10 training error:  tensor(0.3619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  20 training error:  tensor(0.2800, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  30 training error:  tensor(0.1884, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  40 training error:  tensor(0.1666, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  50 training error:  tensor(0.1066, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  60 training error:  tensor(0.0904, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  70 training error:  tensor(0.0754, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  80 training error:  tensor(0.0740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
	 epoch  90 training error:  tensor(0.0749, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  972.3037109375
Memory cached:  1580.0
[I 2024-03-01 17:16:19,807] Trial 22 finished with value: 0.047714147716760635 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -3.4039401912870453, 'log_learning_rate_D': -3.779396953521222, 'log_learning_rate_D_dagger': -3.144408468685878, 'training_batch_size': 6, 'training_p': 7}. Best is trial 22 with value: 0.047714147716760635.
res:  tensor(0.0477, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.0726, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3506.6546652317047
Memory status after this trial: 
Memory allocated:  949.34033203125
Memory cached:  1604.0
--------------------  Trial  23   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 7, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -3.4744499380824365, 'log_learning_rate_D': -4.497474780995899, 'log_learning_rate_D_dagger': -4.875804607579463, 'training_batch_size': 7, 'training_p': 8}
	 epoch  0 training error:  tensor(0.9131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  10 training error:  tensor(0.5423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  20 training error:  tensor(0.4335, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1500.0
	 epoch  30 training error:  tensor(0.3980, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  40 training error:  tensor(0.3931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  50 training error:  tensor(0.3827, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  60 training error:  tensor(0.3763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  70 training error:  tensor(0.3703, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  80 training error:  tensor(0.3645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
	 epoch  90 training error:  tensor(0.3580, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  969.74365234375
Memory cached:  1498.0
[I 2024-03-01 17:45:39,055] Trial 23 finished with value: 0.19108577072620392 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 8, 'W_layer_units_exponent_6': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 7, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -3.4744499380824365, 'log_learning_rate_D': -4.497474780995899, 'log_learning_rate_D_dagger': -4.875804607579463, 'training_batch_size': 7, 'training_p': 8}. Best is trial 22 with value: 0.047714147716760635.
Time for this trial:  1758.6489717960358
Memory status after this trial: 
Memory allocated:  1909.921875
Memory cached:  1940.0
--------------------  Trial  24   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 6, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 10, 'log_learning_rate': -3.1953611161063247, 'log_learning_rate_D': -4.047485636187622, 'log_learning_rate_D_dagger': -2.3904582423158565, 'training_batch_size': 8, 'training_p': 7}
	 epoch  0 training error:  tensor(1.4544, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1462.0
	 epoch  10 training error:  tensor(0.4019, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1460.0
	 epoch  20 training error:  tensor(0.3767, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1460.0
	 epoch  30 training error:  tensor(0.3737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1464.0
	 epoch  40 training error:  tensor(0.4069, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1458.0
	 epoch  50 training error:  tensor(0.3911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1460.0
	 epoch  60 training error:  tensor(0.3847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1460.0
	 epoch  70 training error:  tensor(0.3818, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1460.0
	 epoch  80 training error:  tensor(0.3666, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1464.0
	 epoch  90 training error:  tensor(0.3552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  963.95849609375
Memory cached:  1460.0
[I 2024-03-01 18:04:40,037] Trial 24 finished with value: 0.19967451691627502 and parameters: {'W_layers': 7, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 5, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 6, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 5, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 10, 'log_learning_rate': -3.1953611161063247, 'log_learning_rate_D': -4.047485636187622, 'log_learning_rate_D_dagger': -2.3904582423158565, 'training_batch_size': 8, 'training_p': 7}. Best is trial 22 with value: 0.047714147716760635.
Time for this trial:  1140.3559167385101
Memory status after this trial: 
Memory allocated:  2036.23583984375
Memory cached:  2072.0
--------------------  Trial  25   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -2.539982167833467, 'log_learning_rate_D': -3.7868008467359835, 'log_learning_rate_D_dagger': -3.2356114995925633, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.7018, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  10 training error:  tensor(0.3408, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  20 training error:  tensor(0.2445, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  30 training error:  tensor(0.1644, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  40 training error:  tensor(0.1214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1630.0
	 epoch  50 training error:  tensor(0.1164, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  60 training error:  tensor(0.0951, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  70 training error:  tensor(0.0897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  80 training error:  tensor(0.0557, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
	 epoch  90 training error:  tensor(0.0793, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1024.4208984375
Memory cached:  1628.0
[I 2024-03-01 18:56:10,201] Trial 25 finished with value: 0.03546123579144478 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 3, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'log_learning_rate': -2.539982167833467, 'log_learning_rate_D': -3.7868008467359835, 'log_learning_rate_D_dagger': -3.2356114995925633, 'training_batch_size': 6, 'training_p': 6}. Best is trial 25 with value: 0.03546123579144478.
res:  tensor(0.0355, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.0477, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3089.1153717041016
Memory status after this trial: 
Memory allocated:  1547.6181640625
Memory cached:  1886.0
--------------------  Trial  26   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -2.0590957592830272, 'log_learning_rate_D': -3.8967268803643393, 'log_learning_rate_D_dagger': -3.2107627242637515, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9541, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1636.19189453125
Memory cached:  2068.0
	 epoch  10 training error:  tensor(0.3392, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2040.0
	 epoch  20 training error:  tensor(0.1922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2056.0
	 epoch  30 training error:  tensor(0.1391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2046.0
	 epoch  40 training error:  tensor(0.0691, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2054.0
	 epoch  50 training error:  tensor(0.0674, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2046.0
	 epoch  60 training error:  tensor(0.0437, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2050.0
	 epoch  70 training error:  tensor(0.0458, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2052.0
	 epoch  80 training error:  tensor(0.0538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2044.0
	 epoch  90 training error:  tensor(0.0540, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1635.59521484375
Memory cached:  2052.0
[I 2024-03-01 19:56:11,242] Trial 26 finished with value: 0.02490043081343174 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -2.0590957592830272, 'log_learning_rate_D': -3.8967268803643393, 'log_learning_rate_D_dagger': -3.2107627242637515, 'training_batch_size': 6, 'training_p': 6}. Best is trial 26 with value: 0.02490043081343174.
res:  tensor(0.0249, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.0355, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3599.980217218399
Memory status after this trial: 
Memory allocated:  1713.65576171875
Memory cached:  2334.0
--------------------  Trial  27   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -2.4329896147360675, 'log_learning_rate_D': -3.863540420309323, 'log_learning_rate_D_dagger': -3.251274524224219, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(1.8426, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2422.0
	 epoch  10 training error:  tensor(0.3262, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2390.0
	 epoch  20 training error:  tensor(0.1673, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2390.0
	 epoch  30 training error:  tensor(0.0980, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2392.0
	 epoch  40 training error:  tensor(0.1105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2392.0
	 epoch  50 training error:  tensor(0.0492, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2394.0
	 epoch  60 training error:  tensor(0.0597, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2392.0
	 epoch  70 training error:  tensor(0.0573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2398.0
	 epoch  80 training error:  tensor(0.0440, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2404.0
	 epoch  90 training error:  tensor(0.0574, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.54345703125
Memory cached:  2404.0
[I 2024-03-01 20:52:31,963] Trial 27 finished with value: 0.019390789791941643 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -2.4329896147360675, 'log_learning_rate_D': -3.863540420309323, 'log_learning_rate_D_dagger': -3.251274524224219, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
res:  tensor(0.0194, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.0249, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  3379.3799633979797
Memory status after this trial: 
Memory allocated:  1719.46044921875
Memory cached:  2976.0
--------------------  Trial  28   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -2.2615444388810264, 'log_learning_rate_D': -4.727604810018816, 'log_learning_rate_D_dagger': -3.584174072796479, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(8.1350, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3182, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.3059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.2943, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.2064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.1549, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.1317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.1225, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.1238, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.1080, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1760.169921875
Memory cached:  2716.0
[I 2024-03-01 21:45:01,849] Trial 28 finished with value: 0.05938718467950821 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -2.2615444388810264, 'log_learning_rate_D': -4.727604810018816, 'log_learning_rate_D_dagger': -3.584174072796479, 'training_batch_size': 6, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3148.973320007324
Memory status after this trial: 
Memory allocated:  3102.8154296875
Memory cached:  3138.0
--------------------  Trial  29   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -1.9459569366934066, 'log_learning_rate_D': -4.024254709790286, 'log_learning_rate_D_dagger': -3.1028139954730634, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(5.6250, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.39013671875
Memory cached:  2736.0
	 epoch  10 training error:  tensor(0.3312, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.2372, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.1004, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.0882, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.0780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.0517, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.0645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0529, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.0572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1790.3974609375
Memory cached:  2716.0
[I 2024-03-01 22:42:00,689] Trial 29 finished with value: 0.05953236296772957 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -1.9459569366934066, 'log_learning_rate_D': -4.024254709790286, 'log_learning_rate_D_dagger': -3.1028139954730634, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3417.9856338500977
Memory status after this trial: 
Memory allocated:  3401.2294921875
Memory cached:  3432.0
--------------------  Trial  30   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 8, 'D_dagger_layer_units_exponent_6': 10, 'D_dagger_layer_units_exponent_7': 10, 'log_learning_rate': -2.576059520046725, 'log_learning_rate_D': -4.325837907259867, 'log_learning_rate_D_dagger': -3.7329740483320624, 'training_batch_size': 9, 'training_p': 4}
	 epoch  0 training error:  tensor(1.4304, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2782.0
	 epoch  10 training error:  tensor(0.3443, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  20 training error:  tensor(0.3107, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  30 training error:  tensor(0.2920, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  40 training error:  tensor(0.2871, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  50 training error:  tensor(0.2873, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  60 training error:  tensor(0.2838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  70 training error:  tensor(0.2791, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  80 training error:  tensor(0.2781, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
	 epoch  90 training error:  tensor(0.2791, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.24951171875
Memory cached:  2802.0
[I 2024-03-01 22:58:03,488] Trial 30 finished with value: 0.199973002076149 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 8, 'D_dagger_layer_units_exponent_6': 10, 'D_dagger_layer_units_exponent_7': 10, 'log_learning_rate': -2.576059520046725, 'log_learning_rate_D': -4.325837907259867, 'log_learning_rate_D_dagger': -3.7329740483320624, 'training_batch_size': 9, 'training_p': 4}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  962.1722898483276
Memory status after this trial: 
Memory allocated:  3332.98974609375
Memory cached:  3376.0
--------------------  Trial  31   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.7234747138004531, 'log_learning_rate_D': -3.849874935566011, 'log_learning_rate_D_dagger': -3.2112611651996095, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(1.8023, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.2124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.0972, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.0842, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.0757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.0821, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.0898, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0551, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.0614, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.0830078125
Memory cached:  2716.0
[I 2024-03-01 23:58:10,984] Trial 31 finished with value: 0.029591655358672142 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.7234747138004531, 'log_learning_rate_D': -3.849874935566011, 'log_learning_rate_D_dagger': -3.2112611651996095, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3606.5824019908905
Memory status after this trial: 
Memory allocated:  3056.80224609375
Memory cached:  3092.0
--------------------  Trial  32   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.733797580896419, 'log_learning_rate_D': -3.9249022166760774, 'log_learning_rate_D_dagger': -3.3841555536756025, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(12.5897, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  10 training error:  tensor(0.3144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  20 training error:  tensor(0.1472, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  30 training error:  tensor(0.1283, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  40 training error:  tensor(0.0559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  50 training error:  tensor(0.0697, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  60 training error:  tensor(0.0597, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  70 training error:  tensor(0.0407, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  80 training error:  tensor(0.0469, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
	 epoch  90 training error:  tensor(0.0569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2696.0
[I 2024-03-02 00:57:32,221] Trial 32 finished with value: 0.0412452295422554 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.733797580896419, 'log_learning_rate_D': -3.9249022166760774, 'log_learning_rate_D_dagger': -3.3841555536756025, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3560.167883872986
Memory status after this trial: 
Memory allocated:  3236.86962890625
Memory cached:  3274.0
--------------------  Trial  33   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -1.536585979243136, 'log_learning_rate_D': -4.533108054745481, 'log_learning_rate_D_dagger': -2.6612090311159173, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.3478, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  10 training error:  tensor(0.3055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  20 training error:  tensor(0.1798, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  30 training error:  tensor(0.1181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  40 training error:  tensor(0.0749, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  50 training error:  tensor(0.0832, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  60 training error:  tensor(0.0548, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  70 training error:  tensor(0.0424, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  80 training error:  tensor(0.0422, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
	 epoch  90 training error:  tensor(0.0404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.28369140625
Memory cached:  2770.0
[I 2024-03-02 01:49:48,874] Trial 33 finished with value: 0.046138763427734375 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -1.536585979243136, 'log_learning_rate_D': -4.533108054745481, 'log_learning_rate_D_dagger': -2.6612090311159173, 'training_batch_size': 6, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3135.8213546276093
Memory status after this trial: 
Memory allocated:  3132.291015625
Memory cached:  3156.0
--------------------  Trial  34   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 6, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 8, 'log_learning_rate': -2.603445569416748, 'log_learning_rate_D': -4.22253528924519, 'log_learning_rate_D_dagger': -3.9316955733355554, 'training_batch_size': 7, 'training_p': 6}
	 epoch  0 training error:  tensor(0.7186, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2720.0
	 epoch  10 training error:  tensor(0.3474, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2726.0
	 epoch  20 training error:  tensor(0.3299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2728.0
	 epoch  30 training error:  tensor(0.3105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2724.0
	 epoch  40 training error:  tensor(0.2381, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2722.0
	 epoch  50 training error:  tensor(0.1689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2726.0
	 epoch  60 training error:  tensor(0.1486, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2726.0
	 epoch  70 training error:  tensor(0.1169, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2722.0
	 epoch  80 training error:  tensor(0.0940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2726.0
	 epoch  90 training error:  tensor(0.0911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.16162109375
Memory cached:  2722.0
[I 2024-03-02 02:19:45,019] Trial 34 finished with value: 0.07839250564575195 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 6, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 8, 'log_learning_rate': -2.603445569416748, 'log_learning_rate_D': -4.22253528924519, 'log_learning_rate_D_dagger': -3.9316955733355554, 'training_batch_size': 7, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1795.4751119613647
Memory status after this trial: 
Memory allocated:  2996.81787109375
Memory cached:  3036.0
--------------------  Trial  35   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 6, 'log_learning_rate': -1.2829090606710667, 'log_learning_rate_D': -4.958377486210074, 'log_learning_rate_D_dagger': -3.4873233922113145, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(81.3846, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  10 training error:  tensor(1.0772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  20 training error:  tensor(0.3470, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  30 training error:  tensor(0.3334, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  40 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  50 training error:  tensor(0.2051, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  60 training error:  tensor(0.2685, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  70 training error:  tensor(0.1640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  80 training error:  tensor(0.2029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
	 epoch  90 training error:  tensor(2.0074, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1751.82275390625
Memory cached:  2696.0
[I 2024-03-02 03:12:22,367] Trial 35 finished with value: 0.10161173343658447 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 6, 'log_learning_rate': -1.2829090606710667, 'log_learning_rate_D': -4.958377486210074, 'log_learning_rate_D_dagger': -3.4873233922113145, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3156.4874591827393
Memory status after this trial: 
Memory allocated:  2908.30419921875
Memory cached:  2950.0
--------------------  Trial  36   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.9314142700996642, 'log_learning_rate_D': -3.406839738866288, 'log_learning_rate_D_dagger': -4.3151033378487345, 'training_batch_size': 8, 'training_p': 5}
	 epoch  0 training error:  tensor(3.5236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  10 training error:  tensor(0.5028, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  20 training error:  tensor(0.3531, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  30 training error:  tensor(0.3401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  40 training error:  tensor(0.3303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  50 training error:  tensor(0.3208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  60 training error:  tensor(0.3140, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  70 training error:  tensor(0.3094, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  80 training error:  tensor(0.3038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
	 epoch  90 training error:  tensor(0.2994, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.2119140625
Memory cached:  2700.0
[I 2024-03-02 03:29:37,682] Trial 36 finished with value: 0.18340565264225006 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.9314142700996642, 'log_learning_rate_D': -3.406839738866288, 'log_learning_rate_D_dagger': -4.3151033378487345, 'training_batch_size': 8, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1034.6793801784515
Memory status after this trial: 
Memory allocated:  2808.2353515625
Memory cached:  2850.0
--------------------  Trial  37   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 4, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 10, 'log_learning_rate': -2.4025920208709417, 'log_learning_rate_D': -4.6476120216803185, 'log_learning_rate_D_dagger': -3.064857072262238, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9997, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  10 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  20 training error:  tensor(0.2419, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  30 training error:  tensor(0.1738, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  40 training error:  tensor(0.1483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  50 training error:  tensor(0.1379, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  60 training error:  tensor(0.0996, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  70 training error:  tensor(0.1212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  80 training error:  tensor(0.1083, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
	 epoch  90 training error:  tensor(0.0798, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.0732421875
Memory cached:  2756.0
[I 2024-03-02 04:26:25,788] Trial 37 finished with value: 0.04492693766951561 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 4, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 10, 'log_learning_rate': -2.4025920208709417, 'log_learning_rate_D': -4.6476120216803185, 'log_learning_rate_D_dagger': -3.064857072262238, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3406.9412384033203
Memory status after this trial: 
Memory allocated:  3337.482421875
Memory cached:  3372.0
--------------------  Trial  38   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 4, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -1.3722565210741338, 'log_learning_rate_D': -4.18978220901657, 'log_learning_rate_D_dagger': -3.263203862126002, 'training_batch_size': 7, 'training_p': 4}
	 epoch  0 training error:  tensor(69.5767, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  10 training error:  tensor(0.3934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  20 training error:  tensor(0.2856, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  30 training error:  tensor(37.9756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  40 training error:  tensor(3.2087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  50 training error:  tensor(0.3370, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  60 training error:  tensor(0.2805, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  70 training error:  tensor(0.2770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  80 training error:  tensor(0.2748, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
	 epoch  90 training error:  tensor(0.2709, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1725.52734375
Memory cached:  2636.0
[I 2024-03-02 04:53:22,417] Trial 38 finished with value: 0.18489369750022888 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 4, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -1.3722565210741338, 'log_learning_rate_D': -4.18978220901657, 'log_learning_rate_D_dagger': -3.263203862126002, 'training_batch_size': 7, 'training_p': 4}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1615.8780007362366
Memory status after this trial: 
Memory allocated:  2493.33203125
Memory cached:  2636.0
--------------------  Trial  39   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.9248511168167117, 'log_learning_rate_D': -3.893644983166852, 'log_learning_rate_D_dagger': -2.6415453106488433, 'training_batch_size': 12, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2656.0
	 epoch  10 training error:  tensor(0.4967, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2660.0
	 epoch  20 training error:  tensor(0.3751, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2658.0
	 epoch  30 training error:  tensor(0.3656, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2662.0
	 epoch  40 training error:  tensor(0.3438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2664.0
	 epoch  50 training error:  tensor(0.3355, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2666.0
	 epoch  60 training error:  tensor(0.3267, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2662.0
	 epoch  70 training error:  tensor(0.3193, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2660.0
	 epoch  80 training error:  tensor(0.3171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2662.0
	 epoch  90 training error:  tensor(0.3055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1731.1552734375
Memory cached:  2660.0
[I 2024-03-02 05:01:18,305] Trial 39 finished with value: 0.1925772726535797 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.9248511168167117, 'log_learning_rate_D': -3.893644983166852, 'log_learning_rate_D_dagger': -2.6415453106488433, 'training_batch_size': 12, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  475.2915668487549
Memory status after this trial: 
Memory allocated:  2491.32958984375
Memory cached:  2646.0
--------------------  Trial  40   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'D_layers': 2, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 7, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -2.9372780232838087, 'log_learning_rate_D': -4.391617598704332, 'log_learning_rate_D_dagger': -3.9773548421370997, 'training_batch_size': 11, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9341, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2646.0
	 epoch  10 training error:  tensor(0.6127, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2652.0
	 epoch  20 training error:  tensor(0.4040, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2656.0
	 epoch  30 training error:  tensor(0.3689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2658.0
	 epoch  40 training error:  tensor(0.3607, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2654.0
	 epoch  50 training error:  tensor(0.3561, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2650.0
	 epoch  60 training error:  tensor(0.3490, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2654.0
	 epoch  70 training error:  tensor(0.3451, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2652.0
	 epoch  80 training error:  tensor(0.3418, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2656.0
	 epoch  90 training error:  tensor(0.3462, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1732.1279296875
Memory cached:  2650.0
[I 2024-03-02 05:07:32,144] Trial 40 finished with value: 0.20215237140655518 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'D_layers': 2, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 7, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -2.9372780232838087, 'log_learning_rate_D': -4.391617598704332, 'log_learning_rate_D_dagger': -3.9773548421370997, 'training_batch_size': 11, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  373.3282742500305
Memory status after this trial: 
Memory allocated:  2551.00048828125
Memory cached:  2644.0
--------------------  Trial  41   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.7151267860337793, 'log_learning_rate_D': -3.943899556759779, 'log_learning_rate_D_dagger': -3.3814667248943606, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(3.6423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3244, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.1773, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.1152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.0964, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.0641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.0689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.0481, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0409, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.0566, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1775.71533203125
Memory cached:  2716.0
[I 2024-03-02 06:04:04,001] Trial 41 finished with value: 0.04970167577266693 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.7151267860337793, 'log_learning_rate_D': -3.943899556759779, 'log_learning_rate_D_dagger': -3.3814667248943606, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3391.011830329895
Memory status after this trial: 
Memory allocated:  3138.9599609375
Memory cached:  3176.0
--------------------  Trial  42   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.6940382311789484, 'log_learning_rate_D': -3.6354273907996664, 'log_learning_rate_D_dagger': -3.7248761032423063, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(5.0132, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  10 training error:  tensor(0.3380, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  20 training error:  tensor(0.3188, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  30 training error:  tensor(0.1245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  40 training error:  tensor(0.0887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  50 training error:  tensor(0.0591, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  60 training error:  tensor(0.0513, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  70 training error:  tensor(0.0514, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  80 training error:  tensor(0.0536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
	 epoch  90 training error:  tensor(0.0578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.68603515625
Memory cached:  2756.0
[I 2024-03-02 07:00:29,748] Trial 42 finished with value: 0.02172061800956726 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.6940382311789484, 'log_learning_rate_D': -3.6354273907996664, 'log_learning_rate_D_dagger': -3.7248761032423063, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3384.908411502838
Memory status after this trial: 
Memory allocated:  3146.26318359375
Memory cached:  3174.0
--------------------  Trial  43   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -1.094514901795593, 'log_learning_rate_D': -3.564261947289918, 'log_learning_rate_D_dagger': -3.742981642600348, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(156.0055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  10 training error:  tensor(0.4108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  20 training error:  tensor(0.3417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  30 training error:  tensor(0.3274, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  40 training error:  tensor(0.1973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  50 training error:  tensor(0.1346, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  60 training error:  tensor(0.1027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  70 training error:  tensor(0.1532, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  80 training error:  tensor(0.0862, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
	 epoch  90 training error:  tensor(0.0712, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1744.24951171875
Memory cached:  2696.0
[I 2024-03-02 07:55:54,233] Trial 43 finished with value: 0.05217823013663292 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -1.094514901795593, 'log_learning_rate_D': -3.564261947289918, 'log_learning_rate_D_dagger': -3.742981642600348, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3323.652185678482
Memory status after this trial: 
Memory allocated:  2653.5810546875
Memory cached:  2694.0
--------------------  Trial  44   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.124778853089042, 'log_learning_rate_D': -3.303922476580392, 'log_learning_rate_D_dagger': -4.288850686905695, 'training_batch_size': 7, 'training_p': 5}
	 epoch  0 training error:  tensor(1.5866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2726.0
	 epoch  10 training error:  tensor(0.3628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2722.0
	 epoch  20 training error:  tensor(0.3300, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2722.0
	 epoch  30 training error:  tensor(0.3218, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2724.0
	 epoch  40 training error:  tensor(0.3168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2724.0
	 epoch  50 training error:  tensor(0.3119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2726.0
	 epoch  60 training error:  tensor(0.2971, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2724.0
	 epoch  70 training error:  tensor(0.2803, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2722.0
	 epoch  80 training error:  tensor(0.2734, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2724.0
	 epoch  90 training error:  tensor(0.2641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1781.154296875
Memory cached:  2726.0
[I 2024-03-02 08:23:37,666] Trial 44 finished with value: 0.15978765487670898 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.124778853089042, 'log_learning_rate_D': -3.303922476580392, 'log_learning_rate_D_dagger': -4.288850686905695, 'training_batch_size': 7, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1662.5761795043945
Memory status after this trial: 
Memory allocated:  2943.25830078125
Memory cached:  2978.0
--------------------  Trial  45   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -2.3976174463495235, 'log_learning_rate_D': -3.6768583874900695, 'log_learning_rate_D_dagger': -3.0013985454186, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(0.9077, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2676.0
	 epoch  10 training error:  tensor(0.1823, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  20 training error:  tensor(0.1845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  30 training error:  tensor(0.1748, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  40 training error:  tensor(0.1721, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  50 training error:  tensor(0.1562, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  60 training error:  tensor(0.0883, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  70 training error:  tensor(0.0840, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  80 training error:  tensor(0.0500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
	 epoch  90 training error:  tensor(0.0567, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1741.85302734375
Memory cached:  2696.0
[I 2024-03-02 09:19:00,406] Trial 45 finished with value: 0.04977912828326225 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -2.3976174463495235, 'log_learning_rate_D': -3.6768583874900695, 'log_learning_rate_D_dagger': -3.0013985454186, 'training_batch_size': 6, 'training_p': 2}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3321.8517487049103
Memory status after this trial: 
Memory allocated:  2613.61279296875
Memory cached:  2654.0
--------------------  Trial  46   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.712821234281597, 'log_learning_rate_D': -2.7997013418117054, 'log_learning_rate_D_dagger': -2.8340992991255542, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.4474, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  10 training error:  tensor(0.3397, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  20 training error:  tensor(0.3291, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  30 training error:  tensor(0.3147, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  40 training error:  tensor(0.3920, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  50 training error:  tensor(0.3130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  60 training error:  tensor(0.3033, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  70 training error:  tensor(0.3155, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  80 training error:  tensor(0.1558, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
	 epoch  90 training error:  tensor(0.1098, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1773.138671875
Memory cached:  2776.0
[I 2024-03-02 10:18:29,233] Trial 46 finished with value: 0.07852157205343246 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.712821234281597, 'log_learning_rate_D': -2.7997013418117054, 'log_learning_rate_D_dagger': -2.8340992991255542, 'training_batch_size': 6, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3567.9464399814606
Memory status after this trial: 
Memory allocated:  2881.6669921875
Memory cached:  2914.0
--------------------  Trial  47   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -1.4998572879498375, 'log_learning_rate_D': -4.190428469215456, 'log_learning_rate_D_dagger': -3.7418960950424904, 'training_batch_size': 8, 'training_p': 6}
	 epoch  0 training error:  tensor(16.1564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2644.0
	 epoch  10 training error:  tensor(0.8486, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2646.0
	 epoch  20 training error:  tensor(0.5202, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2648.0
	 epoch  30 training error:  tensor(0.3546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2646.0
	 epoch  40 training error:  tensor(0.3522, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2646.0
	 epoch  50 training error:  tensor(0.3475, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2650.0
	 epoch  60 training error:  tensor(0.3424, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2646.0
	 epoch  70 training error:  tensor(0.3384, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2646.0
	 epoch  80 training error:  tensor(0.3340, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2648.0
	 epoch  90 training error:  tensor(0.3350, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1738.041015625
Memory cached:  2648.0
[I 2024-03-02 10:35:45,894] Trial 47 finished with value: 0.22129257023334503 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -1.4998572879498375, 'log_learning_rate_D': -4.190428469215456, 'log_learning_rate_D_dagger': -3.7418960950424904, 'training_batch_size': 8, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1036.0052399635315
Memory status after this trial: 
Memory allocated:  2849.73583984375
Memory cached:  2896.0
--------------------  Trial  48   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 5, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 6, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -2.772081181737083, 'log_learning_rate_D': -3.133494865092841, 'log_learning_rate_D_dagger': -3.219030379999294, 'training_batch_size': 10, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9904, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.296875
Memory cached:  2678.0
	 epoch  10 training error:  tensor(0.4735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.3505859375
Memory cached:  2702.0
	 epoch  20 training error:  tensor(0.3716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1737.12744140625
Memory cached:  2680.0
	 epoch  30 training error:  tensor(0.3455, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.3505859375
Memory cached:  2704.0
	 epoch  40 training error:  tensor(0.3251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.8505859375
Memory cached:  2700.0
	 epoch  50 training error:  tensor(0.3187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.1005859375
Memory cached:  2704.0
	 epoch  60 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.87744140625
Memory cached:  2682.0
	 epoch  70 training error:  tensor(0.2573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.3505859375
Memory cached:  2696.0
	 epoch  80 training error:  tensor(0.2227, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1737.1005859375
Memory cached:  2696.0
	 epoch  90 training error:  tensor(0.1944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1736.046875
Memory cached:  2682.0
[I 2024-03-02 10:43:41,752] Trial 48 finished with value: 0.13092191517353058 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 5, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 6, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -2.772081181737083, 'log_learning_rate_D': -3.133494865092841, 'log_learning_rate_D_dagger': -3.219030379999294, 'training_batch_size': 10, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  475.25836420059204
Memory status after this trial: 
Memory allocated:  2763.37060546875
Memory cached:  2800.0
--------------------  Trial  49   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 4, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -2.49598086451974, 'log_learning_rate_D': -1.460372658762099, 'log_learning_rate_D_dagger': -4.679345477460323, 'training_batch_size': 7, 'training_p': 4}
	 epoch  0 training error:  tensor(0.7938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2786.0
	 epoch  10 training error:  tensor(0.6400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2790.0
	 epoch  20 training error:  tensor(0.6390, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2790.0
	 epoch  30 training error:  tensor(0.6410, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2788.0
	 epoch  40 training error:  tensor(0.6403, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2788.0
	 epoch  50 training error:  tensor(0.6389, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2794.0
	 epoch  60 training error:  tensor(0.6438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2792.0
	 epoch  70 training error:  tensor(0.6433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2792.0
	 epoch  80 training error:  tensor(0.6384, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2786.0
	 epoch  90 training error:  tensor(0.6386, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1770.4921875
Memory cached:  2794.0
[I 2024-03-02 11:11:23,128] Trial 49 finished with value: 0.6098594069480896 and parameters: {'W_layers': 3, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 4, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -2.49598086451974, 'log_learning_rate_D': -1.460372658762099, 'log_learning_rate_D_dagger': -4.679345477460323, 'training_batch_size': 7, 'training_p': 4}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1660.7256908416748
Memory status after this trial: 
Memory allocated:  2923.25732421875
Memory cached:  2950.0
--------------------  Trial  50   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 4, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -2.0661545246629194, 'log_learning_rate_D': -3.565134707553611, 'log_learning_rate_D_dagger': -3.6280932807375024, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(0.8806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  10 training error:  tensor(0.3483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  20 training error:  tensor(0.2813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  30 training error:  tensor(0.1382, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  40 training error:  tensor(0.0899, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  50 training error:  tensor(0.0793, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  60 training error:  tensor(0.0911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  70 training error:  tensor(0.0631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  80 training error:  tensor(0.0633, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
	 epoch  90 training error:  tensor(0.0641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1734.27978515625
Memory cached:  2636.0
[I 2024-03-02 12:15:16,048] Trial 50 finished with value: 0.06990495324134827 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 5, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 4, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -2.0661545246629194, 'log_learning_rate_D': -3.565134707553611, 'log_learning_rate_D_dagger': -3.6280932807375024, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3831.6872680187225
Memory status after this trial: 
Memory allocated:  2691.22412109375
Memory cached:  2734.0
--------------------  Trial  51   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.8416405062703205, 'log_learning_rate_D': -3.833998630762282, 'log_learning_rate_D_dagger': -3.381971291119639, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(5.2669, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.3091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.1588, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.1246, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.0915, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.0649, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.0701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.0399, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.982421875
Memory cached:  2716.0
[I 2024-03-02 13:15:52,322] Trial 51 finished with value: 0.051674630492925644 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.8416405062703205, 'log_learning_rate_D': -3.833998630762282, 'log_learning_rate_D_dagger': -3.381971291119639, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3635.197108030319
Memory status after this trial: 
Memory allocated:  3236.86962890625
Memory cached:  3274.0
--------------------  Trial  52   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.695087081091144, 'log_learning_rate_D': -4.075031151097049, 'log_learning_rate_D_dagger': -2.945848993549443, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(19.5241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3485, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.2087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.3717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.0878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.0618, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.0538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.0624, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.0473, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1777.685546875
Memory cached:  2716.0
[I 2024-03-02 14:16:25,925] Trial 52 finished with value: 0.028744135051965714 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -1.695087081091144, 'log_learning_rate_D': -4.075031151097049, 'log_learning_rate_D_dagger': -2.945848993549443, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3632.585959672928
Memory status after this trial: 
Memory allocated:  3237.01025390625
Memory cached:  3274.0
--------------------  Trial  53   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -1.3593458811240189, 'log_learning_rate_D': -4.1288576647497575, 'log_learning_rate_D_dagger': -2.750693221877853, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(407.6779, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3871, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  20 training error:  tensor(0.3197, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  30 training error:  tensor(0.2844, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  40 training error:  tensor(0.2005, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  50 training error:  tensor(0.1111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  60 training error:  tensor(0.0875, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  70 training error:  tensor(0.0611, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0514, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
	 epoch  90 training error:  tensor(0.0461, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1766.49169921875
Memory cached:  2716.0
[I 2024-03-02 15:16:41,993] Trial 53 finished with value: 0.04821261391043663 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -1.3593458811240189, 'log_learning_rate_D': -4.1288576647497575, 'log_learning_rate_D_dagger': -2.750693221877853, 'training_batch_size': 6, 'training_p': 5}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3615.1445939540863
Memory status after this trial: 
Memory allocated:  3061.638671875
Memory cached:  3100.0
--------------------  Trial  54   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 5, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -1.0064862476733496, 'log_learning_rate_D': -3.7413823607805003, 'log_learning_rate_D_dagger': -2.965755584320195, 'training_batch_size': 7, 'training_p': 6}
	 epoch  0 training error:  tensor(1347.7054, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2716.0
	 epoch  10 training error:  tensor(0.3510, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2712.0
	 epoch  20 training error:  tensor(0.2933, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2714.0
	 epoch  30 training error:  tensor(0.1580, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2712.0
	 epoch  40 training error:  tensor(0.1081, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2714.0
	 epoch  50 training error:  tensor(0.1251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2714.0
	 epoch  60 training error:  tensor(0.0729, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2714.0
	 epoch  70 training error:  tensor(0.0764, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2710.0
	 epoch  80 training error:  tensor(0.0617, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2710.0
	 epoch  90 training error:  tensor(0.0560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1746.80126953125
Memory cached:  2710.0
[I 2024-03-02 15:47:54,609] Trial 54 finished with value: 0.04165497049689293 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 5, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -1.0064862476733496, 'log_learning_rate_D': -3.7413823607805003, 'log_learning_rate_D_dagger': -2.965755584320195, 'training_batch_size': 7, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1871.7996680736542
Memory status after this trial: 
Memory allocated:  2929.99072265625
Memory cached:  2972.0
--------------------  Trial  55   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 10, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -3.102514721256589, 'log_learning_rate_D': -3.2522493072795733, 'log_learning_rate_D_dagger': -2.506981641775088, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(2.0692, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2818.0
	 epoch  10 training error:  tensor(0.4470, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  20 training error:  tensor(0.3704, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  30 training error:  tensor(0.3639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  40 training error:  tensor(0.3648, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  50 training error:  tensor(0.3547, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  60 training error:  tensor(0.3562, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  70 training error:  tensor(0.3379, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  80 training error:  tensor(0.3371, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
	 epoch  90 training error:  tensor(0.3351, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1812.2529296875
Memory cached:  2840.0
[I 2024-03-02 16:45:40,648] Trial 55 finished with value: 0.25484904646873474 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 10, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -3.102514721256589, 'log_learning_rate_D': -3.2522493072795733, 'log_learning_rate_D_dagger': -2.506981641775088, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3464.937008857727
Memory status after this trial: 
Memory allocated:  3585.548828125
Memory cached:  3618.0
--------------------  Trial  56   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 9, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -1.6485644193985682, 'log_learning_rate_D': -3.4724141250122984, 'log_learning_rate_D_dagger': -2.8991309529159652, 'training_batch_size': 7, 'training_p': 6}
	 epoch  0 training error:  tensor(77.6092, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2698.0
	 epoch  10 training error:  tensor(0.5741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2686.0
	 epoch  20 training error:  tensor(0.4227, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2686.0
	 epoch  30 training error:  tensor(0.3395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2688.0
	 epoch  40 training error:  tensor(0.3296, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2684.0
	 epoch  50 training error:  tensor(0.3265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2688.0
	 epoch  60 training error:  tensor(0.2958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2684.0
	 epoch  70 training error:  tensor(0.1909, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2686.0
	 epoch  80 training error:  tensor(0.1639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2686.0
	 epoch  90 training error:  tensor(0.1672, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1742.63671875
Memory cached:  2686.0
[I 2024-03-02 17:16:32,481] Trial 56 finished with value: 0.112279511988163 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 9, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -1.6485644193985682, 'log_learning_rate_D': -3.4724141250122984, 'log_learning_rate_D_dagger': -2.8991309529159652, 'training_batch_size': 7, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1851.050169467926
Memory status after this trial: 
Memory allocated:  2933.2041015625
Memory cached:  2974.0
--------------------  Trial  57   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.2149060904099165, 'log_learning_rate_D': -4.3193337598594725, 'log_learning_rate_D_dagger': -3.2343620592868314, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(6.4664, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  10 training error:  tensor(0.3579, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  20 training error:  tensor(0.2699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  30 training error:  tensor(0.1433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  40 training error:  tensor(0.1029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  50 training error:  tensor(0.0716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  60 training error:  tensor(0.0775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  70 training error:  tensor(0.0548, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  80 training error:  tensor(0.0633, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
	 epoch  90 training error:  tensor(0.0551, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1782.76123046875
Memory cached:  2736.0
[I 2024-03-02 18:11:39,025] Trial 57 finished with value: 0.0270866546779871 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.2149060904099165, 'log_learning_rate_D': -4.3193337598594725, 'log_learning_rate_D_dagger': -3.2343620592868314, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3305.6377816200256
Memory status after this trial: 
Memory allocated:  3301.04541015625
Memory cached:  3342.0
--------------------  Trial  58   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -2.1977710792802005, 'log_learning_rate_D': -4.335055967908529, 'log_learning_rate_D_dagger': -4.080432175702312, 'training_batch_size': 7, 'training_p': 8}
	 epoch  0 training error:  tensor(1.4299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2710.0
	 epoch  10 training error:  tensor(0.3866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2710.0
	 epoch  20 training error:  tensor(0.3529, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2712.0
	 epoch  30 training error:  tensor(0.3254, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2712.0
	 epoch  40 training error:  tensor(0.2260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2706.0
	 epoch  50 training error:  tensor(0.1700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2712.0
	 epoch  60 training error:  tensor(0.1362, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2710.0
	 epoch  70 training error:  tensor(0.1064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2716.0
	 epoch  80 training error:  tensor(0.0991, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2712.0
	 epoch  90 training error:  tensor(0.0934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1763.54248046875
Memory cached:  2712.0
[I 2024-03-02 18:37:15,289] Trial 58 finished with value: 0.06873047351837158 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -2.1977710792802005, 'log_learning_rate_D': -4.335055967908529, 'log_learning_rate_D_dagger': -4.080432175702312, 'training_batch_size': 7, 'training_p': 8}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1535.5807855129242
Memory status after this trial: 
Memory allocated:  2828.41357421875
Memory cached:  2872.0
--------------------  Trial  59   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -2.0045709482236673, 'log_learning_rate_D': -4.53218284347116, 'log_learning_rate_D_dagger': -3.4658016295905405, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(8.3605, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  10 training error:  tensor(0.3523, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  20 training error:  tensor(0.3389, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  30 training error:  tensor(0.1807, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  40 training error:  tensor(0.1643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  50 training error:  tensor(0.1231, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  60 training error:  tensor(0.0966, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  70 training error:  tensor(0.1004, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  80 training error:  tensor(0.0582, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
	 epoch  90 training error:  tensor(0.0790, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1786.26611328125
Memory cached:  2836.0
[I 2024-03-02 19:42:40,915] Trial 59 finished with value: 0.031239861622452736 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -2.0045709482236673, 'log_learning_rate_D': -4.53218284347116, 'log_learning_rate_D_dagger': -3.4658016295905405, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3924.6197600364685
Memory status after this trial: 
Memory allocated:  3497.9638671875
Memory cached:  3532.0
--------------------  Trial  60   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.3299071432590903, 'log_learning_rate_D': -4.838592375758774, 'log_learning_rate_D_dagger': -3.078424664396801, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(4.6427, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2708.0
	 epoch  10 training error:  tensor(0.3705, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2706.0
	 epoch  20 training error:  tensor(0.3508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2704.0
	 epoch  30 training error:  tensor(0.3104, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2708.0
	 epoch  40 training error:  tensor(0.2260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2706.0
	 epoch  50 training error:  tensor(0.1770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2704.0
	 epoch  60 training error:  tensor(0.1795, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2702.0
	 epoch  70 training error:  tensor(0.1492, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2706.0
	 epoch  80 training error:  tensor(0.1368, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2708.0
	 epoch  90 training error:  tensor(0.1233, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1747.6220703125
Memory cached:  2708.0
[I 2024-03-02 20:12:01,294] Trial 60 finished with value: 0.08813804388046265 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 6, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.3299071432590903, 'log_learning_rate_D': -4.838592375758774, 'log_learning_rate_D_dagger': -3.078424664396801, 'training_batch_size': 7, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1734.5086941719055
Memory status after this trial: 
Memory allocated:  2970.41943359375
Memory cached:  3012.0
--------------------  Trial  61   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 7, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 9, 'D_layer_units_exponent_6': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -2.008543456862988, 'log_learning_rate_D': -4.5418838302220825, 'log_learning_rate_D_dagger': -3.505292712271574, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(13.8504, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  10 training error:  tensor(0.3497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  20 training error:  tensor(0.3310, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  30 training error:  tensor(0.1787, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  40 training error:  tensor(0.1531, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  50 training error:  tensor(0.1166, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  60 training error:  tensor(0.0977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  70 training error:  tensor(0.0765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  80 training error:  tensor(0.0886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
	 epoch  90 training error:  tensor(0.0722, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1789.57763671875
Memory cached:  2816.0
[I 2024-03-02 21:19:36,765] Trial 61 finished with value: 0.033606868237257004 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 7, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 9, 'D_layer_units_exponent_6': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -2.008543456862988, 'log_learning_rate_D': -4.5418838302220825, 'log_learning_rate_D_dagger': -3.505292712271574, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  4054.3526690006256
Memory status after this trial: 
Memory allocated:  3560.3388671875
Memory cached:  3596.0
--------------------  Trial  62   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 10, 'D_layers': 6, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 9, 'log_learning_rate': -1.797005035952242, 'log_learning_rate_D': -4.033872625748041, 'log_learning_rate_D_dagger': -3.8454262098991157, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(56.5719, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1837.443359375
Memory cached:  2894.0
	 epoch  10 training error:  tensor(0.4295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2892.0
	 epoch  20 training error:  tensor(0.3368, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2896.0
	 epoch  30 training error:  tensor(0.3569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2896.0
	 epoch  40 training error:  tensor(0.1502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2896.0
	 epoch  50 training error:  tensor(0.1608, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2898.0
	 epoch  60 training error:  tensor(0.1155, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2898.0
	 epoch  70 training error:  tensor(0.0852, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2894.0
	 epoch  80 training error:  tensor(0.0887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2898.0
	 epoch  90 training error:  tensor(0.0742, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1838.42138671875
Memory cached:  2894.0
[I 2024-03-02 22:23:35,599] Trial 62 finished with value: 1.9559268951416016 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 10, 'D_layers': 6, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 9, 'log_learning_rate': -1.797005035952242, 'log_learning_rate_D': -4.033872625748041, 'log_learning_rate_D_dagger': -3.8454262098991157, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3837.7644412517548
Memory status after this trial: 
Memory allocated:  3947.2412109375
Memory cached:  3990.0
--------------------  Trial  63   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'log_learning_rate': -1.5673721256332915, 'log_learning_rate_D': -4.446553261194368, 'log_learning_rate_D_dagger': -3.1882415144602385, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(464.5772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  10 training error:  tensor(0.4142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  20 training error:  tensor(0.3549, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  30 training error:  tensor(0.1669, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  40 training error:  tensor(0.1458, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  50 training error:  tensor(0.2509, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  60 training error:  tensor(0.0996, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  70 training error:  tensor(0.0752, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  80 training error:  tensor(0.0631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
	 epoch  90 training error:  tensor(0.0645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.9912109375
Memory cached:  2796.0
[I 2024-03-02 23:28:52,389] Trial 63 finished with value: 0.02802693285048008 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'log_learning_rate': -1.5673721256332915, 'log_learning_rate_D': -4.446553261194368, 'log_learning_rate_D_dagger': -3.1882415144602385, 'training_batch_size': 6, 'training_p': 8}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3915.578165769577
Memory status after this trial: 
Memory allocated:  3573.95166015625
Memory cached:  3612.0
--------------------  Trial  64   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 4, 'D_layers': 5, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'log_learning_rate': -1.5376156970549983, 'log_learning_rate_D': -4.239564714180853, 'log_learning_rate_D_dagger': -3.225893811694915, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(3.6690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  10 training error:  tensor(0.3753, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  20 training error:  tensor(0.2442, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  30 training error:  tensor(0.1680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  40 training error:  tensor(0.1214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  50 training error:  tensor(0.1045, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  60 training error:  tensor(0.1039, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  70 training error:  tensor(0.1090, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  80 training error:  tensor(0.0866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
	 epoch  90 training error:  tensor(0.0747, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1765.50634765625
Memory cached:  2736.0
[I 2024-03-03 00:23:07,861] Trial 64 finished with value: 0.03506132960319519 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 4, 'D_layers': 5, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'log_learning_rate': -1.5376156970549983, 'log_learning_rate_D': -4.239564714180853, 'log_learning_rate_D_dagger': -3.225893811694915, 'training_batch_size': 6, 'training_p': 8}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3254.3527381420135
Memory status after this trial: 
Memory allocated:  3067.2978515625
Memory cached:  3106.0
--------------------  Trial  65   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 4, 'log_learning_rate': -1.2793950602307065, 'log_learning_rate_D': -3.984399272201026, 'log_learning_rate_D_dagger': -2.7600348004901876, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(585.2578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  10 training error:  tensor(0.4282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  20 training error:  tensor(0.3032, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  30 training error:  tensor(0.1499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  40 training error:  tensor(0.1777, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  50 training error:  tensor(0.0854, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  60 training error:  tensor(0.1048, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  70 training error:  tensor(0.0839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  80 training error:  tensor(0.0744, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
	 epoch  90 training error:  tensor(0.0808, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.4501953125
Memory cached:  2796.0
[I 2024-03-03 01:30:33,297] Trial 65 finished with value: 0.04059654474258423 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 4, 'D_layers': 6, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 4, 'log_learning_rate': -1.2793950602307065, 'log_learning_rate_D': -3.984399272201026, 'log_learning_rate_D_dagger': -2.7600348004901876, 'training_batch_size': 6, 'training_p': 8}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  4044.100728034973
Memory status after this trial: 
Memory allocated:  3424.87744140625
Memory cached:  3462.0
--------------------  Trial  66   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'D_layers': 5, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -1.5727725805217767, 'log_learning_rate_D': -4.67697007150559, 'log_learning_rate_D_dagger': -3.1535760041832184, 'training_batch_size': 7, 'training_p': 8}
	 epoch  0 training error:  tensor(208.8012, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2804.0
	 epoch  10 training error:  tensor(5.8350, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2808.0
	 epoch  20 training error:  tensor(0.4873, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2806.0
	 epoch  30 training error:  tensor(0.5041, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2802.0
	 epoch  40 training error:  tensor(0.3640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2808.0
	 epoch  50 training error:  tensor(0.3384, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2808.0
	 epoch  60 training error:  tensor(0.2360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2808.0
	 epoch  70 training error:  tensor(0.1596, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2802.0
	 epoch  80 training error:  tensor(0.1211, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2810.0
	 epoch  90 training error:  tensor(0.1162, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1776.60888671875
Memory cached:  2804.0
[I 2024-03-03 02:05:47,356] Trial 66 finished with value: 0.06180727481842041 and parameters: {'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'D_layers': 5, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -1.5727725805217767, 'log_learning_rate_D': -4.67697007150559, 'log_learning_rate_D_dagger': -3.1535760041832184, 'training_batch_size': 7, 'training_p': 8}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  2113.0378448963165
Memory status after this trial: 
Memory allocated:  3213.32861328125
Memory cached:  3250.0
--------------------  Trial  67   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 4, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -1.8713070247164347, 'log_learning_rate_D': -4.274926860520874, 'log_learning_rate_D_dagger': -3.599298291429156, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(2.3718, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  10 training error:  tensor(0.3421, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  20 training error:  tensor(0.1859, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  30 training error:  tensor(0.1869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  40 training error:  tensor(0.1323, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  50 training error:  tensor(0.1057, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  60 training error:  tensor(0.1117, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  70 training error:  tensor(0.0765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  80 training error:  tensor(0.0880, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
	 epoch  90 training error:  tensor(0.0796, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.83740234375
Memory cached:  2836.0
[I 2024-03-03 03:03:32,784] Trial 67 finished with value: 0.1047520861029625 and parameters: {'W_layers': 2, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 4, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -1.8713070247164347, 'log_learning_rate_D': -4.274926860520874, 'log_learning_rate_D_dagger': -3.599298291429156, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3464.2995488643646
Memory status after this trial: 
Memory allocated:  3288.697265625
Memory cached:  3354.0
--------------------  Trial  68   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -1.4068587348710102, 'log_learning_rate_D': -4.374097911006732, 'log_learning_rate_D_dagger': -2.52336131600843, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(17.1445, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  10 training error:  tensor(0.3818, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  20 training error:  tensor(0.2373, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  30 training error:  tensor(0.1942, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  40 training error:  tensor(0.1780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  50 training error:  tensor(0.2741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  60 training error:  tensor(0.1317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  70 training error:  tensor(0.0914, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  80 training error:  tensor(0.0824, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
	 epoch  90 training error:  tensor(0.1184, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1796.3720703125
Memory cached:  2756.0
[I 2024-03-03 04:04:51,336] Trial 68 finished with value: 0.03937028720974922 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 8, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -1.4068587348710102, 'log_learning_rate_D': -4.374097911006732, 'log_learning_rate_D_dagger': -2.52336131600843, 'training_batch_size': 6, 'training_p': 8}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3659.323000431061
Memory status after this trial: 
Memory allocated:  3376.5927734375
Memory cached:  3410.0
--------------------  Trial  69   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -1.238141529168906, 'log_learning_rate_D': -4.10146591478698, 'log_learning_rate_D_dagger': -3.329087401691144, 'training_batch_size': 7, 'training_p': 7}
	 epoch  0 training error:  tensor(1242.0778, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2724.0
	 epoch  10 training error:  tensor(0.3935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2724.0
	 epoch  20 training error:  tensor(0.3448, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2726.0
	 epoch  30 training error:  tensor(0.3447, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2722.0
	 epoch  40 training error:  tensor(0.1932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2732.0
	 epoch  50 training error:  tensor(0.1317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2726.0
	 epoch  60 training error:  tensor(0.1139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2726.0
	 epoch  70 training error:  tensor(0.1153, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2726.0
	 epoch  80 training error:  tensor(0.0912, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2726.0
	 epoch  90 training error:  tensor(0.0764, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1785.85302734375
Memory cached:  2724.0
[I 2024-03-03 04:37:45,967] Trial 69 finished with value: 0.09925884008407593 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 6, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 9, 'log_learning_rate': -1.238141529168906, 'log_learning_rate_D': -4.10146591478698, 'log_learning_rate_D_dagger': -3.329087401691144, 'training_batch_size': 7, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  1960.8161962032318
Memory status after this trial: 
Memory allocated:  3300.09814453125
Memory cached:  3334.0
--------------------  Trial  70   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 5, 'W_layer_units_exponent_6': 10, 'W_layer_units_exponent_7': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'log_learning_rate': -2.196332998580764, 'log_learning_rate_D': -4.890311197032488, 'log_learning_rate_D_dagger': -2.958622282630498, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(3.2857, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  10 training error:  tensor(0.3459, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  20 training error:  tensor(0.3230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  30 training error:  tensor(0.2009, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  40 training error:  tensor(0.1837, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  50 training error:  tensor(0.1639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  60 training error:  tensor(0.1594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  70 training error:  tensor(0.1473, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  80 training error:  tensor(0.1053, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
	 epoch  90 training error:  tensor(0.0958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1752.9716796875
Memory cached:  2696.0
[I 2024-03-03 05:33:11,500] Trial 70 finished with value: 0.0718565285205841 and parameters: {'W_layers': 8, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 7, 'W_layer_units_exponent_5': 5, 'W_layer_units_exponent_6': 10, 'W_layer_units_exponent_7': 10, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'log_learning_rate': -2.196332998580764, 'log_learning_rate_D': -4.890311197032488, 'log_learning_rate_D_dagger': -2.958622282630498, 'training_batch_size': 6, 'training_p': 6}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3314.7464349269867
Memory status after this trial: 
Memory allocated:  2983.33642578125
Memory cached:  3022.0
--------------------  Trial  71   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -1.6637869978333546, 'log_learning_rate_D': -4.484688069980077, 'log_learning_rate_D_dagger': -3.4628974486518174, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(52.5685, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  10 training error:  tensor(0.3543, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  20 training error:  tensor(0.3303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  30 training error:  tensor(0.1508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  40 training error:  tensor(0.1105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  50 training error:  tensor(0.0834, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  60 training error:  tensor(0.0682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  70 training error:  tensor(0.0698, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  80 training error:  tensor(0.0420, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
	 epoch  90 training error:  tensor(0.0496, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1778.67724609375
Memory cached:  2776.0
[I 2024-03-03 06:34:21,901] Trial 71 finished with value: 0.053943514823913574 and parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 6, 'D_layers': 6, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -1.6637869978333546, 'log_learning_rate_D': -4.484688069980077, 'log_learning_rate_D_dagger': -3.4628974486518174, 'training_batch_size': 6, 'training_p': 7}. Best is trial 27 with value: 0.019390789791941643.
Time for this trial:  3646.664885997772
Memory status after this trial: 
Memory allocated:  3374.2197265625
Memory cached:  3412.0
--------------------  Trial  72   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 7, 'D_layers': 6, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 9, 'log_learning_rate': -2.0722646139655807, 'log_learning_rate_D': -4.559792736586435, 'log_learning_rate_D_dagger': -3.5309514925568117, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(6.3245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  10 training error:  tensor(0.3483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  20 training error:  tensor(0.2261, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  30 training error:  tensor(0.1185, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  40 training error:  tensor(0.0845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  50 training error:  tensor(0.1186, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  60 training error:  tensor(0.0619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  70 training error:  tensor(0.0722, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
	 epoch  80 training error:  tensor(0.0644, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1804.43896484375
Memory cached:  2916.0
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
Pruned database has best value 0.019390789791941643.
--------------------  Trial  74   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'W_layer_units_exponent_5': 7, 'D_layers': 6, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 9, 'log_learning_rate': -2.0571107562716686, 'log_learning_rate_D': -4.56972289543298, 'log_learning_rate_D_dagger': -3.530290036040169, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(18.9845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  644.0
	 epoch  10 training error:  tensor(0.3354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  474.0
	 epoch  20 training error:  tensor(0.2785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  436.0
	 epoch  30 training error:  tensor(0.1489, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  460.0
	 epoch  40 training error:  tensor(0.0937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  472.0
	 epoch  50 training error:  tensor(0.0762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  408.0
	 epoch  60 training error:  tensor(0.0725, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  460.0
	 epoch  70 training error:  tensor(0.0775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  456.0
	 epoch  80 training error:  tensor(0.0564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  498.0
	 epoch  90 training error:  tensor(0.0649, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  101.97265625
Memory cached:  512.0
Time for this trial:  4040.5037863254547
Memory status after this trial: 
Memory allocated:  1946.396484375
Memory cached:  1966.0
--------------------  Trial  75   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 10, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -1.7954978112402, 'log_learning_rate_D': -3.894050074790703, 'log_learning_rate_D_dagger': -3.1446890186359897, 'training_batch_size': 6, 'training_p': 8}
	 epoch  0 training error:  tensor(10.1774, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  700.0
	 epoch  10 training error:  tensor(0.3790, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  452.0
	 epoch  20 training error:  tensor(0.2630, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  408.0
	 epoch  30 training error:  tensor(0.1759, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  412.0
	 epoch  40 training error:  tensor(0.1042, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  394.0
	 epoch  50 training error:  tensor(0.1245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  456.0
	 epoch  60 training error:  tensor(0.0893, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  422.0
	 epoch  70 training error:  tensor(0.0850, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  450.0
	 epoch  80 training error:  tensor(0.0580, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  426.0
	 epoch  90 training error:  tensor(0.0493, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  115.79296875
Memory cached:  434.0
Time for this trial:  3860.336449623108
Memory status after this trial: 
Memory allocated:  2036.9501953125
Memory cached:  2054.0
--------------------  Trial  76   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 4, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -1.9847520711594144, 'log_learning_rate_D': -2.113408771198525, 'log_learning_rate_D_dagger': -3.2982987492451614, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(13.3052, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.34765625
Memory cached:  634.0
	 epoch  10 training error:  tensor(1.0265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  594.0
	 epoch  20 training error:  tensor(1.0248, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  600.0
	 epoch  30 training error:  tensor(1.0237, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  604.0
	 epoch  40 training error:  tensor(1.0225, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  584.0
	 epoch  50 training error:  tensor(1.0212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  600.0
	 epoch  60 training error:  tensor(1.0198, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  602.0
	 epoch  70 training error:  tensor(1.0177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  584.0
	 epoch  80 training error:  tensor(1.0162, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  572.0
	 epoch  90 training error:  tensor(1.0146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  110.33935546875
Memory cached:  574.0
Time for this trial:  3861.1924936771393
Memory status after this trial: 
Memory allocated:  1931.99365234375
Memory cached:  1974.0
--------------------  Trial  77   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 10, 'D_layers': 7, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_layer_units_exponent_5': 7, 'D_layer_units_exponent_6': 8, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -2.668530611341591, 'log_learning_rate_D': -4.762147005934904, 'log_learning_rate_D_dagger': -3.7069409728267333, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(2.1794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  748.0
	 epoch  10 training error:  tensor(0.3461, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  596.0
	 epoch  20 training error:  tensor(0.3416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  520.0
	 epoch  30 training error:  tensor(0.3295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  532.0
	 epoch  40 training error:  tensor(0.1948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  474.0
	 epoch  50 training error:  tensor(0.1593, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  528.0
	 epoch  60 training error:  tensor(0.1380, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  518.0
	 epoch  70 training error:  tensor(0.1190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  486.0
	 epoch  80 training error:  tensor(0.1036, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  512.0
	 epoch  90 training error:  tensor(0.0621, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  77.2265625
Memory cached:  488.0
Time for this trial:  4020.450704097748
Memory status after this trial: 
Memory allocated:  1878.7431640625
Memory cached:  1914.0
--------------------  Trial  78   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 5, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'log_learning_rate': -2.2914786982241035, 'log_learning_rate_D': -4.075205401858692, 'log_learning_rate_D_dagger': -3.86943765050258, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.9425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  518.0
	 epoch  10 training error:  tensor(0.3256, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  264.0
	 epoch  20 training error:  tensor(0.3026, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  230.0
	 epoch  30 training error:  tensor(0.2940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  240.0
	 epoch  40 training error:  tensor(0.2095, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  228.0
	 epoch  50 training error:  tensor(0.1278, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  228.0
	 epoch  60 training error:  tensor(0.0799, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  242.0
	 epoch  70 training error:  tensor(0.0845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  236.0
	 epoch  80 training error:  tensor(0.0722, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  248.0
	 epoch  90 training error:  tensor(0.0595, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  70.6376953125
Memory cached:  250.0
Time for this trial:  3498.2631306648254
Memory status after this trial: 
Memory allocated:  1286.90673828125
Memory cached:  1312.0
--------------------  Trial  79   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 6, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 10, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 9, 'D_dagger_layer_units_exponent_6': 8, 'log_learning_rate': -1.453346711700317, 'log_learning_rate_D': -4.435563488199046, 'log_learning_rate_D_dagger': -3.007765444033319, 'training_batch_size': 7, 'training_p': 3}
	 epoch  0 training error:  tensor(3.1122, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  396.0
	 epoch  10 training error:  tensor(0.2552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  250.0
	 epoch  20 training error:  tensor(0.2921, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  232.0
	 epoch  30 training error:  tensor(0.2388, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  234.0
	 epoch  40 training error:  tensor(0.2320, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  216.0
	 epoch  50 training error:  tensor(0.1754, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  216.0
	 epoch  60 training error:  tensor(0.1508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  216.0
	 epoch  70 training error:  tensor(0.1356, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  202.0
	 epoch  80 training error:  tensor(0.1282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  218.0
	 epoch  90 training error:  tensor(0.1239, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  57.662109375
Memory cached:  204.0
Time for this trial:  1710.1131420135498
Memory status after this trial: 
Memory allocated:  1407.095703125
Memory cached:  1432.0
--------------------  Trial  80   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 9, 'W_layer_units_exponent_4': 7, 'D_layers': 7, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 8, 'D_layer_units_exponent_5': 8, 'D_layer_units_exponent_6': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 10, 'log_learning_rate': -1.6028782621536406, 'log_learning_rate_D': -4.628579531593733, 'log_learning_rate_D_dagger': -2.774518774357688, 'training_batch_size': 11, 'training_p': 6}
	 epoch  0 training error:  tensor(1.0412, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  304.0
	 epoch  10 training error:  tensor(2.4181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  532.0
	 epoch  20 training error:  tensor(39.5752, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  506.0
	 epoch  30 training error:  tensor(176.4064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  514.0
	 epoch  40 training error:  tensor(150.2393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  528.0
	 epoch  50 training error:  tensor(6.1532, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  480.0
	 epoch  60 training error:  tensor(15.5919, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  484.0
	 epoch  70 training error:  tensor(3.8547, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  532.0
	 epoch  80 training error:  tensor(28.4877, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  506.0
	 epoch  90 training error:  tensor(34.4511, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  71.921875
Memory cached:  500.0
Time for this trial:  618.9840798377991
Memory status after this trial: 
Memory allocated:  1634.66455078125
Memory cached:  1670.0
--------------------  Trial  81   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'log_learning_rate': -2.79129701489288, 'log_learning_rate_D': -3.664517365327065, 'log_learning_rate_D_dagger': -3.390720491267764, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(0.5858, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  276.0
	 epoch  10 training error:  tensor(0.3567, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  220.0
	 epoch  20 training error:  tensor(0.2523, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  212.0
	 epoch  30 training error:  tensor(0.0937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  208.0
	 epoch  40 training error:  tensor(0.0948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  196.0
	 epoch  50 training error:  tensor(0.0934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  194.0
	 epoch  60 training error:  tensor(0.0715, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  198.0
	 epoch  70 training error:  tensor(0.0745, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  204.0
	 epoch  80 training error:  tensor(0.0645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  208.0
	 epoch  90 training error:  tensor(0.0488, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  64.6513671875
Memory cached:  194.0
Time for this trial:  3501.9834434986115
Memory status after this trial: 
Memory allocated:  1337.904296875
Memory cached:  1370.0
--------------------  Trial  82   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'log_learning_rate': -2.8101806388586623, 'log_learning_rate_D': -3.617310871769457, 'log_learning_rate_D_dagger': -2.897871311468542, 'training_batch_size': 10, 'training_p': 6}
	 epoch  0 training error:  tensor(1.0013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  144.0
	 epoch  10 training error:  tensor(0.4219, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  240.0
	 epoch  20 training error:  tensor(0.4091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  228.0
	 epoch  30 training error:  tensor(0.3750, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  234.0
	 epoch  40 training error:  tensor(0.3677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  238.0
	 epoch  50 training error:  tensor(0.3582, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  228.0
	 epoch  60 training error:  tensor(0.3606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  242.0
	 epoch  70 training error:  tensor(0.3567, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  242.0
	 epoch  80 training error:  tensor(0.3487, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  244.0
	 epoch  90 training error:  tensor(0.3388, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  36.189453125
Memory cached:  248.0
Time for this trial:  486.62908482551575
Memory status after this trial: 
Memory allocated:  1134.50048828125
Memory cached:  1152.0
--------------------  Trial  83   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'log_learning_rate': -2.417090591293412, 'log_learning_rate_D': -3.7512781957500048, 'log_learning_rate_D_dagger': -3.4277931760340263, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(1.5151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  272.0
	 epoch  10 training error:  tensor(0.3663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  240.0
	 epoch  20 training error:  tensor(0.3435, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  212.0
	 epoch  30 training error:  tensor(0.2350, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  208.0
	 epoch  40 training error:  tensor(0.1021, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  204.0
	 epoch  50 training error:  tensor(0.0847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  204.0
	 epoch  60 training error:  tensor(0.0975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  198.0
	 epoch  70 training error:  tensor(0.0568, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  204.0
	 epoch  80 training error:  tensor(0.0604, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  202.0
	 epoch  90 training error:  tensor(0.0500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.9423828125
Memory cached:  196.0
Time for this trial:  3336.001370191574
Memory status after this trial: 
Memory allocated:  1308.54296875
Memory cached:  1340.0
--------------------  Trial  84   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 7, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 9, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 8, 'log_learning_rate': -2.076272432505759, 'log_learning_rate_D': -3.5210508296575362, 'log_learning_rate_D_dagger': -3.656095269508421, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(2.2575, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  564.0
	 epoch  10 training error:  tensor(0.3613, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  444.0
	 epoch  20 training error:  tensor(0.2804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  384.0
	 epoch  30 training error:  tensor(0.1776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  338.0
	 epoch  40 training error:  tensor(0.0957, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  350.0
	 epoch  50 training error:  tensor(0.1248, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  318.0
	 epoch  60 training error:  tensor(0.0589, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  336.0
	 epoch  70 training error:  tensor(0.0533, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  362.0
	 epoch  80 training error:  tensor(0.0571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  316.0
	 epoch  90 training error:  tensor(0.0562, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  50.21533203125
Memory cached:  396.0
Time for this trial:  3664.4836037158966
Memory status after this trial: 
Memory allocated:  1368.32373046875
Memory cached:  1384.0
--------------------  Trial  85   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 10, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 9, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 7, 'D_layer_units_exponent_5': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'log_learning_rate': -2.9581375019505627, 'log_learning_rate_D': -3.86097463702522, 'log_learning_rate_D_dagger': -3.2432597692503284, 'training_batch_size': 6, 'training_p': 7}
	 epoch  0 training error:  tensor(0.5426, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  312.0
	 epoch  10 training error:  tensor(0.3501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  214.0
	 epoch  20 training error:  tensor(0.2002, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  190.0
	 epoch  30 training error:  tensor(0.1091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  184.0
	 epoch  40 training error:  tensor(0.0718, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  184.0
	 epoch  50 training error:  tensor(0.0658, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  180.0
	 epoch  60 training error:  tensor(0.0757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  180.0
	 epoch  70 training error:  tensor(0.0531, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  176.0
	 epoch  80 training error:  tensor(0.0519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  182.0
	 epoch  90 training error:  tensor(0.0540, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.41748046875
Memory cached:  176.0
Time for this trial:  3646.1581077575684
Memory status after this trial: 
Memory allocated:  1583.9326171875
Memory cached:  1604.0
--------------------  Trial  86   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'log_learning_rate': -2.5137606485061137, 'log_learning_rate_D': -3.6912079615270548, 'log_learning_rate_D_dagger': -3.143208702732097, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.7844, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  572.0
	 epoch  10 training error:  tensor(0.3484, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  436.0
	 epoch  20 training error:  tensor(0.3170, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  400.0
	 epoch  30 training error:  tensor(0.1668, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  414.0
	 epoch  40 training error:  tensor(0.1003, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  406.0
	 epoch  50 training error:  tensor(0.0686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  384.0
	 epoch  60 training error:  tensor(0.0947, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  408.0
	 epoch  70 training error:  tensor(0.0585, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  400.0
	 epoch  80 training error:  tensor(0.0578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  406.0
	 epoch  90 training error:  tensor(0.0559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  88.35107421875
Memory cached:  384.0
Time for this trial:  4027.2038152217865
Memory status after this trial: 
Memory allocated:  1543.5751953125
Memory cached:  1564.0
--------------------  Trial  87   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 5, 'W_layer_units_exponent_6': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'log_learning_rate': -2.468416134530132, 'log_learning_rate_D': -3.698366907451217, 'log_learning_rate_D_dagger': -3.10763570304817, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.7358, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  538.0
	 epoch  10 training error:  tensor(0.3232, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  438.0
	 epoch  20 training error:  tensor(0.1254, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  352.0
	 epoch  30 training error:  tensor(0.0880, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  332.0
	 epoch  40 training error:  tensor(0.0686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  332.0
	 epoch  50 training error:  tensor(0.0763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  326.0
	 epoch  60 training error:  tensor(0.0631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  326.0
	 epoch  70 training error:  tensor(0.0382, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  322.0
	 epoch  80 training error:  tensor(0.0401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  334.0
	 epoch  90 training error:  tensor(0.2285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  106.27978515625
Memory cached:  340.0
Time for this trial:  3893.666305065155
Memory status after this trial: 
Memory allocated:  1782.8818359375
Memory cached:  1804.0
--------------------  Trial  88   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 6, 'W_layer_units_exponent_6': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 9, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 6, 'log_learning_rate': -2.7144460864807143, 'log_learning_rate_D': -3.353813750369011, 'log_learning_rate_D_dagger': -1.7061077355619951, 'training_batch_size': 7, 'training_p': 6}
	 epoch  0 training error:  tensor(3.0573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  302.0
	 epoch  10 training error:  tensor(0.6524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  250.0
	 epoch  20 training error:  tensor(0.6641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  256.0
	 epoch  30 training error:  tensor(0.6536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  248.0
	 epoch  40 training error:  tensor(0.6510, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  264.0
	 epoch  50 training error:  tensor(0.6587, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  256.0
	 epoch  60 training error:  tensor(0.6502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  254.0
	 epoch  70 training error:  tensor(0.6559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  246.0
	 epoch  80 training error:  tensor(0.6539, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  258.0
	 epoch  90 training error:  tensor(0.6526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  65.70263671875
Memory cached:  254.0
Time for this trial:  1946.8044567108154
Memory status after this trial: 
Memory allocated:  1276.43798828125
Memory cached:  1302.0
--------------------  Trial  89   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.2060049107057074, 'log_learning_rate_D': -3.966226498273231, 'log_learning_rate_D_dagger': -3.318217135252355, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(3.5761, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  444.0
	 epoch  10 training error:  tensor(0.3046, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  284.0
	 epoch  20 training error:  tensor(0.1680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  284.0
	 epoch  30 training error:  tensor(0.0848, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  270.0
	 epoch  40 training error:  tensor(0.0581, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  292.0
	 epoch  50 training error:  tensor(0.0637, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  284.0
	 epoch  60 training error:  tensor(0.0641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  294.0
	 epoch  70 training error:  tensor(0.0396, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  294.0
	 epoch  80 training error:  tensor(0.0495, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  282.0
	 epoch  90 training error:  tensor(0.0347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  62.447265625
Memory cached:  282.0
Time for this trial:  3661.645751476288
Memory status after this trial: 
Memory allocated:  1246.41552734375
Memory cached:  1274.0
--------------------  Trial  90   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'log_learning_rate': -2.209272186731431, 'log_learning_rate_D': -3.959533056471698, 'log_learning_rate_D_dagger': -3.0353724408366864, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(3.1812, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  444.0
	 epoch  10 training error:  tensor(0.3077, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  352.0
	 epoch  20 training error:  tensor(0.1643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  316.0
	 epoch  30 training error:  tensor(0.0764, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  322.0
	 epoch  40 training error:  tensor(0.0526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  330.0
	 epoch  50 training error:  tensor(0.0674, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  348.0
	 epoch  60 training error:  tensor(0.0622, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  336.0
	 epoch  70 training error:  tensor(0.0505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  334.0
	 epoch  80 training error:  tensor(0.0367, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  322.0
	 epoch  90 training error:  tensor(0.0552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  52.328125
Memory cached:  316.0
Time for this trial:  3160.7848789691925
Memory status after this trial: 
Memory allocated:  1123.90869140625
Memory cached:  1154.0
--------------------  Trial  91   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.3181020192516693, 'log_learning_rate_D': -3.214966995751131, 'log_learning_rate_D_dagger': -3.317032167441085, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(3.2491, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  468.0
	 epoch  10 training error:  tensor(0.3025, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  342.0
	 epoch  20 training error:  tensor(0.2131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  320.0
	 epoch  30 training error:  tensor(0.1045, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  294.0
	 epoch  40 training error:  tensor(0.0635, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  306.0
	 epoch  50 training error:  tensor(0.0399, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  286.0
	 epoch  60 training error:  tensor(0.0439, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  280.0
	 epoch  70 training error:  tensor(0.0340, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  290.0
	 epoch  80 training error:  tensor(0.0347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  320.0
	 epoch  90 training error:  tensor(0.0398, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  60.4033203125
Memory cached:  358.0
res:  tensor(0.0173, grad_fn=<ToCopyBackward0>)
self.bestValue:  0.019390789791941643
Save this model!
Time for this trial:  3668.8059692382812
Memory status after this trial: 
Memory allocated:  1258.701171875
Memory cached:  1286.0
--------------------  Trial  92   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.605472069853798, 'log_learning_rate_D': -3.035048694879072, 'log_learning_rate_D_dagger': -3.352369161183347, 'training_batch_size': 8, 'training_p': 4}
	 epoch  0 training error:  tensor(4.4870, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1410.0
	 epoch  10 training error:  tensor(0.6788, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1412.0
	 epoch  20 training error:  tensor(0.6391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1412.0
	 epoch  30 training error:  tensor(0.6381, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1420.0
	 epoch  40 training error:  tensor(0.3991, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1410.0
	 epoch  50 training error:  tensor(0.3013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1408.0
	 epoch  60 training error:  tensor(0.3039, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1412.0
	 epoch  70 training error:  tensor(0.2955, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1408.0
	 epoch  80 training error:  tensor(0.2797, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1410.0
	 epoch  90 training error:  tensor(0.2745, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.55810546875
Memory cached:  1410.0
Time for this trial:  1134.982033252716
Memory status after this trial: 
Memory allocated:  2376.39697265625
Memory cached:  2400.0
--------------------  Trial  93   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.317778964832453, 'log_learning_rate_D': -3.239151174993054, 'log_learning_rate_D_dagger': -3.1586508074146447, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.5035, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1430.0
	 epoch  10 training error:  tensor(0.2877, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1734.0
	 epoch  20 training error:  tensor(0.1179, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1748.0
	 epoch  30 training error:  tensor(0.0835, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1758.0
	 epoch  40 training error:  tensor(0.0739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1756.0
	 epoch  50 training error:  tensor(0.0748, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1726.0
	 epoch  60 training error:  tensor(0.0420, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1726.0
	 epoch  70 training error:  tensor(0.0442, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1702.0
	 epoch  80 training error:  tensor(0.0416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1736.0
	 epoch  90 training error:  tensor(0.0409, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1752.0
Time for this trial:  3697.7514021396637
Memory status after this trial: 
Memory allocated:  2492.056640625
Memory cached:  2514.0
--------------------  Trial  94   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.297885743120699, 'log_learning_rate_D': -3.247712776404895, 'log_learning_rate_D_dagger': -3.564110740562941, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(3.6973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1434.0
	 epoch  10 training error:  tensor(0.2994, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1730.0
	 epoch  20 training error:  tensor(0.1576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1730.0
	 epoch  30 training error:  tensor(0.0811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1724.0
	 epoch  40 training error:  tensor(0.0597, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1740.0
	 epoch  50 training error:  tensor(0.0536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1762.0
	 epoch  60 training error:  tensor(0.0494, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1776.0
	 epoch  70 training error:  tensor(0.0440, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1756.0
	 epoch  80 training error:  tensor(0.0670, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1766.0
	 epoch  90 training error:  tensor(0.0431, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.9013671875
Memory cached:  1742.0
Time for this trial:  3699.624050140381
Memory status after this trial: 
Memory allocated:  2492.056640625
Memory cached:  2514.0
--------------------  Trial  95   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.4877809656780387, 'log_learning_rate_D': -2.844049463512114, 'log_learning_rate_D_dagger': -3.1675942937966846, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.7435, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1428.0
	 epoch  10 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1746.0
	 epoch  20 training error:  tensor(0.1563, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1722.0
	 epoch  30 training error:  tensor(0.0687, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1740.0
	 epoch  40 training error:  tensor(0.0828, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1728.0
	 epoch  50 training error:  tensor(0.0602, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1742.0
	 epoch  60 training error:  tensor(0.0494, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1744.0
	 epoch  70 training error:  tensor(0.0526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1718.0
	 epoch  80 training error:  tensor(0.0395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1748.0
	 epoch  90 training error:  tensor(0.0510, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.49560546875
Memory cached:  1738.0
Time for this trial:  3712.9662778377533
Memory status after this trial: 
Memory allocated:  2489.59375
Memory cached:  2512.0
--------------------  Trial  96   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.845012484647812, 'log_learning_rate_D': -2.951552238444804, 'log_learning_rate_D_dagger': -3.357034496842858, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.9678, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1434.0
	 epoch  10 training error:  tensor(0.3004, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1700.0
	 epoch  20 training error:  tensor(0.2878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1724.0
	 epoch  30 training error:  tensor(0.0961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1714.0
	 epoch  40 training error:  tensor(0.0504, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1722.0
	 epoch  50 training error:  tensor(0.0377, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1736.0
	 epoch  60 training error:  tensor(0.0354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1712.0
	 epoch  70 training error:  tensor(0.0480, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1724.0
	 epoch  80 training error:  tensor(0.0406, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1700.0
	 epoch  90 training error:  tensor(0.0343, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.52685546875
Memory cached:  1734.0
Time for this trial:  3600.1413893699646
Memory status after this trial: 
Memory allocated:  2493.01953125
Memory cached:  2514.0
--------------------  Trial  97   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.8733548372000737, 'log_learning_rate_D': -2.8922521050483603, 'log_learning_rate_D_dagger': -3.3136389400282793, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1434.0
	 epoch  10 training error:  tensor(0.2955, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1690.0
	 epoch  20 training error:  tensor(0.1569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1702.0
	 epoch  30 training error:  tensor(0.0970, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1708.0
	 epoch  40 training error:  tensor(0.0888, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1662.0
	 epoch  50 training error:  tensor(0.0500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1686.0
	 epoch  60 training error:  tensor(0.0472, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1688.0
	 epoch  70 training error:  tensor(0.0400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1676.0
	 epoch  80 training error:  tensor(0.0533, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1686.0
	 epoch  90 training error:  tensor(0.0407, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.52685546875
Memory cached:  1678.0
Time for this trial:  3341.4823863506317
Memory status after this trial: 
Memory allocated:  2367.26953125
Memory cached:  2390.0
--------------------  Trial  98   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.5235248829276924, 'log_learning_rate_D': -2.685355023821059, 'log_learning_rate_D_dagger': -3.7949815759343988, 'training_batch_size': 7, 'training_p': 4}
	 epoch  0 training error:  tensor(4.7513, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1448.0
	 epoch  10 training error:  tensor(0.6251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1438.0
	 epoch  20 training error:  tensor(0.2780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1434.0
	 epoch  30 training error:  tensor(0.3327, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1442.0
	 epoch  40 training error:  tensor(0.2655, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1438.0
	 epoch  50 training error:  tensor(0.2614, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1440.0
	 epoch  60 training error:  tensor(0.2643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1436.0
	 epoch  70 training error:  tensor(0.2581, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1434.0
	 epoch  80 training error:  tensor(0.1375, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1442.0
	 epoch  90 training error:  tensor(0.0982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.68408203125
Memory cached:  1434.0
Time for this trial:  1839.0204968452454
Memory status after this trial: 
Memory allocated:  2604.4013671875
Memory cached:  2624.0
--------------------  Trial  99   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 6, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.6717447128355194, 'log_learning_rate_D': -3.1871467298763547, 'log_learning_rate_D_dagger': -3.4124806051942946, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.9189, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1412.0
	 epoch  10 training error:  tensor(0.3030, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1620.0
	 epoch  20 training error:  tensor(0.2902, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1594.0
	 epoch  30 training error:  tensor(0.1498, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1588.0
	 epoch  40 training error:  tensor(0.1024, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1584.0
	 epoch  50 training error:  tensor(0.0661, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1588.0
	 epoch  60 training error:  tensor(0.0698, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1580.0
	 epoch  70 training error:  tensor(0.0554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1594.0
	 epoch  80 training error:  tensor(0.0499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1604.0
	 epoch  90 training error:  tensor(0.0319, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.2666015625
Memory cached:  1590.0
Time for this trial:  3508.3292553424835
Memory status after this trial: 
Memory allocated:  2470.482421875
Memory cached:  2496.0
--------------------  Trial  100   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 7, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -3.067159301718258, 'log_learning_rate_D': -2.9258873572104007, 'log_learning_rate_D_dagger': -3.0705674289928817, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.3479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1424.0
	 epoch  10 training error:  tensor(0.6086, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1664.0
	 epoch  20 training error:  tensor(0.3253, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1656.0
	 epoch  30 training error:  tensor(0.3015, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1640.0
	 epoch  40 training error:  tensor(0.2939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1618.0
	 epoch  50 training error:  tensor(0.2932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1650.0
	 epoch  60 training error:  tensor(0.3482, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1640.0
	 epoch  70 training error:  tensor(0.6448, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1646.0
	 epoch  80 training error:  tensor(0.6436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1630.0
	 epoch  90 training error:  tensor(0.6452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.47607421875
Memory cached:  1614.0
Time for this trial:  3347.347186088562
Memory status after this trial: 
Memory allocated:  2324.52734375
Memory cached:  2348.0
--------------------  Trial  101   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.3781298981421957, 'log_learning_rate_D': -2.511063048343749, 'log_learning_rate_D_dagger': -3.9960116604523153, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.9087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1426.0
	 epoch  10 training error:  tensor(0.3512, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1554.0
	 epoch  20 training error:  tensor(0.2965, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1558.0
	 epoch  30 training error:  tensor(0.2983, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1554.0
	 epoch  40 training error:  tensor(0.2925, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1576.0
	 epoch  50 training error:  tensor(0.2889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1556.0
	 epoch  60 training error:  tensor(0.2891, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1562.0
	 epoch  70 training error:  tensor(0.2253, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1560.0
	 epoch  80 training error:  tensor(0.1483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1572.0
	 epoch  90 training error:  tensor(0.1079, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.572265625
Memory cached:  1574.0
Time for this trial:  3509.480532884598
Memory status after this trial: 
Memory allocated:  2502.24951171875
Memory cached:  2524.0
--------------------  Trial  102   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.5935923900866893, 'log_learning_rate_D': -3.026555646841104, 'log_learning_rate_D_dagger': -3.5446054343733517, 'training_batch_size': 7, 'training_p': 5}
	 epoch  0 training error:  tensor(1.9912, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1406.0
	 epoch  10 training error:  tensor(0.3230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1430.0
	 epoch  20 training error:  tensor(0.2917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1432.0
	 epoch  30 training error:  tensor(0.2866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1424.0
	 epoch  40 training error:  tensor(0.2311, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1418.0
	 epoch  50 training error:  tensor(0.1326, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1414.0
	 epoch  60 training error:  tensor(0.1024, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1420.0
	 epoch  70 training error:  tensor(0.0799, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1416.0
	 epoch  80 training error:  tensor(0.0572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1428.0
	 epoch  90 training error:  tensor(0.0660, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.9189453125
Memory cached:  1420.0
Time for this trial:  1733.9103388786316
Memory status after this trial: 
Memory allocated:  2415.9716796875
Memory cached:  2436.0
--------------------  Trial  103   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.8064739776722236, 'log_learning_rate_D': -2.6087334419796884, 'log_learning_rate_D_dagger': -3.6502341414895403, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.4285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1462.0
	 epoch  10 training error:  tensor(0.3035, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1710.0
	 epoch  20 training error:  tensor(0.3491, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1710.0
	 epoch  30 training error:  tensor(0.3014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1664.0
	 epoch  40 training error:  tensor(0.2870, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1696.0
	 epoch  50 training error:  tensor(0.1452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1708.0
	 epoch  60 training error:  tensor(0.1008, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1724.0
	 epoch  70 training error:  tensor(0.0713, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1712.0
	 epoch  80 training error:  tensor(0.0617, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1712.0
	 epoch  90 training error:  tensor(0.0632, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.1435546875
Memory cached:  1710.0
Time for this trial:  3346.994276046753
Memory status after this trial: 
Memory allocated:  2416.220703125
Memory cached:  2436.0
--------------------  Trial  104   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.567037694977867, 'log_learning_rate_D': -3.0662970513776746, 'log_learning_rate_D_dagger': -3.4998837983482733, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.2432, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1398.0
	 epoch  10 training error:  tensor(0.2935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1708.0
	 epoch  20 training error:  tensor(0.2808, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1698.0
	 epoch  30 training error:  tensor(0.1130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1708.0
	 epoch  40 training error:  tensor(0.0836, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1702.0
	 epoch  50 training error:  tensor(0.0670, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1708.0
	 epoch  60 training error:  tensor(0.0534, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1704.0
	 epoch  70 training error:  tensor(0.0506, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1716.0
	 epoch  80 training error:  tensor(0.0423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1704.0
	 epoch  90 training error:  tensor(0.0395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.828125
Memory cached:  1708.0
Time for this trial:  3187.589784145355
Memory status after this trial: 
Memory allocated:  2282.6640625
Memory cached:  2306.0
--------------------  Trial  105   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.5733283697812537, 'log_learning_rate_D': -3.0693945713549464, 'log_learning_rate_D_dagger': -3.4836971506607455, 'training_batch_size': 7, 'training_p': 5}
	 epoch  0 training error:  tensor(5.6446, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1406.0
	 epoch  10 training error:  tensor(0.6487, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1420.0
	 epoch  20 training error:  tensor(0.3557, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1418.0
	 epoch  30 training error:  tensor(0.3572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1420.0
	 epoch  40 training error:  tensor(0.2945, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1418.0
	 epoch  50 training error:  tensor(0.1876, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1414.0
	 epoch  60 training error:  tensor(0.1113, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1422.0
	 epoch  70 training error:  tensor(0.0875, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1422.0
	 epoch  80 training error:  tensor(0.0736, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1416.0
	 epoch  90 training error:  tensor(0.0717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.767578125
Memory cached:  1432.0
Time for this trial:  1663.3067049980164
Memory status after this trial: 
Memory allocated:  2454.82666015625
Memory cached:  2476.0
--------------------  Trial  106   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'D_layers': 2, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 4, 'log_learning_rate': -2.4666751725071436, 'log_learning_rate_D': -3.446228925914854, 'log_learning_rate_D_dagger': -3.5566328247948795, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.8131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1402.0
	 epoch  10 training error:  tensor(0.2966, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1652.0
	 epoch  20 training error:  tensor(0.2831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1628.0
	 epoch  30 training error:  tensor(0.1577, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1630.0
	 epoch  40 training error:  tensor(0.1228, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1638.0
	 epoch  50 training error:  tensor(0.1053, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1640.0
	 epoch  60 training error:  tensor(0.0770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1656.0
	 epoch  70 training error:  tensor(0.0673, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1684.0
	 epoch  80 training error:  tensor(0.0616, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1672.0
	 epoch  90 training error:  tensor(0.0544, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1290.22509765625
Memory cached:  1644.0
Time for this trial:  3036.881229877472
Memory status after this trial: 
Memory allocated:  2249.11328125
Memory cached:  2272.0
--------------------  Trial  107   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.6068608605880015, 'log_learning_rate_D': -2.753490439264244, 'log_learning_rate_D_dagger': -3.2864081473602598, 'training_batch_size': 9, 'training_p': 4}
	 epoch  0 training error:  tensor(1.9009, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1368.0
	 epoch  10 training error:  tensor(0.7130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1366.0
	 epoch  20 training error:  tensor(0.4631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1366.0
	 epoch  30 training error:  tensor(0.3168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1372.0
	 epoch  40 training error:  tensor(0.2862, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1364.0
	 epoch  50 training error:  tensor(0.2697, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1366.0
	 epoch  60 training error:  tensor(0.2790, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1366.0
	 epoch  70 training error:  tensor(0.2726, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1366.0
	 epoch  80 training error:  tensor(0.2582, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1372.0
	 epoch  90 training error:  tensor(0.3128, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1285.25146484375
Memory cached:  1370.0
Time for this trial:  831.9031286239624
Memory status after this trial: 
Memory allocated:  2186.27392578125
Memory cached:  2208.0
--------------------  Trial  108   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 6, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.152174301218242, 'log_learning_rate_D': -3.177778834457458, 'log_learning_rate_D_dagger': -3.127107266323799, 'training_batch_size': 7, 'training_p': 5}
	 epoch  0 training error:  tensor(11.3169, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1404.0
	 epoch  10 training error:  tensor(0.3174, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1442.0
	 epoch  20 training error:  tensor(0.3019, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1436.0
	 epoch  30 training error:  tensor(0.2988, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1434.0
	 epoch  40 training error:  tensor(0.3030, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1436.0
	 epoch  50 training error:  tensor(0.2289, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1434.0
	 epoch  60 training error:  tensor(0.1416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1432.0
	 epoch  70 training error:  tensor(0.0978, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1434.0
	 epoch  80 training error:  tensor(0.0753, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1442.0
	 epoch  90 training error:  tensor(0.1326, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.42431640625
Memory cached:  1436.0
Time for this trial:  1661.835953950882
Memory status after this trial: 
Memory allocated:  2348.98095703125
Memory cached:  2370.0
--------------------  Trial  109   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 5, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.3690764051366373, 'log_learning_rate_D': -3.3468896752159236, 'log_learning_rate_D_dagger': -3.7220394759242232, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(3.1699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1390.0
	 epoch  10 training error:  tensor(0.2965, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1604.0
	 epoch  20 training error:  tensor(0.2844, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1552.0
	 epoch  30 training error:  tensor(0.1244, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1544.0
	 epoch  40 training error:  tensor(0.1006, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1554.0
	 epoch  50 training error:  tensor(0.0806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1530.0
	 epoch  60 training error:  tensor(0.0545, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1534.0
	 epoch  70 training error:  tensor(0.0558, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1544.0
	 epoch  80 training error:  tensor(0.0581, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1530.0
	 epoch  90 training error:  tensor(0.0499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.20068359375
Memory cached:  1526.0
Time for this trial:  3191.7762339115143
Memory status after this trial: 
Memory allocated:  2110.9873046875
Memory cached:  2134.0
--------------------  Trial  110   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 6, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.744475831479532, 'log_learning_rate_D': -3.681483816749255, 'log_learning_rate_D_dagger': -3.188064816347979, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(1.6445, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1440.0
	 epoch  10 training error:  tensor(0.2654, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1680.0
	 epoch  20 training error:  tensor(0.2643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1650.0
	 epoch  30 training error:  tensor(0.1092, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1640.0
	 epoch  40 training error:  tensor(0.0875, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1618.0
	 epoch  50 training error:  tensor(0.0621, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1622.0
	 epoch  60 training error:  tensor(0.0574, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1618.0
	 epoch  70 training error:  tensor(0.0519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1634.0
	 epoch  80 training error:  tensor(0.0502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1626.0
	 epoch  90 training error:  tensor(0.0420, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1286.955078125
Memory cached:  1620.0
Time for this trial:  3191.6207926273346
Memory status after this trial: 
Memory allocated:  2118.51806640625
Memory cached:  2140.0
--------------------  Trial  111   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -3.25810224840407, 'log_learning_rate_D': -3.282707795713965, 'log_learning_rate_D_dagger': -3.423141270197134, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.0217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1414.0
	 epoch  10 training error:  tensor(0.3290, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1524.0
	 epoch  20 training error:  tensor(0.2361, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1532.0
	 epoch  30 training error:  tensor(0.1400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1546.0
	 epoch  40 training error:  tensor(0.1264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1540.0
	 epoch  50 training error:  tensor(0.1064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1546.0
	 epoch  60 training error:  tensor(0.0935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1546.0
	 epoch  70 training error:  tensor(0.0741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1528.0
	 epoch  80 training error:  tensor(0.0686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1522.0
	 epoch  90 training error:  tensor(0.0757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.310546875
Memory cached:  1518.0
Time for this trial:  3062.8786754608154
Memory status after this trial: 
Memory allocated:  2290.4951171875
Memory cached:  2312.0
--------------------  Trial  112   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 7, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.462725910934176, 'log_learning_rate_D': -3.4895404664985894, 'log_learning_rate_D_dagger': -1.0770267890826601, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1385.8934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1378.0
	 epoch  10 training error:  tensor(0.6462, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1544.0
	 epoch  20 training error:  tensor(0.6456, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1552.0
	 epoch  30 training error:  tensor(0.6455, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1516.0
	 epoch  40 training error:  tensor(0.6530, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1528.0
	 epoch  50 training error:  tensor(0.6526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1524.0
	 epoch  60 training error:  tensor(0.6486, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1520.0
	 epoch  70 training error:  tensor(0.6468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1524.0
	 epoch  80 training error:  tensor(0.6502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1524.0
	 epoch  90 training error:  tensor(0.6497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1284.7470703125
Memory cached:  1528.0
Time for this trial:  3379.6754834651947
Memory status after this trial: 
Memory allocated:  2218.40966796875
Memory cached:  2244.0
--------------------  Trial  113   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.6747240317895096, 'log_learning_rate_D': -2.9036368585155703, 'log_learning_rate_D_dagger': -3.3511865993920766, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.1332, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1434.0
	 epoch  10 training error:  tensor(0.3040, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1696.0
	 epoch  20 training error:  tensor(0.2693, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1722.0
	 epoch  30 training error:  tensor(0.1292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1712.0
	 epoch  40 training error:  tensor(0.0706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1734.0
	 epoch  50 training error:  tensor(0.0546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1692.0
	 epoch  60 training error:  tensor(0.0655, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1728.0
	 epoch  70 training error:  tensor(0.0466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1728.0
	 epoch  80 training error:  tensor(0.0447, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1720.0
	 epoch  90 training error:  tensor(0.0370, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.8466796875
Memory cached:  1718.0
Time for this trial:  3165.035595178604
Memory status after this trial: 
Memory allocated:  2493.01953125
Memory cached:  2514.0
--------------------  Trial  114   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.840009374471408, 'log_learning_rate_D': -2.8437369333347933, 'log_learning_rate_D_dagger': -3.528602034332433, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.2230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1424.0
	 epoch  10 training error:  tensor(0.3050, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1700.0
	 epoch  20 training error:  tensor(0.2958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1674.0
	 epoch  30 training error:  tensor(0.1404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1678.0
	 epoch  40 training error:  tensor(0.1096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1668.0
	 epoch  50 training error:  tensor(0.0564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1678.0
	 epoch  60 training error:  tensor(0.0484, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1682.0
	 epoch  70 training error:  tensor(0.0647, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1704.0
	 epoch  80 training error:  tensor(0.0452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1680.0
	 epoch  90 training error:  tensor(0.0310, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.7177734375
Memory cached:  1672.0
Time for this trial:  3151.8904383182526
Memory status after this trial: 
Memory allocated:  2269.05615234375
Memory cached:  2294.0
--------------------  Trial  115   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.9172412326618793, 'log_learning_rate_D': -2.832579600767133, 'log_learning_rate_D_dagger': -3.532469670224654, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.9206, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1424.0
	 epoch  10 training error:  tensor(0.2973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1676.0
	 epoch  20 training error:  tensor(0.2956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1666.0
	 epoch  30 training error:  tensor(0.2863, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1674.0
	 epoch  40 training error:  tensor(0.1415, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1680.0
	 epoch  50 training error:  tensor(0.0760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1678.0
	 epoch  60 training error:  tensor(0.0701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1686.0
	 epoch  70 training error:  tensor(0.0511, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1694.0
	 epoch  80 training error:  tensor(0.0470, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1696.0
	 epoch  90 training error:  tensor(0.0489, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.6474609375
Memory cached:  1684.0
Time for this trial:  3138.931152820587
Memory status after this trial: 
Memory allocated:  2222.63232421875
Memory cached:  2248.0
--------------------  Trial  116   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 5, 'W_layer_units_exponent_7': 4, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -2.3129259979135, 'log_learning_rate_D': -3.0424517276602354, 'log_learning_rate_D_dagger': -3.8340362670211197, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.1336, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1524.0
	 epoch  10 training error:  tensor(0.3028, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1710.0
	 epoch  20 training error:  tensor(0.2889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1708.0
	 epoch  30 training error:  tensor(0.2134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1722.0
	 epoch  40 training error:  tensor(0.0959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1726.0
	 epoch  50 training error:  tensor(0.0771, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1710.0
	 epoch  60 training error:  tensor(0.0576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1738.0
	 epoch  70 training error:  tensor(0.0414, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1736.0
	 epoch  80 training error:  tensor(0.0612, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1730.0
	 epoch  90 training error:  tensor(0.0497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.6982421875
Memory cached:  1720.0
Time for this trial:  3723.735500574112
Memory status after this trial: 
Memory allocated:  2548.72216796875
Memory cached:  2574.0
--------------------  Trial  117   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 5, 'W_layer_units_exponent_7': 4, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 9, 'log_learning_rate': -4.639607970450953, 'log_learning_rate_D': -2.683644979239392, 'log_learning_rate_D_dagger': -3.943611183845731, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.9417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1484.0
	 epoch  10 training error:  tensor(0.3015, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1646.0
	 epoch  20 training error:  tensor(0.2932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1648.0
	 epoch  30 training error:  tensor(0.2910, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1664.0
	 epoch  40 training error:  tensor(0.1261, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1650.0
	 epoch  50 training error:  tensor(0.0845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1672.0
	 epoch  60 training error:  tensor(0.0560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1658.0
	 epoch  70 training error:  tensor(0.0677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1642.0
	 epoch  80 training error:  tensor(0.0731, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1670.0
	 epoch  90 training error:  tensor(0.0488, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.1123046875
Memory cached:  1656.0
Time for this trial:  3736.5948853492737
Memory status after this trial: 
Memory allocated:  2587.7626953125
Memory cached:  2612.0
--------------------  Trial  118   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 5, 'W_layer_units_exponent_7': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 8, 'log_learning_rate': -2.5156992362781305, 'log_learning_rate_D': -2.2874250416642985, 'log_learning_rate_D_dagger': -3.802813734624947, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.8266, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1516.0
	 epoch  10 training error:  tensor(0.3095, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1750.0
	 epoch  20 training error:  tensor(0.2954, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1756.0
	 epoch  30 training error:  tensor(0.3274, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1722.0
	 epoch  40 training error:  tensor(0.3053, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1724.0
	 epoch  50 training error:  tensor(0.3039, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1738.0
	 epoch  60 training error:  tensor(0.2983, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1766.0
	 epoch  70 training error:  tensor(0.3002, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1724.0
	 epoch  80 training error:  tensor(0.2944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1720.0
	 epoch  90 training error:  tensor(0.2879, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1316.17919921875
Memory cached:  1734.0
Time for this trial:  3889.674342393875
Memory status after this trial: 
Memory allocated:  2891.44384765625
Memory cached:  2912.0
--------------------  Trial  119   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 7, 'W_layer_units_exponent_7': 7, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -3.016903651923451, 'log_learning_rate_D': -2.9891722948759667, 'log_learning_rate_D_dagger': -4.2988353562002795, 'training_batch_size': 7, 'training_p': 4}
	 epoch  0 training error:  tensor(0.9421, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1428.0
	 epoch  10 training error:  tensor(0.4715, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1440.0
	 epoch  20 training error:  tensor(0.3434, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1432.0
	 epoch  30 training error:  tensor(0.2889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1442.0
	 epoch  40 training error:  tensor(0.2692, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1440.0
	 epoch  50 training error:  tensor(0.2650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1436.0
	 epoch  60 training error:  tensor(0.2606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1440.0
	 epoch  70 training error:  tensor(0.2579, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1446.0
	 epoch  80 training error:  tensor(0.2737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1442.0
	 epoch  90 training error:  tensor(0.1878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.88427734375
Memory cached:  1442.0
Time for this trial:  1843.307859659195
Memory status after this trial: 
Memory allocated:  2512.265625
Memory cached:  2542.0
--------------------  Trial  120   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 4, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -2.627584438205172, 'log_learning_rate_D': -3.100542642212873, 'log_learning_rate_D_dagger': -4.142523704816873, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(6.8321, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1430.0
	 epoch  10 training error:  tensor(0.3598, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1722.0
	 epoch  20 training error:  tensor(0.2995, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1740.0
	 epoch  30 training error:  tensor(0.2639, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1746.0
	 epoch  40 training error:  tensor(0.1125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1752.0
	 epoch  50 training error:  tensor(0.0779, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1756.0
	 epoch  60 training error:  tensor(0.0606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1758.0
	 epoch  70 training error:  tensor(0.0613, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1742.0
	 epoch  80 training error:  tensor(0.0463, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1758.0
	 epoch  90 training error:  tensor(0.0564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1317.95263671875
Memory cached:  1756.0
Time for this trial:  3027.2013413906097
Memory status after this trial: 
Memory allocated:  2685.6044921875
Memory cached:  2704.0
--------------------  Trial  121   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.2515977078221625, 'log_learning_rate_D': -3.6038464449443537, 'log_learning_rate_D_dagger': -3.667841171458744, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(2.8759, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1440.0
	 epoch  10 training error:  tensor(0.3204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1654.0
	 epoch  20 training error:  tensor(0.1666, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1694.0
	 epoch  30 training error:  tensor(0.1175, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1694.0
	 epoch  40 training error:  tensor(0.1179, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1742.0
	 epoch  50 training error:  tensor(0.0895, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1736.0
	 epoch  60 training error:  tensor(0.0637, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1728.0
	 epoch  70 training error:  tensor(0.0449, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1712.0
	 epoch  80 training error:  tensor(0.0449, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1706.0
	 epoch  90 training error:  tensor(0.0501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.501953125
Memory cached:  1706.0
Time for this trial:  3028.4182691574097
Memory status after this trial: 
Memory allocated:  2395.64013671875
Memory cached:  2420.0
--------------------  Trial  122   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 6, 'W_layer_units_exponent_5': 7, 'W_layer_units_exponent_6': 5, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -3.1545682102303036, 'log_learning_rate_D': -3.6373354514239202, 'log_learning_rate_D_dagger': -3.6614996322609996, 'training_batch_size': 6, 'training_p': 4}
	 epoch  0 training error:  tensor(0.7426, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1508.0
	 epoch  10 training error:  tensor(0.2646, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1772.0
	 epoch  20 training error:  tensor(0.2552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1740.0
	 epoch  30 training error:  tensor(0.1292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1758.0
	 epoch  40 training error:  tensor(0.0868, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1722.0
	 epoch  50 training error:  tensor(0.0714, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1744.0
	 epoch  60 training error:  tensor(0.0747, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1748.0
	 epoch  70 training error:  tensor(0.0654, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1756.0
	 epoch  80 training error:  tensor(0.0487, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1746.0
	 epoch  90 training error:  tensor(0.0488, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1289.76513671875
Memory cached:  1748.0
Time for this trial:  3295.326459646225
Memory status after this trial: 
Memory allocated:  2368.953125
Memory cached:  2392.0
--------------------  Trial  123   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 8, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.2550202995803685, 'log_learning_rate_D': -3.564145544928213, 'log_learning_rate_D_dagger': -3.8806273067903394, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(2.2594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1348.0
	 epoch  10 training error:  tensor(0.3194, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1486.0
	 epoch  20 training error:  tensor(0.3149, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1472.0
	 epoch  30 training error:  tensor(0.1926, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1508.0
	 epoch  40 training error:  tensor(0.1437, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1502.0
	 epoch  50 training error:  tensor(0.1055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1506.0
	 epoch  60 training error:  tensor(0.0699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1468.0
	 epoch  70 training error:  tensor(0.0733, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1492.0
	 epoch  80 training error:  tensor(0.0555, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1496.0
	 epoch  90 training error:  tensor(0.0531, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.93017578125
Memory cached:  1496.0
Time for this trial:  3158.159196615219
Memory status after this trial: 
Memory allocated:  2432.865234375
Memory cached:  2462.0
--------------------  Trial  124   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 8, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.4072457110301926, 'log_learning_rate_D': -3.5175880641184807, 'log_learning_rate_D_dagger': -4.165718645923616, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(2.0604, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1406.0
	 epoch  10 training error:  tensor(0.3444, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1636.0
	 epoch  20 training error:  tensor(0.3257, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1616.0
	 epoch  30 training error:  tensor(0.3155, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1624.0
	 epoch  40 training error:  tensor(0.3045, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1606.0
	 epoch  50 training error:  tensor(0.1712, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1640.0
	 epoch  60 training error:  tensor(0.1456, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1622.0
	 epoch  70 training error:  tensor(0.1301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1614.0
	 epoch  80 training error:  tensor(0.0960, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1612.0
	 epoch  90 training error:  tensor(0.0691, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.5087890625
Memory cached:  1608.0
Time for this trial:  3155.166122198105
Memory status after this trial: 
Memory allocated:  2400.62646484375
Memory cached:  2428.0
--------------------  Trial  125   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 7, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 8, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.2498518871656517, 'log_learning_rate_D': -3.787249655456466, 'log_learning_rate_D_dagger': -3.844929347059652, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(7.9955, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1394.0
	 epoch  10 training error:  tensor(0.3536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1660.0
	 epoch  20 training error:  tensor(0.3290, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1662.0
	 epoch  30 training error:  tensor(0.3199, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1682.0
	 epoch  40 training error:  tensor(0.2812, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1694.0
	 epoch  50 training error:  tensor(0.1657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1694.0
	 epoch  60 training error:  tensor(0.1077, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1686.0
	 epoch  70 training error:  tensor(0.1314, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1670.0
	 epoch  80 training error:  tensor(0.0694, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1702.0
	 epoch  90 training error:  tensor(0.0536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1313.00927734375
Memory cached:  1678.0
Time for this trial:  3019.8185064792633
Memory status after this trial: 
Memory allocated:  2548.126953125
Memory cached:  2568.0
--------------------  Trial  126   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.529025541101459, 'log_learning_rate_D': -3.4403034994693487, 'log_learning_rate_D_dagger': -3.6197557900840414, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(3.1056, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1406.0
	 epoch  10 training error:  tensor(0.3339, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1668.0
	 epoch  20 training error:  tensor(0.3125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1678.0
	 epoch  30 training error:  tensor(0.2026, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1666.0
	 epoch  40 training error:  tensor(0.1326, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1674.0
	 epoch  50 training error:  tensor(0.1096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1696.0
	 epoch  60 training error:  tensor(0.0787, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1692.0
	 epoch  70 training error:  tensor(0.0501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1680.0
	 epoch  80 training error:  tensor(0.0652, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1702.0
	 epoch  90 training error:  tensor(0.0417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1309.25146484375
Memory cached:  1684.0
Time for this trial:  3031.63573551178
Memory status after this trial: 
Memory allocated:  2467.41455078125
Memory cached:  2486.0
--------------------  Trial  127   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 9, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.731700191177526, 'log_learning_rate_D': -3.5562899719788503, 'log_learning_rate_D_dagger': -4.017571020896776, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.7628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1424.0
	 epoch  10 training error:  tensor(0.3241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1708.0
	 epoch  20 training error:  tensor(0.3062, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1744.0
	 epoch  30 training error:  tensor(0.2813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1732.0
	 epoch  40 training error:  tensor(0.2124, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1730.0
	 epoch  50 training error:  tensor(0.1701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1740.0
	 epoch  60 training error:  tensor(0.1216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1744.0
	 epoch  70 training error:  tensor(0.0985, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1728.0
	 epoch  80 training error:  tensor(0.0765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1732.0
	 epoch  90 training error:  tensor(0.0724, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1287.294921875
Memory cached:  1728.0
Time for this trial:  3162.1967833042145
Memory status after this trial: 
Memory allocated:  2458.732421875
Memory cached:  2480.0
--------------------  Trial  128   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.130083950661218, 'log_learning_rate_D': -2.8427346655769785, 'log_learning_rate_D_dagger': -3.7735816614791124, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(4.5255, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1410.0
	 epoch  10 training error:  tensor(0.3373, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1468.0
	 epoch  20 training error:  tensor(0.3171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1466.0
	 epoch  30 training error:  tensor(0.3103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1472.0
	 epoch  40 training error:  tensor(0.1676, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1434.0
	 epoch  50 training error:  tensor(0.1277, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1468.0
	 epoch  60 training error:  tensor(0.0713, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1450.0
	 epoch  70 training error:  tensor(0.0710, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1460.0
	 epoch  80 training error:  tensor(0.0807, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1472.0
	 epoch  90 training error:  tensor(0.0686, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1296.31640625
Memory cached:  1454.0
Time for this trial:  3181.4752712249756
Memory status after this trial: 
Memory allocated:  2304.74560546875
Memory cached:  2324.0
--------------------  Trial  129   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 8, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.344370280777887, 'log_learning_rate_D': -3.0137443682841574, 'log_learning_rate_D_dagger': -3.4656438501875235, 'training_batch_size': 7, 'training_p': 5}
	 epoch  0 training error:  tensor(8.7438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1314.0
	 epoch  10 training error:  tensor(0.3162, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1326.0
	 epoch  20 training error:  tensor(0.2987, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1318.0
	 epoch  30 training error:  tensor(0.2889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1322.0
	 epoch  40 training error:  tensor(0.2924, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1328.0
	 epoch  50 training error:  tensor(0.3445, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1322.0
	 epoch  60 training error:  tensor(0.1679, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1320.0
	 epoch  70 training error:  tensor(0.1076, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1320.0
	 epoch  80 training error:  tensor(0.1030, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1320.0
	 epoch  90 training error:  tensor(0.1324, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1274.20361328125
Memory cached:  1322.0
Time for this trial:  1493.6524851322174
Memory status after this trial: 
Memory allocated:  2244.66943359375
Memory cached:  2274.0
--------------------  Trial  130   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.5286259872954777, 'log_learning_rate_D': -3.379364652791628, 'log_learning_rate_D_dagger': -3.9067101096422063, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(6.0593, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1404.0
	 epoch  10 training error:  tensor(0.3479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1466.0
	 epoch  20 training error:  tensor(0.3286, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1476.0
	 epoch  30 training error:  tensor(0.3166, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1476.0
	 epoch  40 training error:  tensor(0.2054, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1486.0
	 epoch  50 training error:  tensor(0.0806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1478.0
	 epoch  60 training error:  tensor(0.0737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1464.0
	 epoch  70 training error:  tensor(0.0733, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1468.0
	 epoch  80 training error:  tensor(0.0508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1470.0
	 epoch  90 training error:  tensor(0.0488, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1314.47216796875
Memory cached:  1482.0
Time for this trial:  3041.663012742996
Memory status after this trial: 
Memory allocated:  2592.42236328125
Memory cached:  2616.0
--------------------  Trial  131   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 5, 'D_layers': 3, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 9, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.2504834991129616, 'log_learning_rate_D': -3.6111607705434277, 'log_learning_rate_D_dagger': -3.6841111796069432, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.1431, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1326.0
	 epoch  10 training error:  tensor(0.3078, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1350.0
	 epoch  20 training error:  tensor(0.2872, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1370.0
	 epoch  30 training error:  tensor(0.1423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1384.0
	 epoch  40 training error:  tensor(0.0905, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1390.0
	 epoch  50 training error:  tensor(0.0851, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1364.0
	 epoch  60 training error:  tensor(0.0653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1372.0
	 epoch  70 training error:  tensor(0.0653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1376.0
	 epoch  80 training error:  tensor(0.0641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1364.0
	 epoch  90 training error:  tensor(0.0495, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1277.46875
Memory cached:  1372.0
Time for this trial:  3048.50133562088
Memory status after this trial: 
Memory allocated:  2194.04248046875
Memory cached:  2226.0
--------------------  Trial  132   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 8, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'log_learning_rate': -2.4592914946268647, 'log_learning_rate_D': -3.1099566663306866, 'log_learning_rate_D_dagger': -3.555598822227861, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(7.9671, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1432.0
	 epoch  10 training error:  tensor(0.3254, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1448.0
	 epoch  20 training error:  tensor(0.1830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1462.0
	 epoch  30 training error:  tensor(0.1040, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1470.0
	 epoch  40 training error:  tensor(0.0880, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1482.0
	 epoch  50 training error:  tensor(0.0613, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1472.0
	 epoch  60 training error:  tensor(0.0584, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1480.0
	 epoch  70 training error:  tensor(0.0546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1468.0
	 epoch  80 training error:  tensor(0.0625, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1466.0
	 epoch  90 training error:  tensor(0.0570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1322.90673828125
Memory cached:  1476.0
Time for this trial:  3063.456822156906
Memory status after this trial: 
Memory allocated:  2801.984375
Memory cached:  2830.0
--------------------  Trial  133   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 4, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 6, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 7, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -1.9067103650130797, 'log_learning_rate_D': -3.811196592450431, 'log_learning_rate_D_dagger': -3.270945752335715, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(1.2111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1406.0
	 epoch  10 training error:  tensor(0.3410, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1620.0
	 epoch  20 training error:  tensor(0.2408, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1656.0
	 epoch  30 training error:  tensor(0.1608, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1646.0
	 epoch  40 training error:  tensor(0.1462, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1656.0
	 epoch  50 training error:  tensor(0.0794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1646.0
	 epoch  60 training error:  tensor(0.0741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1650.0
	 epoch  70 training error:  tensor(0.0695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1644.0
	 epoch  80 training error:  tensor(0.0616, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1650.0
	 epoch  90 training error:  tensor(0.0453, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.02392578125
Memory cached:  1662.0
Time for this trial:  3317.6549446582794
Memory status after this trial: 
Memory allocated:  2522.15869140625
Memory cached:  2546.0
--------------------  Trial  134   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 5, 'W_layer_units_exponent_4': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 5, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.091933072111919, 'log_learning_rate_D': -3.7304341126519436, 'log_learning_rate_D_dagger': -3.403717489700536, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(1.9242, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1354.0
	 epoch  10 training error:  tensor(0.3251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1580.0
	 epoch  20 training error:  tensor(0.3041, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1592.0
	 epoch  30 training error:  tensor(0.1574, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1608.0
	 epoch  40 training error:  tensor(0.1415, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1596.0
	 epoch  50 training error:  tensor(0.0816, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1600.0
	 epoch  60 training error:  tensor(0.0592, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1552.0
	 epoch  70 training error:  tensor(0.0675, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1604.0
	 epoch  80 training error:  tensor(0.0637, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1586.0
	 epoch  90 training error:  tensor(0.0463, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1271.51025390625
Memory cached:  1618.0
Time for this trial:  3441.65185213089
Memory status after this trial: 
Memory allocated:  2182.89990234375
Memory cached:  2210.0
--------------------  Trial  135   --------------------
Start timing: 
Parameters: 
{'W_layers': 8, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 8, 'W_layer_units_exponent_5': 9, 'W_layer_units_exponent_6': 6, 'W_layer_units_exponent_7': 4, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 8, 'D_dagger_layer_units_exponent_7': 10, 'log_learning_rate': -2.3524152248219066, 'log_learning_rate_D': -3.9847481380502825, 'log_learning_rate_D_dagger': -3.4980304246522422, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.7709, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1568.0
	 epoch  10 training error:  tensor(0.3374, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1724.0
	 epoch  20 training error:  tensor(0.1866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1736.0
	 epoch  30 training error:  tensor(0.1601, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1740.0
	 epoch  40 training error:  tensor(0.1594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1716.0
	 epoch  50 training error:  tensor(0.1018, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1734.0
	 epoch  60 training error:  tensor(0.0904, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1732.0
	 epoch  70 training error:  tensor(0.0619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1728.0
	 epoch  80 training error:  tensor(0.0589, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1728.0
	 epoch  90 training error:  tensor(0.0642, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1294.3994140625
Memory cached:  1714.0
Time for this trial:  3918.62181019783
Memory status after this trial: 
Memory allocated:  2821.75
Memory cached:  2854.0
--------------------  Trial  136   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 6, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 8, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 8, 'log_learning_rate': -2.154210170160068, 'log_learning_rate_D': -3.8551045538916715, 'log_learning_rate_D_dagger': -3.7533596970234897, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.3197, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1424.0
	 epoch  10 training error:  tensor(0.2995, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1570.0
	 epoch  20 training error:  tensor(0.2659, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1562.0
	 epoch  30 training error:  tensor(0.1417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1550.0
	 epoch  40 training error:  tensor(0.1231, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1542.0
	 epoch  50 training error:  tensor(0.0795, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1538.0
	 epoch  60 training error:  tensor(0.0663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1532.0
	 epoch  70 training error:  tensor(0.0704, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1542.0
	 epoch  80 training error:  tensor(0.0477, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1534.0
	 epoch  90 training error:  tensor(0.0603, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.03466796875
Memory cached:  1538.0
Time for this trial:  3300.662326812744
Memory status after this trial: 
Memory allocated:  2270.6591796875
Memory cached:  2296.0
--------------------  Trial  137   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 8, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 4, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.0051826966408717, 'log_learning_rate_D': -2.538517598862489, 'log_learning_rate_D_dagger': -3.620708335171669, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.9702, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1346.0
	 epoch  10 training error:  tensor(0.3181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1448.0
	 epoch  20 training error:  tensor(0.3241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1448.0
	 epoch  30 training error:  tensor(0.3030, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1442.0
	 epoch  40 training error:  tensor(0.3161, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1428.0
	 epoch  50 training error:  tensor(0.3186, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1414.0
	 epoch  60 training error:  tensor(0.3143, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1424.0
	 epoch  70 training error:  tensor(0.3797, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1430.0
	 epoch  80 training error:  tensor(0.3243, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1420.0
	 epoch  90 training error:  tensor(0.3038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1270.62158203125
Memory cached:  1412.0
Time for this trial:  3153.2768325805664
Memory status after this trial: 
Memory allocated:  2157.56103515625
Memory cached:  2184.0
--------------------  Trial  138   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 6, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -2.2874668681437913, 'log_learning_rate_D': -3.913080281501078, 'log_learning_rate_D_dagger': -3.301831544534257, 'training_batch_size': 12, 'training_p': 5}
	 epoch  0 training error:  tensor(0.9923, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1293.57861328125
Memory cached:  1506.0
	 epoch  10 training error:  tensor(1.2693, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1574.0
	 epoch  20 training error:  tensor(0.4700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1572.0
	 epoch  30 training error:  tensor(0.4207, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1576.0
	 epoch  40 training error:  tensor(0.3646, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1574.0
	 epoch  50 training error:  tensor(0.3462, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1578.0
	 epoch  60 training error:  tensor(0.3278, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1576.0
	 epoch  70 training error:  tensor(0.3054, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1576.0
	 epoch  80 training error:  tensor(0.2973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1578.0
	 epoch  90 training error:  tensor(0.2937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.57861328125
Memory cached:  1580.0
Time for this trial:  479.2619082927704
Memory status after this trial: 
Memory allocated:  2292.52001953125
Memory cached:  2316.0
--------------------  Trial  139   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'W_layer_units_exponent_4': 9, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 5, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 9, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.7947420351031735, 'log_learning_rate_D': -3.1830139559671915, 'log_learning_rate_D_dagger': -3.0395047571255347, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.5509, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1372.0
	 epoch  10 training error:  tensor(0.1972, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1412.0
	 epoch  20 training error:  tensor(0.0781, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1424.0
	 epoch  30 training error:  tensor(0.0737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1410.0
	 epoch  40 training error:  tensor(0.0769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1428.0
	 epoch  50 training error:  tensor(0.0501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1422.0
	 epoch  60 training error:  tensor(0.0540, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1428.0
	 epoch  70 training error:  tensor(0.0508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1426.0
	 epoch  80 training error:  tensor(0.0411, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1428.0
	 epoch  90 training error:  tensor(0.0438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1282.00927734375
Memory cached:  1420.0
Time for this trial:  3317.0628294944763
Memory status after this trial: 
Memory allocated:  2423.787109375
Memory cached:  2452.0
--------------------  Trial  140   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 7, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 8, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.429949481283588, 'log_learning_rate_D': -4.167168297341173, 'log_learning_rate_D_dagger': -4.0600039959561, 'training_batch_size': 8, 'training_p': 6}
	 epoch  0 training error:  tensor(3.4300, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1414.0
	 epoch  10 training error:  tensor(0.5458, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1426.0
	 epoch  20 training error:  tensor(0.3616, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1426.0
	 epoch  30 training error:  tensor(0.3481, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1422.0
	 epoch  40 training error:  tensor(0.3350, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1428.0
	 epoch  50 training error:  tensor(0.3297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1424.0
	 epoch  60 training error:  tensor(0.3280, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1420.0
	 epoch  70 training error:  tensor(0.3271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1426.0
	 epoch  80 training error:  tensor(0.3233, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1426.0
	 epoch  90 training error:  tensor(0.3027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.3935546875
Memory cached:  1420.0
Time for this trial:  1107.3869364261627
Memory status after this trial: 
Memory allocated:  2566.484375
Memory cached:  2596.0
--------------------  Trial  141   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 5, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 7, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.5976204619672973, 'log_learning_rate_D': -3.588416949005048, 'log_learning_rate_D_dagger': -3.2261700732269167, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.8484, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1518.0
	 epoch  10 training error:  tensor(0.2044, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1802.0
	 epoch  20 training error:  tensor(0.1097, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1818.0
	 epoch  30 training error:  tensor(0.0736, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1816.0
	 epoch  40 training error:  tensor(0.0527, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1820.0
	 epoch  50 training error:  tensor(0.0443, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1816.0
	 epoch  60 training error:  tensor(0.0560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1818.0
	 epoch  70 training error:  tensor(0.0475, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1806.0
	 epoch  80 training error:  tensor(0.0628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1798.0
	 epoch  90 training error:  tensor(0.0466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1302.857421875
Memory cached:  1814.0
Time for this trial:  3181.986094713211
Memory status after this trial: 
Memory allocated:  2590.95263671875
Memory cached:  2608.0
--------------------  Trial  142   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -2.1813391532744344, 'log_learning_rate_D': -2.720691650423028, 'log_learning_rate_D_dagger': -3.866962357426705, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(5.7650, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1418.0
	 epoch  10 training error:  tensor(0.3601, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1580.0
	 epoch  20 training error:  tensor(0.3213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1576.0
	 epoch  30 training error:  tensor(0.2282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1576.0
	 epoch  40 training error:  tensor(0.1213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1570.0
	 epoch  50 training error:  tensor(0.0702, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1564.0
	 epoch  60 training error:  tensor(0.0609, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1580.0
	 epoch  70 training error:  tensor(0.0569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1572.0
	 epoch  80 training error:  tensor(0.0565, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1572.0
	 epoch  90 training error:  tensor(0.0606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1297.619140625
Memory cached:  1562.0
Time for this trial:  2760.9234693050385
Memory status after this trial: 
Memory allocated:  2331.0068359375
Memory cached:  2352.0
--------------------  Trial  143   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -2.1728022680773136, 'log_learning_rate_D': -2.763422849448623, 'log_learning_rate_D_dagger': -3.8457300291300913, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(1.5216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1394.0
	 epoch  10 training error:  tensor(0.6508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1562.0
	 epoch  20 training error:  tensor(0.6183, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1562.0
	 epoch  30 training error:  tensor(0.3303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1598.0
	 epoch  40 training error:  tensor(0.3199, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1598.0
	 epoch  50 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1588.0
	 epoch  60 training error:  tensor(0.4328, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1600.0
	 epoch  70 training error:  tensor(0.2901, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1578.0
	 epoch  80 training error:  tensor(0.1082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1584.0
	 epoch  90 training error:  tensor(0.0871, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.150390625
Memory cached:  1564.0
Time for this trial:  2608.791613340378
Memory status after this trial: 
Memory allocated:  2122.0498046875
Memory cached:  2146.0
--------------------  Trial  144   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 7, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'log_learning_rate': -2.260447607355356, 'log_learning_rate_D': -1.0858044939540124, 'log_learning_rate_D_dagger': -3.9020029934781046, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(5.1717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1410.0
	 epoch  10 training error:  tensor(0.6737, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1484.0
	 epoch  20 training error:  tensor(0.6721, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1508.0
	 epoch  30 training error:  tensor(0.6633, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1518.0
	 epoch  40 training error:  tensor(0.6561, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1532.0
	 epoch  50 training error:  tensor(0.6617, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1524.0
	 epoch  60 training error:  tensor(0.6619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1524.0
	 epoch  70 training error:  tensor(0.6519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1516.0
	 epoch  80 training error:  tensor(0.6559, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1518.0
	 epoch  90 training error:  tensor(0.6871, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1300.24609375
Memory cached:  1494.0
Time for this trial:  2765.6652534008026
Memory status after this trial: 
Memory allocated:  2355.3701171875
Memory cached:  2380.0
--------------------  Trial  145   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -2.3683874968554486, 'log_learning_rate_D': -2.984522867652515, 'log_learning_rate_D_dagger': -3.6935042596451915, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(4.9813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1388.0
	 epoch  10 training error:  tensor(0.3382, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1628.0
	 epoch  20 training error:  tensor(0.3369, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1646.0
	 epoch  30 training error:  tensor(0.1250, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1656.0
	 epoch  40 training error:  tensor(0.1028, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1644.0
	 epoch  50 training error:  tensor(0.0814, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1650.0
	 epoch  60 training error:  tensor(0.0695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1648.0
	 epoch  70 training error:  tensor(0.0548, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1660.0
	 epoch  80 training error:  tensor(0.0527, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1652.0
	 epoch  90 training error:  tensor(0.0603, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1298.05859375
Memory cached:  1654.0
Time for this trial:  2902.588461637497
Memory status after this trial: 
Memory allocated:  2321.14697265625
Memory cached:  2342.0
--------------------  Trial  146   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'D_layers': 2, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 4, 'log_learning_rate': -2.0483660125575813, 'log_learning_rate_D': -2.7091140285843904, 'log_learning_rate_D_dagger': -3.599366440818906, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(9.6044, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1364.0
	 epoch  10 training error:  tensor(0.3121, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1544.0
	 epoch  20 training error:  tensor(0.2929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1562.0
	 epoch  30 training error:  tensor(0.2869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1566.0
	 epoch  40 training error:  tensor(0.1250, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1582.0
	 epoch  50 training error:  tensor(0.0775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1578.0
	 epoch  60 training error:  tensor(0.0631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1586.0
	 epoch  70 training error:  tensor(0.0496, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1584.0
	 epoch  80 training error:  tensor(0.0616, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1586.0
	 epoch  90 training error:  tensor(0.0516, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1288.01904296875
Memory cached:  1576.0
Time for this trial:  2484.7516276836395
Memory status after this trial: 
Memory allocated:  2087.3173828125
Memory cached:  2108.0
--------------------  Trial  147   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.681537171373858, 'log_learning_rate_D': -2.585089948868297, 'log_learning_rate_D_dagger': -3.4086640966406225, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(0.7493, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1392.0
	 epoch  10 training error:  tensor(0.5379, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1620.0
	 epoch  20 training error:  tensor(0.3563, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1612.0
	 epoch  30 training error:  tensor(0.3492, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1614.0
	 epoch  40 training error:  tensor(0.3310, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1602.0
	 epoch  50 training error:  tensor(0.3247, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1610.0
	 epoch  60 training error:  tensor(0.3352, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1624.0
	 epoch  70 training error:  tensor(0.3241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1630.0
	 epoch  80 training error:  tensor(0.3049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1614.0
	 epoch  90 training error:  tensor(0.3066, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1281.39599609375
Memory cached:  1612.0
Time for this trial:  2760.8320169448853
Memory status after this trial: 
Memory allocated:  2245.61474609375
Memory cached:  2270.0
--------------------  Trial  148   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -2.225851005480596, 'log_learning_rate_D': -2.3920464598261724, 'log_learning_rate_D_dagger': -3.521706124097313, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.3244, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1500.0
	 epoch  10 training error:  tensor(0.3034, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1742.0
	 epoch  20 training error:  tensor(0.2922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1736.0
	 epoch  30 training error:  tensor(0.3096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1742.0
	 epoch  40 training error:  tensor(0.3193, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1742.0
	 epoch  50 training error:  tensor(0.3173, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1740.0
	 epoch  60 training error:  tensor(0.3168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1758.0
	 epoch  70 training error:  tensor(0.3177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1744.0
	 epoch  80 training error:  tensor(0.3289, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1744.0
	 epoch  90 training error:  tensor(0.3177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1328.353515625
Memory cached:  1726.0
Time for this trial:  3214.267603158951
Memory status after this trial: 
Memory allocated:  2721.88623046875
Memory cached:  2736.0
--------------------  Trial  149   --------------------
Start timing: 
Parameters: 
{'W_layers': 7, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'W_layer_units_exponent_3': 6, 'W_layer_units_exponent_4': 5, 'W_layer_units_exponent_5': 10, 'W_layer_units_exponent_6': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 8, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 7, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 6, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 7, 'log_learning_rate': -2.5543029410620286, 'log_learning_rate_D': -2.829453046697183, 'log_learning_rate_D_dagger': -3.34103814253291, 'training_batch_size': 11, 'training_p': 5}
	 epoch  0 training error:  tensor(0.9866, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1512.0
	 epoch  10 training error:  tensor(0.6215, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1754.0
	 epoch  20 training error:  tensor(0.4699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1752.0
	 epoch  30 training error:  tensor(0.3916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1740.0
	 epoch  40 training error:  tensor(0.3549, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1766.0
	 epoch  50 training error:  tensor(0.3234, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1766.0
	 epoch  60 training error:  tensor(0.3081, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1742.0
	 epoch  70 training error:  tensor(0.2999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1754.0
	 epoch  80 training error:  tensor(0.2918, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1734.0
	 epoch  90 training error:  tensor(0.2975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1331.16064453125
Memory cached:  1762.0
Time for this trial:  487.5615999698639
Memory status after this trial: 
Memory allocated:  2950.3251953125
Memory cached:  2966.0
--------------------  Trial  150   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 7, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 8, 'log_learning_rate': -2.4596907878961014, 'log_learning_rate_D': -2.8855856885074442, 'log_learning_rate_D_dagger': -3.0983877560082376, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(1.0212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1454.0
	 epoch  10 training error:  tensor(0.5677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1604.0
	 epoch  20 training error:  tensor(0.3618, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1582.0
	 epoch  30 training error:  tensor(0.3787, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1592.0
	 epoch  40 training error:  tensor(0.3504, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1568.0
	 epoch  50 training error:  tensor(0.3605, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1556.0
	 epoch  60 training error:  tensor(0.3520, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1592.0
	 epoch  70 training error:  tensor(0.3475, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1572.0
	 epoch  80 training error:  tensor(0.3635, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1564.0
	 epoch  90 training error:  tensor(0.3483, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.083984375
Memory cached:  1580.0
Time for this trial:  3191.8966324329376
Memory status after this trial: 
Memory allocated:  2419.67626953125
Memory cached:  2440.0
--------------------  Trial  151   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 5, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 7, 'W_layer_units_exponent_3': 8, 'W_layer_units_exponent_4': 9, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 8, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 7, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 5, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 4, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.9015923003966444, 'log_learning_rate_D': -3.675769079887151, 'log_learning_rate_D_dagger': -4.203891391960958, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.7676, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1374.0
	 epoch  10 training error:  tensor(0.3700, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1602.0
	 epoch  20 training error:  tensor(0.3168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1592.0
	 epoch  30 training error:  tensor(0.2954, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1614.0
	 epoch  40 training error:  tensor(0.2911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1596.0
	 epoch  50 training error:  tensor(0.2881, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1596.0
	 epoch  60 training error:  tensor(0.2855, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1608.0
	 epoch  70 training error:  tensor(0.2794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1588.0
	 epoch  80 training error:  tensor(0.2831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1608.0
	 epoch  90 training error:  tensor(0.1413, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1275.41015625
Memory cached:  1602.0
Time for this trial:  3177.1397767066956
Memory status after this trial: 
Memory allocated:  2317.849609375
Memory cached:  2348.0
--------------------  Trial  152   --------------------
Start timing: 
Parameters: 
{'W_layers': 3, 'W_layer_units_exponent_0': 6, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 10, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 9, 'D_dagger_layers': 5, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 6, 'log_learning_rate': -1.8440185587167255, 'log_learning_rate_D': -3.7557513873652124, 'log_learning_rate_D_dagger': -3.9639825493489442, 'training_batch_size': 7, 'training_p': 6}
	 epoch  0 training error:  tensor(23.3859, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1428.0
	 epoch  10 training error:  tensor(0.3849, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1444.0
	 epoch  20 training error:  tensor(0.3477, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1458.0
	 epoch  30 training error:  tensor(0.3400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1454.0
	 epoch  40 training error:  tensor(0.3303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1450.0
	 epoch  50 training error:  tensor(0.3268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1454.0
	 epoch  60 training error:  tensor(0.3220, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1452.0
	 epoch  70 training error:  tensor(0.2463, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1450.0
	 epoch  80 training error:  tensor(0.1578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1452.0
	 epoch  90 training error:  tensor(0.1129, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1304.95556640625
Memory cached:  1454.0
Time for this trial:  1459.7111587524414
Memory status after this trial: 
Memory allocated:  2587.19677734375
Memory cached:  2608.0
--------------------  Trial  153   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 4, 'D_dagger_layer_units_exponent_1': 5, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.3720663085338, 'log_learning_rate_D': -3.2446592634976206, 'log_learning_rate_D_dagger': -3.191399683598886, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(2.4846, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1424.0
	 epoch  10 training error:  tensor(0.3206, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1696.0
	 epoch  20 training error:  tensor(0.3194, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1678.0
	 epoch  30 training error:  tensor(0.3111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1678.0
	 epoch  40 training error:  tensor(0.3094, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1692.0
	 epoch  50 training error:  tensor(0.3070, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1696.0
	 epoch  60 training error:  tensor(0.2968, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1700.0
	 epoch  70 training error:  tensor(0.2340, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1696.0
	 epoch  80 training error:  tensor(0.1219, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1674.0
	 epoch  90 training error:  tensor(0.0781, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1295.78271484375
Memory cached:  1686.0
Time for this trial:  3198.6802971363068
Memory status after this trial: 
Memory allocated:  2231.26025390625
Memory cached:  2250.0
--------------------  Trial  154   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 4, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 6, 'log_learning_rate': -2.2979507967581085, 'log_learning_rate_D': -3.119220395888841, 'log_learning_rate_D_dagger': -2.9509428736974184, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(3.5772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1422.0
	 epoch  10 training error:  tensor(0.2939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1688.0
	 epoch  20 training error:  tensor(0.1020, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1676.0
	 epoch  30 training error:  tensor(0.0667, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1692.0
	 epoch  40 training error:  tensor(0.0631, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1676.0
	 epoch  50 training error:  tensor(0.0581, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1674.0
	 epoch  60 training error:  tensor(0.0598, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1704.0
	 epoch  70 training error:  tensor(0.0451, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1692.0
	 epoch  80 training error:  tensor(0.0393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1676.0
	 epoch  90 training error:  tensor(0.0267, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1291.87841796875
Memory cached:  1682.0
Time for this trial:  3195.229903459549
Memory status after this trial: 
Memory allocated:  2400.2041015625
Memory cached:  2424.0
--------------------  Trial  155   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -1.95485549396478, 'log_learning_rate_D': -3.294967782594689, 'log_learning_rate_D_dagger': -3.165267963752712, 'training_batch_size': 6, 'training_p': 3}
	 epoch  0 training error:  tensor(11.8215, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1434.0
	 epoch  10 training error:  tensor(0.2419, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1754.0
	 epoch  20 training error:  tensor(0.2302, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1750.0
	 epoch  30 training error:  tensor(0.2571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1754.0
	 epoch  40 training error:  tensor(0.1216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1764.0
	 epoch  50 training error:  tensor(0.1023, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1756.0
	 epoch  60 training error:  tensor(0.0547, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1766.0
	 epoch  70 training error:  tensor(0.0403, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1770.0
	 epoch  80 training error:  tensor(0.0390, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1764.0
	 epoch  90 training error:  tensor(0.0423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1301.7919921875
Memory cached:  1746.0
Time for this trial:  3205.403597831726
Memory status after this trial: 
Memory allocated:  2482.751953125
Memory cached:  2504.0
--------------------  Trial  156   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 7, 'D_layer_units_exponent_3': 9, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 4, 'log_learning_rate': -2.1471298162095604, 'log_learning_rate_D': -3.0494551041966242, 'log_learning_rate_D_dagger': -3.4617044922852886, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(4.8151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1432.0
	 epoch  10 training error:  tensor(0.3470, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1626.0
	 epoch  20 training error:  tensor(0.3295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1636.0
	 epoch  30 training error:  tensor(0.3232, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1630.0
	 epoch  40 training error:  tensor(0.1789, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1624.0
	 epoch  50 training error:  tensor(0.1496, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1634.0
	 epoch  60 training error:  tensor(0.1428, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1638.0
	 epoch  70 training error:  tensor(0.1339, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1636.0
	 epoch  80 training error:  tensor(0.0868, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1630.0
	 epoch  90 training error:  tensor(0.0735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1279.10498046875
Memory cached:  1634.0
Time for this trial:  2897.5787403583527
Memory status after this trial: 
Memory allocated:  2067.14306640625
Memory cached:  2092.0
--------------------  Trial  157   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 5, 'W_layer_units_exponent_3': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 8, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 4, 'D_dagger_layer_units_exponent_5': 5, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 4, 'log_learning_rate': -4.208635786905498, 'log_learning_rate_D': -3.439806549963153, 'log_learning_rate_D_dagger': -3.2851340560087445, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.7235, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1476.0
	 epoch  10 training error:  tensor(0.2940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1756.0
	 epoch  20 training error:  tensor(0.1592, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1750.0
	 epoch  30 training error:  tensor(0.1171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1780.0
	 epoch  40 training error:  tensor(0.0710, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1780.0
	 epoch  50 training error:  tensor(0.0495, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1756.0
	 epoch  60 training error:  tensor(0.0401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1780.0
	 epoch  70 training error:  tensor(0.0391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1766.0
	 epoch  80 training error:  tensor(0.0352, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1780.0
	 epoch  90 training error:  tensor(0.0344, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1292.619140625
Memory cached:  1768.0
Time for this trial:  3338.060270547867
Memory status after this trial: 
Memory allocated:  2430.68603515625
Memory cached:  2454.0
--------------------  Trial  158   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 8, 'W_layer_units_exponent_3': 6, 'D_layers': 8, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 8, 'D_layer_units_exponent_4': 9, 'D_layer_units_exponent_5': 5, 'D_layer_units_exponent_6': 8, 'D_layer_units_exponent_7': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 4, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 4, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 7, 'log_learning_rate': -2.311394761884774, 'log_learning_rate_D': -3.214396538042881, 'log_learning_rate_D_dagger': -3.79650989609169, 'training_batch_size': 6, 'training_p': 6}
	 epoch  0 training error:  tensor(3.9568, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1476.0
	 epoch  10 training error:  tensor(0.3410, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1806.0
	 epoch  20 training error:  tensor(0.3231, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1830.0
	 epoch  30 training error:  tensor(0.2970, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1810.0
	 epoch  40 training error:  tensor(0.1013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1814.0
	 epoch  50 training error:  tensor(0.0604, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1828.0
	 epoch  60 training error:  tensor(0.0605, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1832.0
	 epoch  70 training error:  tensor(0.0663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1804.0
	 epoch  80 training error:  tensor(0.0400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1814.0
	 epoch  90 training error:  tensor(0.0458, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1318.37890625
Memory cached:  1790.0
Time for this trial:  3800.26246714592
Memory status after this trial: 
Memory allocated:  2975.62158203125
Memory cached:  3002.0
--------------------  Trial  159   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 9, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 7, 'D_layers': 4, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 10, 'D_layer_units_exponent_2': 10, 'D_layer_units_exponent_3': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 10, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 5, 'D_dagger_layer_units_exponent_3': 6, 'D_dagger_layer_units_exponent_4': 7, 'D_dagger_layer_units_exponent_5': 6, 'D_dagger_layer_units_exponent_6': 5, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.628499466051327, 'log_learning_rate_D': -2.959307653948158, 'log_learning_rate_D_dagger': -3.3760659217640447, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(0.8660, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1462.0
	 epoch  10 training error:  tensor(0.3147, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1648.0
	 epoch  20 training error:  tensor(0.2963, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1644.0
	 epoch  30 training error:  tensor(0.2880, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1640.0
	 epoch  40 training error:  tensor(0.3099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1652.0
	 epoch  50 training error:  tensor(0.2938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1638.0
	 epoch  60 training error:  tensor(0.3457, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1642.0
	 epoch  70 training error:  tensor(0.2854, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1640.0
	 epoch  80 training error:  tensor(0.2450, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1646.0
	 epoch  90 training error:  tensor(0.2452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1327.24267578125
Memory cached:  1640.0
Time for this trial:  3257.0554513931274
Memory status after this trial: 
Memory allocated:  2647.8837890625
Memory cached:  2662.0
--------------------  Trial  160   --------------------
Start timing: 
Parameters: 
{'W_layers': 5, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 6, 'W_layer_units_exponent_3': 4, 'W_layer_units_exponent_4': 4, 'D_layers': 3, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 10, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 8, 'D_dagger_layer_units_exponent_1': 9, 'D_dagger_layer_units_exponent_2': 6, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 5, 'D_dagger_layer_units_exponent_5': 5, 'log_learning_rate': -2.48698993759426, 'log_learning_rate_D': -3.3725985061435533, 'log_learning_rate_D_dagger': -3.148269595368481, 'training_batch_size': 6, 'training_p': 2}
	 epoch  0 training error:  tensor(0.6558, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1522.0
	 epoch  10 training error:  tensor(0.1855, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1606.0
	 epoch  20 training error:  tensor(0.1798, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1590.0
	 epoch  30 training error:  tensor(0.1752, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1596.0
	 epoch  40 training error:  tensor(0.1082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1576.0
	 epoch  50 training error:  tensor(0.0932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1568.0
	 epoch  60 training error:  tensor(0.0501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1580.0
	 epoch  70 training error:  tensor(0.0438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1572.0
	 epoch  80 training error:  tensor(0.0546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1576.0
	 epoch  90 training error:  tensor(0.0293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1278.83251953125
Memory cached:  1582.0
Time for this trial:  2918.2567222118378
Memory status after this trial: 
Memory allocated:  2125.6044921875
Memory cached:  2150.0
--------------------  Trial  161   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 4, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 9, 'W_layer_units_exponent_3': 8, 'D_layers': 5, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 9, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 9, 'D_layer_units_exponent_4': 10, 'D_dagger_layers': 8, 'D_dagger_layer_units_exponent_0': 9, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 7, 'D_dagger_layer_units_exponent_3': 7, 'D_dagger_layer_units_exponent_4': 6, 'D_dagger_layer_units_exponent_5': 10, 'D_dagger_layer_units_exponent_6': 6, 'D_dagger_layer_units_exponent_7': 5, 'log_learning_rate': -2.7590828809612833, 'log_learning_rate_D': -3.9080090703601966, 'log_learning_rate_D_dagger': -3.5721616491367465, 'training_batch_size': 6, 'training_p': 5}
	 epoch  0 training error:  tensor(1.3950, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1458.0
	 epoch  10 training error:  tensor(0.3186, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1650.0
	 epoch  20 training error:  tensor(0.2995, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1632.0
	 epoch  30 training error:  tensor(0.1560, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1632.0
	 epoch  40 training error:  tensor(0.0911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1610.0
	 epoch  50 training error:  tensor(0.0728, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1640.0
	 epoch  60 training error:  tensor(0.0833, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1299.89697265625
Memory cached:  1628.0
