/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[I 2024-03-12 11:34:26,353] Using an existing study with name 'my_study1' instead of creating a new one.
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_smallDRS_smallA_400.pt
Vs.shape:  torch.Size([400, 100])
thetas.shape:  torch.Size([400, 100])
fs.shape:  torch.Size([400, 100])
ts.shape:  torch.Size([400, 100])
Xs.shape:  torch.Size([400, 100])
No pruned database has been founded.
--------------------  Trial  1   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.503296600565975, 'log_learning_rate_D': -4.945240276811711, 'log_learning_rate_D_dagger': -3.899280454200234, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(3.8055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.7792, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  20 training error:  tensor(3.7531, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  30 training error:  tensor(3.7271, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  40 training error:  tensor(3.7011, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  2.0
	 epoch  50 training error:  tensor(3.6752, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  60 training error:  tensor(3.6493, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  70 training error:  tensor(3.6236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  80 training error:  tensor(3.5980, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  4.0
	 epoch  90 training error:  tensor(3.5724, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.033203125
Memory cached:  2.0
[I 2024-03-12 11:37:00,042] Trial 1 finished with value: 3.04042649269104 and parameters: {'log_learning_rate': -3.503296600565975, 'log_learning_rate_D': -4.945240276811711, 'log_learning_rate_D_dagger': -3.899280454200234, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 2}. Best is trial 1 with value: 3.04042649269104.
res:  tensor(3.0404, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  153.46298789978027
Memory status after this trial: 
Memory allocated:  0.87939453125
Memory cached:  2.0
--------------------  Trial  2   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.921090911927871, 'log_learning_rate_D': -4.202396297417765, 'log_learning_rate_D_dagger': -1.0281422240456912, 'training_batch_size': 11, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(172.6707, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27587890625
Memory cached:  6.0
[W 2024-03-12 11:37:02,985] Trial 2 failed with parameters: {'log_learning_rate': -1.921090911927871, 'log_learning_rate_D': -4.202396297417765, 'log_learning_rate_D_dagger': -1.0281422240456912, 'training_batch_size': 11, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 11:37:02,985] Trial 2 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.7610926628112793
Memory status after this trial: 
Memory allocated:  2.8828125
Memory cached:  6.0
--------------------  Trial  3   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6960067371892422, 'log_learning_rate_D': -1.6029785254244833, 'log_learning_rate_D_dagger': -3.6490280146550513, 'training_batch_size': 8, 'training_p': 8, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(2.4935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  10 training error:  tensor(2.1171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  20 training error:  tensor(1.7526, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  30 training error:  tensor(1.4179, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  40 training error:  tensor(1.2325, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  50 training error:  tensor(1.1606, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  60 training error:  tensor(1.1494, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  70 training error:  tensor(1.1357, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  80 training error:  tensor(1.1217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
	 epoch  90 training error:  tensor(1.1080, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  0.90966796875
Memory cached:  2.0
[I 2024-03-12 11:39:37,170] Trial 3 finished with value: 0.9308595657348633 and parameters: {'log_learning_rate': -1.6960067371892422, 'log_learning_rate_D': -1.6029785254244833, 'log_learning_rate_D_dagger': -3.6490280146550513, 'training_batch_size': 8, 'training_p': 8, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 2}. Best is trial 3 with value: 0.9308595657348633.
res:  tensor(0.9309, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(3.0404, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  153.9752779006958
Memory status after this trial: 
Memory allocated:  0.87939453125
Memory cached:  4.0
--------------------  Trial  4   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.779882628429821, 'log_learning_rate_D': -3.4130183096539457, 'log_learning_rate_D_dagger': -4.063514057823172, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

[W 2024-03-12 11:39:41,000] Trial 4 failed with parameters: {'log_learning_rate': -2.779882628429821, 'log_learning_rate_D': -3.4130183096539457, 'log_learning_rate_D_dagger': -4.063514057823172, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 11:39:41,000] Trial 4 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.6283185482025146
Memory status after this trial: 
Memory allocated:  3.2734375
Memory cached:  4.0
--------------------  Trial  5   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.3011605048668486, 'log_learning_rate_D': -4.116238179481009, 'log_learning_rate_D_dagger': -1.339523135496545, 'training_batch_size': 6, 'training_p': 8, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(209.4896, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.8628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5696, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5513, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5419, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5398, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5402, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.27783203125
Memory cached:  4.0
[I 2024-03-12 11:47:11,416] Trial 5 finished with value: 0.4071817994117737 and parameters: {'log_learning_rate': -3.3011605048668486, 'log_learning_rate_D': -4.116238179481009, 'log_learning_rate_D_dagger': -1.339523135496545, 'training_batch_size': 6, 'training_p': 8, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 3}. Best is trial 5 with value: 0.4071817994117737.
res:  tensor(0.4072, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.9309, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  450.16140961647034
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  6   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.395085622565337, 'log_learning_rate_D': -4.939969158959922, 'log_learning_rate_D_dagger': -2.3019380962537106, 'training_batch_size': 10, 'training_p': 8, 'p_order_W': 5, 'p_order_D': 2, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(406.0682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(353.6851, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(305.4835, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(261.7872, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(222.6708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(188.0123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  60 training error:  tensor(157.5667, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(131.0247, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(108.0489, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(88.2962, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 11:49:05,775] Trial 6 finished with value: 40.53414535522461 and parameters: {'log_learning_rate': -4.395085622565337, 'log_learning_rate_D': -4.939969158959922, 'log_learning_rate_D_dagger': -2.3019380962537106, 'training_batch_size': 10, 'training_p': 8, 'p_order_W': 5, 'p_order_D': 2, 'p_order_D_dagger': 3}. Best is trial 5 with value: 0.4071817994117737.
Time for this trial:  114.17075228691101
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  8.0
--------------------  Trial  7   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.598409035666462, 'log_learning_rate_D': -1.8477973781096515, 'log_learning_rate_D_dagger': -4.48430141863183, 'training_batch_size': 8, 'training_p': 6, 'p_order_W': 5, 'p_order_D': 3, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(1.0248, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.0213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.0177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.0142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(1.0107, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.0073, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.0038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.0003, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.9969, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.9935, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 11:52:25,038] Trial 7 finished with value: 0.802330493927002 and parameters: {'log_learning_rate': -4.598409035666462, 'log_learning_rate_D': -1.8477973781096515, 'log_learning_rate_D_dagger': -4.48430141863183, 'training_batch_size': 8, 'training_p': 6, 'p_order_W': 5, 'p_order_D': 3, 'p_order_D_dagger': 2}. Best is trial 5 with value: 0.4071817994117737.
Time for this trial:  199.05262851715088
Memory status after this trial: 
Memory allocated:  3.3505859375
Memory cached:  4.0
--------------------  Trial  8   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.7996986051431927, 'log_learning_rate_D': -3.4987008074464874, 'log_learning_rate_D_dagger': -2.7685519085668155, 'training_batch_size': 9, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[W 2024-03-12 11:52:28,622] Trial 8 failed with parameters: {'log_learning_rate': -2.7996986051431927, 'log_learning_rate_D': -3.4987008074464874, 'log_learning_rate_D_dagger': -2.7685519085668155, 'training_batch_size': 9, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 11:52:28,622] Trial 8 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.379140853881836
Memory status after this trial: 
Memory allocated:  4.328125
Memory cached:  6.0
--------------------  Trial  9   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.3740148384211133, 'log_learning_rate_D': -3.972424491900281, 'log_learning_rate_D_dagger': -2.680284136177319, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(3348.6548, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  10 training error:  tensor(2625.4653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  20 training error:  tensor(1980.9633, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  30 training error:  tensor(1418.3832, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  40 training error:  tensor(933.6502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  50 training error:  tensor(516.2464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  60 training error:  tensor(155.3680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  70 training error:  tensor(161.6859, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  80 training error:  tensor(135.3315, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  90 training error:  tensor(110.3071, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
[I 2024-03-12 11:54:10,809] Trial 9 finished with value: 77.36930084228516 and parameters: {'log_learning_rate': -3.3740148384211133, 'log_learning_rate_D': -3.972424491900281, 'log_learning_rate_D_dagger': -2.680284136177319, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 4}. Best is trial 5 with value: 0.4071817994117737.
Time for this trial:  101.96224355697632
Memory status after this trial: 
Memory allocated:  3.201171875
Memory cached:  6.0
--------------------  Trial  10   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.7657754464270035, 'log_learning_rate_D': -4.737444155996021, 'log_learning_rate_D_dagger': -1.6462697122651107, 'training_batch_size': 7, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(5840.6392, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(238.0846, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[W 2024-03-12 11:55:11,901] Trial 10 failed with parameters: {'log_learning_rate': -4.7657754464270035, 'log_learning_rate_D': -4.737444155996021, 'log_learning_rate_D_dagger': -1.6462697122651107, 'training_batch_size': 7, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 11:55:11,901] Trial 10 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  60.86395597457886
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  11   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.5693623471217624, 'log_learning_rate_D': -2.5478206116423046, 'log_learning_rate_D_dagger': -4.502466456436638, 'training_batch_size': 10, 'training_p': 3, 'p_order_W': 2, 'p_order_D': 5, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(1.2452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.2234, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.2026, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.1830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  40 training error:  tensor(1.1645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  50 training error:  tensor(1.1473, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.1313, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.1164, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  80 training error:  tensor(1.1027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
	 epoch  90 training error:  tensor(1.0901, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  4.0
[I 2024-03-12 11:56:41,630] Trial 11 finished with value: 0.976439893245697 and parameters: {'log_learning_rate': -2.5693623471217624, 'log_learning_rate_D': -2.5478206116423046, 'log_learning_rate_D_dagger': -4.502466456436638, 'training_batch_size': 10, 'training_p': 3, 'p_order_W': 2, 'p_order_D': 5, 'p_order_D_dagger': 2}. Best is trial 5 with value: 0.4071817994117737.
Time for this trial:  89.5182409286499
Memory status after this trial: 
Memory allocated:  2.419921875
Memory cached:  4.0
--------------------  Trial  12   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1729375200554322, 'log_learning_rate_D': -4.182938129427146, 'log_learning_rate_D_dagger': -1.2500125262242237, 'training_batch_size': 12, 'training_p': 8, 'p_order_W': 5, 'p_order_D': 5, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[W 2024-03-12 11:56:45,485] Trial 12 failed with parameters: {'log_learning_rate': -1.1729375200554322, 'log_learning_rate_D': -4.182938129427146, 'log_learning_rate_D_dagger': -1.2500125262242237, 'training_batch_size': 12, 'training_p': 8, 'p_order_W': 5, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 11:56:45,485] Trial 12 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.646381378173828
Memory status after this trial: 
Memory allocated:  4.5224609375
Memory cached:  8.0
--------------------  Trial  13   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.4608362720876125, 'log_learning_rate_D': -3.0640753997441688, 'log_learning_rate_D_dagger': -1.0048078858619216, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(5296.2104, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  10 training error:  tensor(75.6091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  20 training error:  tensor(9.5800, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.5908, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6302, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4907, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4753, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
[I 2024-03-12 12:01:15,817] Trial 13 finished with value: 0.38328081369400024 and parameters: {'log_learning_rate': -2.4608362720876125, 'log_learning_rate_D': -3.0640753997441688, 'log_learning_rate_D_dagger': -1.0048078858619216, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 13 with value: 0.38328081369400024.
res:  tensor(0.3833, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.4072, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  270.10968565940857
Memory status after this trial: 
Memory allocated:  1.66064453125
Memory cached:  4.0
--------------------  Trial  14   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.9765954158366075, 'log_learning_rate_D': -3.568547782202887, 'log_learning_rate_D_dagger': -4.71155174097608, 'training_batch_size': 8, 'training_p': 8, 'p_order_W': 4, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  10 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  20 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  30 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  40 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  50 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  60 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  70 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  80 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  90 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
[I 2024-03-12 12:04:53,890] Trial 14 finished with value: 5004.85302734375 and parameters: {'log_learning_rate': -2.9765954158366075, 'log_learning_rate_D': -3.568547782202887, 'log_learning_rate_D_dagger': -4.71155174097608, 'training_batch_size': 8, 'training_p': 8, 'p_order_W': 4, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 13 with value: 0.38328081369400024.
Time for this trial:  217.872642993927
Memory status after this trial: 
Memory allocated:  4.0546875
Memory cached:  6.0
--------------------  Trial  15   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -4.832735033157547, 'log_learning_rate_D': -2.216766560938126, 'log_learning_rate_D_dagger': -1.8920681939004607, 'training_batch_size': 10, 'training_p': 8, 'p_order_W': 4, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(34.3657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  10 training error:  tensor(4.6152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  20 training error:  tensor(4.0113, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  30 training error:  tensor(2.1276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.9563, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.7249, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.6934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.6452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.6261, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.6159, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
[I 2024-03-12 12:06:46,188] Trial 15 finished with value: 0.5414871573448181 and parameters: {'log_learning_rate': -4.832735033157547, 'log_learning_rate_D': -2.216766560938126, 'log_learning_rate_D_dagger': -1.8920681939004607, 'training_batch_size': 10, 'training_p': 8, 'p_order_W': 4, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 13 with value: 0.38328081369400024.
Time for this trial:  112.08714890480042
Memory status after this trial: 
Memory allocated:  3.6640625
Memory cached:  8.0
--------------------  Trial  16   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.0809142372298295, 'log_learning_rate_D': -1.1022292862340595, 'log_learning_rate_D_dagger': -1.073578563937649, 'training_batch_size': 12, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(241673.2969, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  10 training error:  tensor(5611.1538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  10.0
	 epoch  20 training error:  tensor(1634.2079, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  10.0
	 epoch  30 training error:  tensor(1458.9139, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  10.0
	 epoch  40 training error:  tensor(1105.7098, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  50 training error:  tensor(705.8468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  60 training error:  tensor(399.7982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  70 training error:  tensor(200.2168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  80 training error:  tensor(111.3282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  90 training error:  tensor(67.5576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
[I 2024-03-12 12:08:44,267] Trial 16 finished with value: 37.53911209106445 and parameters: {'log_learning_rate': -1.0809142372298295, 'log_learning_rate_D': -1.1022292862340595, 'log_learning_rate_D_dagger': -1.073578563937649, 'training_batch_size': 12, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 5}. Best is trial 13 with value: 0.38328081369400024.
Time for this trial:  117.82487392425537
Memory status after this trial: 
Memory allocated:  4.2509765625
Memory cached:  6.0
--------------------  Trial  17   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.6224648406399194, 'log_learning_rate_D': -3.0401432103765194, 'log_learning_rate_D_dagger': -1.0691996390474983, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  10 training error:  tensor(254.1876, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  20 training error:  tensor(31.0417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  30 training error:  tensor(3.4766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  40 training error:  tensor(5.8848, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  50 training error:  tensor(10.1171, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  60 training error:  tensor(6.1674, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  70 training error:  tensor(3.1554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  80 training error:  tensor(47.0358, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  90 training error:  tensor(13876.7812, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
[W 2024-03-12 12:16:41,974] Trial 17 failed with parameters: {'log_learning_rate': -2.6224648406399194, 'log_learning_rate_D': -3.0401432103765194, 'log_learning_rate_D_dagger': -1.0691996390474983, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:16:41,975] Trial 17 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  477.4321858882904
Memory status after this trial: 
Memory allocated:  4.3505859375
Memory cached:  6.0
--------------------  Trial  18   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.366358896699847, 'log_learning_rate_D': -3.1329618548470055, 'log_learning_rate_D_dagger': -1.035169999412934, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  10 training error:  tensor(2829.8748, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  20 training error:  tensor(65.0123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  30 training error:  tensor(31.3744, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  40 training error:  tensor(11.6756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  50 training error:  tensor(9.9396, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  60 training error:  tensor(10.9158, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  70 training error:  tensor(6.1051, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  80 training error:  tensor(3.7675, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  90 training error:  tensor(6.9496, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
[I 2024-03-12 12:25:18,983] Trial 18 finished with value: 7.364121913909912 and parameters: {'log_learning_rate': -2.366358896699847, 'log_learning_rate_D': -3.1329618548470055, 'log_learning_rate_D_dagger': -1.035169999412934, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}. Best is trial 13 with value: 0.38328081369400024.
Time for this trial:  516.6880497932434
Memory status after this trial: 
Memory allocated:  4.3505859375
Memory cached:  6.0
--------------------  Trial  19   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.7582226632640494, 'log_learning_rate_D': -3.7251649488542116, 'log_learning_rate_D_dagger': -1.709772193721417, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(22015.6152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
[W 2024-03-12 12:25:49,115] Trial 19 failed with parameters: {'log_learning_rate': -3.7582226632640494, 'log_learning_rate_D': -3.7251649488542116, 'log_learning_rate_D_dagger': -1.709772193721417, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:25:49,115] Trial 19 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  29.866018772125244
Memory status after this trial: 
Memory allocated:  3.9599609375
Memory cached:  6.0
--------------------  Trial  20   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.80071373193577, 'log_learning_rate_D': -3.923130215407501, 'log_learning_rate_D_dagger': -1.6193764547030687, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(19703.0918, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  10 training error:  tensor(314.7659, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  20 training error:  tensor(57.6909, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  30 training error:  tensor(22.9543, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  40 training error:  tensor(3.2552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  50 training error:  tensor(2.1115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  60 training error:  tensor(1.6032, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  70 training error:  tensor(1.2551, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.9460, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.7640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
[I 2024-03-12 12:33:46,519] Trial 20 finished with value: 0.5980397462844849 and parameters: {'log_learning_rate': -3.80071373193577, 'log_learning_rate_D': -3.923130215407501, 'log_learning_rate_D_dagger': -1.6193764547030687, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 13 with value: 0.38328081369400024.
Time for this trial:  477.1299512386322
Memory status after this trial: 
Memory allocated:  3.9599609375
Memory cached:  6.0
--------------------  Trial  21   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3895521013448318, 'log_learning_rate_D': -2.867334841547763, 'log_learning_rate_D_dagger': -1.6326106494245993, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(50.0026, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.7239, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5540, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5365, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5148, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5086, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5050, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5007, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  4.0
[I 2024-03-12 12:41:17,360] Trial 21 finished with value: 0.3790839612483978 and parameters: {'log_learning_rate': -2.3895521013448318, 'log_learning_rate_D': -2.867334841547763, 'log_learning_rate_D_dagger': -1.6326106494245993, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 3}. Best is trial 21 with value: 0.3790839612483978.
res:  tensor(0.3791, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3833, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  450.557404756546
Memory status after this trial: 
Memory allocated:  1.91162109375
Memory cached:  4.0
--------------------  Trial  22   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1757093289765796, 'log_learning_rate_D': -2.791207155158271, 'log_learning_rate_D_dagger': -1.8308816342492282, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

[W 2024-03-12 12:41:21,559] Trial 22 failed with parameters: {'log_learning_rate': -2.1757093289765796, 'log_learning_rate_D': -2.791207155158271, 'log_learning_rate_D_dagger': -1.8308816342492282, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:41:21,559] Trial 22 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.9641103744506836
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  23   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.200219847453791, 'log_learning_rate_D': -2.907660660219667, 'log_learning_rate_D_dagger': -1.940947558254062, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(24111.9180, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1176.8573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(316.9455, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(125.5311, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(121.7501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(86.8448, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(66.5637, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(57.6743, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(35.1118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[W 2024-03-12 12:46:12,140] Trial 23 failed with parameters: {'log_learning_rate': -2.200219847453791, 'log_learning_rate_D': -2.907660660219667, 'log_learning_rate_D_dagger': -1.940947558254062, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:46:12,140] Trial 23 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  290.32528424263
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  24   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1068524063650744, 'log_learning_rate_D': -2.6938904438350333, 'log_learning_rate_D_dagger': -1.810548759257356, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(27953.4434, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(838.0822, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(230.5240, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(107.8814, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(62.1201, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(22.9324, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(34.3043, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(35.9345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(51.1742, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(19.3769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-12 12:51:22,236] Trial 24 finished with value: 4.506443023681641 and parameters: {'log_learning_rate': -2.1068524063650744, 'log_learning_rate_D': -2.6938904438350333, 'log_learning_rate_D_dagger': -1.810548759257356, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  309.80715918540955
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  25   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.703991395250501, 'log_learning_rate_D': -2.9779081215149317, 'log_learning_rate_D_dagger': -2.1514409990475096, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(236.9177, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  10 training error:  tensor(124.5272, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  20 training error:  tensor(56.6959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  30 training error:  tensor(21.4334, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  40 training error:  tensor(5.3770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.8808, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.6969, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5648, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5087, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  4.0
[I 2024-03-12 12:55:32,907] Trial 25 finished with value: 0.5091535449028015 and parameters: {'log_learning_rate': -2.703991395250501, 'log_learning_rate_D': -2.9779081215149317, 'log_learning_rate_D_dagger': -2.1514409990475096, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 3}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  250.4321632385254
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  4.0
--------------------  Trial  26   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.97363213494444, 'log_learning_rate_D': -3.238954301174083, 'log_learning_rate_D_dagger': -1.4213100313351505, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:55:37,196] Trial 26 failed with parameters: {'log_learning_rate': -1.97363213494444, 'log_learning_rate_D': -3.238954301174083, 'log_learning_rate_D_dagger': -1.4213100313351505, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:55:37,197] Trial 26 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.045319080352783
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  27   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9935938007368534, 'log_learning_rate_D': -2.3784306264558106, 'log_learning_rate_D_dagger': -1.520916526535556, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:55:41,478] Trial 27 failed with parameters: {'log_learning_rate': -1.9935938007368534, 'log_learning_rate_D': -2.3784306264558106, 'log_learning_rate_D_dagger': -1.520916526535556, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:55:41,478] Trial 27 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.037700653076172
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  28   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0771181567836665, 'log_learning_rate_D': -3.164455618658523, 'log_learning_rate_D_dagger': -1.466458852321527, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:55:45,806] Trial 28 failed with parameters: {'log_learning_rate': -2.0771181567836665, 'log_learning_rate_D': -3.164455618658523, 'log_learning_rate_D_dagger': -1.466458852321527, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:55:45,806] Trial 28 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.054617881774902
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  29   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8292115184033766, 'log_learning_rate_D': -3.320153221520101, 'log_learning_rate_D_dagger': -1.468645284306517, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:55:50,095] Trial 29 failed with parameters: {'log_learning_rate': -1.8292115184033766, 'log_learning_rate_D': -3.320153221520101, 'log_learning_rate_D_dagger': -1.468645284306517, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:55:50,096] Trial 29 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.018587589263916
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  30   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.804981212477719, 'log_learning_rate_D': -3.193151508555439, 'log_learning_rate_D_dagger': -1.5864230900497107, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:55:54,349] Trial 30 failed with parameters: {'log_learning_rate': -1.804981212477719, 'log_learning_rate_D': -3.193151508555439, 'log_learning_rate_D_dagger': -1.5864230900497107, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:55:54,349] Trial 30 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.9935081005096436
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  31   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.918647069253828, 'log_learning_rate_D': -3.2085640722534823, 'log_learning_rate_D_dagger': -1.517788614064389, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:55:58,667] Trial 31 failed with parameters: {'log_learning_rate': -1.918647069253828, 'log_learning_rate_D': -3.2085640722534823, 'log_learning_rate_D_dagger': -1.517788614064389, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:55:58,667] Trial 31 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.070694923400879
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  32   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8682328157628605, 'log_learning_rate_D': -3.246003002460337, 'log_learning_rate_D_dagger': -1.486630317752128, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:56:02,981] Trial 32 failed with parameters: {'log_learning_rate': -1.8682328157628605, 'log_learning_rate_D': -3.246003002460337, 'log_learning_rate_D_dagger': -1.486630317752128, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:56:02,981] Trial 32 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.056346416473389
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  33   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0221696608906754, 'log_learning_rate_D': -3.3274300511364663, 'log_learning_rate_D_dagger': -1.515386410134244, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 12:56:07,363] Trial 33 failed with parameters: {'log_learning_rate': -2.0221696608906754, 'log_learning_rate_D': -3.3274300511364663, 'log_learning_rate_D_dagger': -1.515386410134244, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 12:56:07,363] Trial 33 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.11839747428894
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  34   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9593685540179921, 'log_learning_rate_D': -3.2396426532582674, 'log_learning_rate_D_dagger': -1.459257103852265, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(536.2047, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  20 training error:  tensor(41.7878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  30 training error:  tensor(32.9798, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  40 training error:  tensor(20.6776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  50 training error:  tensor(55.6839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  60 training error:  tensor(33.6665, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  70 training error:  tensor(23.8874, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  80 training error:  tensor(47.2782, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  90 training error:  tensor(148.5760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[I 2024-03-12 13:01:24,535] Trial 34 finished with value: 22.473731994628906 and parameters: {'log_learning_rate': -1.9593685540179921, 'log_learning_rate_D': -3.2396426532582674, 'log_learning_rate_D_dagger': -1.459257103852265, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 5}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  316.8870687484741
Memory status after this trial: 
Memory allocated:  4.1337890625
Memory cached:  6.0
--------------------  Trial  35   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.8030275954172925, 'log_learning_rate_D': -2.418454494004095, 'log_learning_rate_D_dagger': -2.709596670252173, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(19516.9766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  10 training error:  tensor(11812.0801, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  20 training error:  tensor(5663.9243, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  30 training error:  tensor(658.8345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  40 training error:  tensor(241.1647, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  50 training error:  tensor(228.2831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  60 training error:  tensor(217.8694, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  70 training error:  tensor(207.1805, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  80 training error:  tensor(197.2664, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
	 epoch  90 training error:  tensor(184.5354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  4.0
[I 2024-03-12 13:08:42,412] Trial 35 finished with value: 132.99295043945312 and parameters: {'log_learning_rate': -2.8030275954172925, 'log_learning_rate_D': -2.418454494004095, 'log_learning_rate_D_dagger': -2.709596670252173, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  437.6048731803894
Memory status after this trial: 
Memory allocated:  3.203125
Memory cached:  4.0
--------------------  Trial  36   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5936746078408464, 'log_learning_rate_D': -2.8473553002501095, 'log_learning_rate_D_dagger': -1.0188322686512006, 'training_batch_size': 12, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(126.8520, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(11.9961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.4381, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(1.8655, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(1.0409, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.6944, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4848, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 13:10:28,608] Trial 36 finished with value: 0.3969709575176239 and parameters: {'log_learning_rate': -1.5936746078408464, 'log_learning_rate_D': -2.8473553002501095, 'log_learning_rate_D_dagger': -1.0188322686512006, 'training_batch_size': 12, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  105.96579194068909
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  37   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2537428633230805, 'log_learning_rate_D': -2.2072664102862936, 'log_learning_rate_D_dagger': -1.5162689947199526, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(3533.2246, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
[W 2024-03-12 13:10:41,132] Trial 37 failed with parameters: {'log_learning_rate': -2.2537428633230805, 'log_learning_rate_D': -2.2072664102862936, 'log_learning_rate_D_dagger': -1.5162689947199526, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:10:41,133] Trial 37 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  12.263001441955566
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  38   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3685395162607454, 'log_learning_rate_D': -2.08612188112732, 'log_learning_rate_D_dagger': -1.510015316381475, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 4}

[W 2024-03-12 13:10:45,458] Trial 38 failed with parameters: {'log_learning_rate': -2.3685395162607454, 'log_learning_rate_D': -2.08612188112732, 'log_learning_rate_D_dagger': -1.510015316381475, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:10:45,458] Trial 38 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  4.058103084564209
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  39   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.331402115600243, 'log_learning_rate_D': -3.363383390248951, 'log_learning_rate_D_dagger': -1.5579260204373717, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  4.0
	 epoch  10 training error:  tensor(329.7204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(137.7829, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(43.3427, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(25.5315, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(31.1380, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(14.6225, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(7.5495, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(5.4301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(4.4590, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-12 13:16:03,383] Trial 39 finished with value: 5.503692626953125 and parameters: {'log_learning_rate': -2.331402115600243, 'log_learning_rate_D': -3.363383390248951, 'log_learning_rate_D_dagger': -1.5579260204373717, 'training_batch_size': 7, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 2, 'p_order_D_dagger': 4}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  317.6546576023102
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  40   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.901228596687301, 'log_learning_rate_D': -2.233366537356982, 'log_learning_rate_D_dagger': -2.085519980777457, 'training_batch_size': 9, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 5, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(99.1822, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  10 training error:  tensor(55.3954, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  20 training error:  tensor(21.5431, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  30 training error:  tensor(4.6973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  40 training error:  tensor(5.2135, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  50 training error:  tensor(3.4444, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  60 training error:  tensor(2.1105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  70 training error:  tensor(1.2497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  80 training error:  tensor(1.1889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  90 training error:  tensor(1.0065, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
[I 2024-03-12 13:17:40,335] Trial 40 finished with value: 0.8804454207420349 and parameters: {'log_learning_rate': -2.901228596687301, 'log_learning_rate_D': -2.233366537356982, 'log_learning_rate_D_dagger': -2.085519980777457, 'training_batch_size': 9, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 5, 'p_order_D_dagger': 3}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  96.70795178413391
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  4.0
--------------------  Trial  41   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.601743142782623, 'log_learning_rate_D': -2.7928651323711944, 'log_learning_rate_D_dagger': -1.0145748885688557, 'training_batch_size': 12, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(236.7262, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(2.3001, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(16.1187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(3.7173, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.5404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.8766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5580, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5207, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.5130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.5070, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 13:19:26,415] Trial 41 finished with value: 0.44378575682640076 and parameters: {'log_learning_rate': -1.601743142782623, 'log_learning_rate_D': -2.7928651323711944, 'log_learning_rate_D_dagger': -1.0145748885688557, 'training_batch_size': 12, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  105.84190773963928
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  42   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3824279784675593, 'log_learning_rate_D': -2.863519644929432, 'log_learning_rate_D_dagger': -1.4887547050587049, 'training_batch_size': 11, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(135.4477, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(22.8082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(4.4511, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(4.4727, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.7641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.9013, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.6878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5516, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.5499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.5438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 13:21:12,484] Trial 42 finished with value: 0.48181167244911194 and parameters: {'log_learning_rate': -1.3824279784675593, 'log_learning_rate_D': -2.863519644929432, 'log_learning_rate_D_dagger': -1.4887547050587049, 'training_batch_size': 11, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 21 with value: 0.3790839612483978.
Time for this trial:  105.80896949768066
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  43   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.939942857631245, 'log_learning_rate_D': -2.9763247911366237, 'log_learning_rate_D_dagger': -1.2612656433368021, 'training_batch_size': 11, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(316.0692, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(17.9543, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(11.7508, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(1.9266, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(1.8646, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  50 training error:  tensor(1.7297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4907, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5942, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4711, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4791, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[I 2024-03-12 13:22:58,120] Trial 43 finished with value: 0.36884307861328125 and parameters: {'log_learning_rate': -1.939942857631245, 'log_learning_rate_D': -2.9763247911366237, 'log_learning_rate_D_dagger': -1.2612656433368021, 'training_batch_size': 11, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 43 with value: 0.36884307861328125.
res:  tensor(0.3688, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3791, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  105.40497422218323
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  6.0
--------------------  Trial  44   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1996229816391475, 'log_learning_rate_D': -3.4644734243967905, 'log_learning_rate_D_dagger': -1.7969607814378707, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[W 2024-03-12 13:23:01,572] Trial 44 failed with parameters: {'log_learning_rate': -2.1996229816391475, 'log_learning_rate_D': -3.4644734243967905, 'log_learning_rate_D_dagger': -1.7969607814378707, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:23:01,572] Trial 44 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.2295331954956055
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  8.0
--------------------  Trial  45   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1482798179713414, 'log_learning_rate_D': -3.436697646534506, 'log_learning_rate_D_dagger': -1.8006081451932088, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[W 2024-03-12 13:23:05,026] Trial 45 failed with parameters: {'log_learning_rate': -2.1482798179713414, 'log_learning_rate_D': -3.436697646534506, 'log_learning_rate_D_dagger': -1.8006081451932088, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:23:05,027] Trial 45 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.229825258255005
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  8.0
--------------------  Trial  46   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0622973499616446, 'log_learning_rate_D': -3.5665777013569975, 'log_learning_rate_D_dagger': -1.7826375377783816, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
[W 2024-03-12 13:23:08,479] Trial 46 failed with parameters: {'log_learning_rate': -2.0622973499616446, 'log_learning_rate_D': -3.5665777013569975, 'log_learning_rate_D_dagger': -1.7826375377783816, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:23:08,480] Trial 46 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.2330191135406494
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  8.0
--------------------  Trial  47   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0467602288692683, 'log_learning_rate_D': -3.090312598657671, 'log_learning_rate_D_dagger': -1.8411688617248414, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(inf, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
	 epoch  20 training error:  tensor(17469.8906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(7388.9956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(1556.1547, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
	 epoch  50 training error:  tensor(977.3465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
	 epoch  60 training error:  tensor(450.4673, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
	 epoch  70 training error:  tensor(326.8869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
	 epoch  80 training error:  tensor(151.8280, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
	 epoch  90 training error:  tensor(104.3768, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  10.0
[I 2024-03-12 13:25:07,293] Trial 47 finished with value: 40.47876739501953 and parameters: {'log_learning_rate': -2.0467602288692683, 'log_learning_rate_D': -3.090312598657671, 'log_learning_rate_D_dagger': -1.8411688617248414, 'training_batch_size': 11, 'training_p': 7, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  118.55159878730774
Memory status after this trial: 
Memory allocated:  3.9375
Memory cached:  6.0
--------------------  Trial  48   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.4445081702253355, 'log_learning_rate_D': -2.5658435749847137, 'log_learning_rate_D_dagger': -1.3031825947339792, 'training_batch_size': 11, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(65.3643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(3.6251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(2.4374, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.6694, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.8031, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4844, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-12 13:26:50,958] Trial 48 finished with value: 0.4183550477027893 and parameters: {'log_learning_rate': -2.4445081702253355, 'log_learning_rate_D': -2.5658435749847137, 'log_learning_rate_D_dagger': -1.3031825947339792, 'training_batch_size': 11, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  103.41262984275818
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  49   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.173226059910786, 'log_learning_rate_D': -3.570676244357394, 'log_learning_rate_D_dagger': -1.367724049353198, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(3.2878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.8685, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.6078, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5668, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5597, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5547, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5529, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5095, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5085, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 13:30:36,436] Trial 49 finished with value: 0.40792274475097656 and parameters: {'log_learning_rate': -2.173226059910786, 'log_learning_rate_D': -3.570676244357394, 'log_learning_rate_D_dagger': -1.367724049353198, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 2}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  225.25202536582947
Memory status after this trial: 
Memory allocated:  2.419921875
Memory cached:  6.0
--------------------  Trial  50   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.5251034089033397, 'log_learning_rate_D': -3.290534865461124, 'log_learning_rate_D_dagger': -1.719950742959783, 'training_batch_size': 6, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(31817.8008, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  10 training error:  tensor(179.2042, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  20 training error:  tensor(2.4937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4351, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4227, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4172, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4153, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
[I 2024-03-12 13:38:41,377] Trial 50 finished with value: 0.3794870972633362 and parameters: {'log_learning_rate': -2.5251034089033397, 'log_learning_rate_D': -3.290534865461124, 'log_learning_rate_D_dagger': -1.719950742959783, 'training_batch_size': 6, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  484.65930819511414
Memory status after this trial: 
Memory allocated:  3.8427734375
Memory cached:  6.0
--------------------  Trial  51   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9476812240156187, 'log_learning_rate_D': -3.4491890658819124, 'log_learning_rate_D_dagger': -2.3857975010096766, 'training_batch_size': 6, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 13:38:48,050] Trial 51 failed with parameters: {'log_learning_rate': -1.9476812240156187, 'log_learning_rate_D': -3.4491890658819124, 'log_learning_rate_D_dagger': -2.3857975010096766, 'training_batch_size': 6, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:38:48,051] Trial 51 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  6.398228406906128
Memory status after this trial: 
Memory allocated:  4.427734375
Memory cached:  6.0
--------------------  Trial  52   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8939595115446073, 'log_learning_rate_D': -3.4560042036703824, 'log_learning_rate_D_dagger': -2.3327261854386627, 'training_batch_size': 6, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 5, 'p_order_D_dagger': 5}

[W 2024-03-12 13:38:54,625] Trial 52 failed with parameters: {'log_learning_rate': -1.8939595115446073, 'log_learning_rate_D': -3.4560042036703824, 'log_learning_rate_D_dagger': -2.3327261854386627, 'training_batch_size': 6, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 5, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 13:38:54,625] Trial 52 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  6.2956507205963135
Memory status after this trial: 
Memory allocated:  4.427734375
Memory cached:  6.0
--------------------  Trial  53   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8715292233512302, 'log_learning_rate_D': -3.4569562406665155, 'log_learning_rate_D_dagger': -2.3959822685245005, 'training_batch_size': 6, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 5, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(412031.3438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  10 training error:  tensor(14883.7832, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  20 training error:  tensor(2299.7441, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  30 training error:  tensor(915.8564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  40 training error:  tensor(1079.0032, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  50 training error:  tensor(738.4949, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  60 training error:  tensor(868.2111, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  70 training error:  tensor(903.9911, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  80 training error:  tensor(1779.2391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  90 training error:  tensor(1543.2444, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
[I 2024-03-12 13:48:09,468] Trial 53 finished with value: 967.4352416992188 and parameters: {'log_learning_rate': -1.8715292233512302, 'log_learning_rate_D': -3.4569562406665155, 'log_learning_rate_D_dagger': -2.3959822685245005, 'training_batch_size': 6, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 5, 'p_order_D_dagger': 5}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  554.5579655170441
Memory status after this trial: 
Memory allocated:  4.427734375
Memory cached:  6.0
--------------------  Trial  54   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.0476521436664843, 'log_learning_rate_D': -3.72989889676992, 'log_learning_rate_D_dagger': -1.7510898444206746, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(3.0716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.7363, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.9499, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6211, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5263, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5035, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4984, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-12 13:49:49,986] Trial 54 finished with value: 0.4914141595363617 and parameters: {'log_learning_rate': -3.0476521436664843, 'log_learning_rate_D': -3.72989889676992, 'log_learning_rate_D_dagger': -1.7510898444206746, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 2}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  100.22703409194946
Memory status after this trial: 
Memory allocated:  2.9619140625
Memory cached:  6.0
--------------------  Trial  55   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2583934392284646, 'log_learning_rate_D': -3.229301221321557, 'log_learning_rate_D_dagger': -2.0387166005506105, 'training_batch_size': 10, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(206.2236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  10 training error:  tensor(147.4176, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  20 training error:  tensor(99.7259, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  30 training error:  tensor(63.3391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  40 training error:  tensor(37.2948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  50 training error:  tensor(19.7839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  60 training error:  tensor(8.2795, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  70 training error:  tensor(1.7387, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
	 epoch  80 training error:  tensor(2.7228, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  8.0
	 epoch  90 training error:  tensor(1.6538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.93994140625
Memory cached:  6.0
[I 2024-03-12 13:51:35,287] Trial 55 finished with value: 1.2180579900741577 and parameters: {'log_learning_rate': -2.2583934392284646, 'log_learning_rate_D': -3.229301221321557, 'log_learning_rate_D_dagger': -2.0387166005506105, 'training_batch_size': 10, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  105.04755854606628
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  56   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.578094561252443, 'log_learning_rate_D': -2.9838420198074713, 'log_learning_rate_D_dagger': -1.2933828998843213, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(13173.9170, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  10 training error:  tensor(302.4137, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  20 training error:  tensor(6.6596, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.9347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.8052, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.7028, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.6265, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5392, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5161, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
[I 2024-03-12 13:58:54,928] Trial 56 finished with value: 0.3813466727733612 and parameters: {'log_learning_rate': -2.578094561252443, 'log_learning_rate_D': -2.9838420198074713, 'log_learning_rate_D_dagger': -1.2933828998843213, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  439.36855602264404
Memory status after this trial: 
Memory allocated:  3.203125
Memory cached:  6.0
--------------------  Trial  57   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.6620654747080303, 'log_learning_rate_D': -2.8599907159030584, 'log_learning_rate_D_dagger': -1.661659961347435, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(26635.0098, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  10 training error:  tensor(131.2587, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  20 training error:  tensor(8.3782, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  30 training error:  tensor(4.1699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  40 training error:  tensor(3.3080, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  50 training error:  tensor(2.8466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  60 training error:  tensor(2.5208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  70 training error:  tensor(2.3400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  80 training error:  tensor(2.1622, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  90 training error:  tensor(1.9896, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
[I 2024-03-12 14:06:13,228] Trial 57 finished with value: 1.5699869394302368 and parameters: {'log_learning_rate': -2.6620654747080303, 'log_learning_rate_D': -2.8599907159030584, 'log_learning_rate_D_dagger': -1.661659961347435, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  438.04821491241455
Memory status after this trial: 
Memory allocated:  3.203125
Memory cached:  6.0
--------------------  Trial  58   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7955277122808508, 'log_learning_rate_D': -3.3070462110340357, 'log_learning_rate_D_dagger': -1.2834488908782125, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(19082.2695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
[W 2024-03-12 14:06:28,765] Trial 58 failed with parameters: {'log_learning_rate': -1.7955277122808508, 'log_learning_rate_D': -3.3070462110340357, 'log_learning_rate_D_dagger': -1.2834488908782125, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 14:06:28,765] Trial 58 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  15.26102089881897
Memory status after this trial: 
Memory allocated:  3.8427734375
Memory cached:  6.0
--------------------  Trial  59   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8180371988397672, 'log_learning_rate_D': -3.3744262528152396, 'log_learning_rate_D_dagger': -1.2698420955724328, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(37973.8711, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  10 training error:  tensor(252.2555, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.6010, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.9916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.7198, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.6024, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5654, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5415, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5386, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.94189453125
Memory cached:  6.0
[I 2024-03-12 14:14:36,236] Trial 59 finished with value: 0.4592364430427551 and parameters: {'log_learning_rate': -1.8180371988397672, 'log_learning_rate_D': -3.3744262528152396, 'log_learning_rate_D_dagger': -1.2698420955724328, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 4}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  487.1903667449951
Memory status after this trial: 
Memory allocated:  3.8427734375
Memory cached:  6.0
--------------------  Trial  60   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.217951179965928, 'log_learning_rate_D': -3.040777144852251, 'log_learning_rate_D_dagger': -3.2668673383170876, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(144.6847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  10 training error:  tensor(139.5847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(134.6919, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(129.9269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(125.2812, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(120.7312, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(116.3100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(112.0110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(107.7902, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(103.7203, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-12 14:17:47,676] Trial 60 finished with value: 89.53341674804688 and parameters: {'log_learning_rate': -2.217951179965928, 'log_learning_rate_D': -3.040777144852251, 'log_learning_rate_D_dagger': -3.2668673383170876, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  191.18554186820984
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  61   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.701986168506034, 'log_learning_rate_D': -2.6143332523905265, 'log_learning_rate_D_dagger': -1.2589045577211628, 'training_batch_size': 8, 'training_p': 7, 'p_order_W': 5, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(168.9695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[W 2024-03-12 14:17:59,505] Trial 61 failed with parameters: {'log_learning_rate': -2.701986168506034, 'log_learning_rate_D': -2.6143332523905265, 'log_learning_rate_D_dagger': -1.2589045577211628, 'training_batch_size': 8, 'training_p': 7, 'p_order_W': 5, 'p_order_D': 3, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 14:17:59,505] Trial 61 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  11.583630323410034
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  62   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.466608636369622, 'log_learning_rate_D': -2.6198406425580743, 'log_learning_rate_D_dagger': -1.2650334737641922, 'training_batch_size': 8, 'training_p': 7, 'p_order_W': 5, 'p_order_D': 3, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(177.0391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  10 training error:  tensor(12.6562, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.5791, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5636, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5368, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5312, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5279, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5259, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5242, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-12 14:21:32,693] Trial 62 finished with value: 0.41058096289634705 and parameters: {'log_learning_rate': -2.466608636369622, 'log_learning_rate_D': -2.6198406425580743, 'log_learning_rate_D_dagger': -1.2650334737641922, 'training_batch_size': 8, 'training_p': 7, 'p_order_W': 5, 'p_order_D': 3, 'p_order_D_dagger': 3}. Best is trial 43 with value: 0.36884307861328125.
Time for this trial:  212.93830490112305
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  63   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.6899070833053123, 'log_learning_rate_D': -4.253653687119171, 'log_learning_rate_D_dagger': -1.622072010368637, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(13118.1895, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  10 training error:  tensor(57.0012, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  20 training error:  tensor(11.1146, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  30 training error:  tensor(5.9730, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  40 training error:  tensor(4.3824, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  50 training error:  tensor(2.1826, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5920, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5047, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5035, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5032, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57568359375
Memory cached:  6.0
[I 2024-03-12 14:28:49,468] Trial 63 finished with value: 0.36037781834602356 and parameters: {'log_learning_rate': -2.6899070833053123, 'log_learning_rate_D': -4.253653687119171, 'log_learning_rate_D_dagger': -1.622072010368637, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
res:  tensor(0.3604, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3688, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  436.52924156188965
Memory status after this trial: 
Memory allocated:  1.66259765625
Memory cached:  6.0
--------------------  Trial  64   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.1578305225956598, 'log_learning_rate_D': -4.439822633766953, 'log_learning_rate_D_dagger': -1.9407571881610828, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(78.4024, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  10 training error:  tensor(8.3888, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  20 training error:  tensor(2.5024, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.3541, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.7285, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5733, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5679, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5629, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5529, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
[I 2024-03-12 14:33:47,458] Trial 64 finished with value: 0.48022565245628357 and parameters: {'log_learning_rate': -3.1578305225956598, 'log_learning_rate_D': -4.439822633766953, 'log_learning_rate_D_dagger': -1.9407571881610828, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  297.7484953403473
Memory status after this trial: 
Memory allocated:  3.6640625
Memory cached:  6.0
--------------------  Trial  65   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.71953280511657, 'log_learning_rate_D': -4.306111249144283, 'log_learning_rate_D_dagger': -2.288539105988181, 'training_batch_size': 9, 'training_p': 7, 'p_order_W': 2, 'p_order_D': 5, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(3.3273, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  10 training error:  tensor(2.8105, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  20 training error:  tensor(2.3469, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.9420, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.6027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  50 training error:  tensor(1.3329, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  60 training error:  tensor(1.1245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.9683, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.8545, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.7732, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.78857421875
Memory cached:  6.0
[I 2024-03-12 14:35:17,639] Trial 65 finished with value: 0.66615229845047 and parameters: {'log_learning_rate': -2.71953280511657, 'log_learning_rate_D': -4.306111249144283, 'log_learning_rate_D_dagger': -2.288539105988181, 'training_batch_size': 9, 'training_p': 7, 'p_order_W': 2, 'p_order_D': 5, 'p_order_D_dagger': 2}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  89.93665957450867
Memory status after this trial: 
Memory allocated:  2.537109375
Memory cached:  6.0
--------------------  Trial  66   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0263265227978744, 'log_learning_rate_D': -4.737450099097351, 'log_learning_rate_D_dagger': -1.6136643808850055, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(10630.7363, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  10 training error:  tensor(48.5671, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  20 training error:  tensor(11.6428, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  30 training error:  tensor(7.7663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  40 training error:  tensor(3.7382, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  50 training error:  tensor(3.1658, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.6426, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.6898, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.9398, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.6228, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[I 2024-03-12 14:43:24,623] Trial 66 finished with value: 0.4496365487575531 and parameters: {'log_learning_rate': -2.0263265227978744, 'log_learning_rate_D': -4.737450099097351, 'log_learning_rate_D_dagger': -1.6136643808850055, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  486.70848631858826
Memory status after this trial: 
Memory allocated:  3.9599609375
Memory cached:  6.0
--------------------  Trial  67   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.2541995477443817, 'log_learning_rate_D': -3.7411500962150415, 'log_learning_rate_D_dagger': -1.7106414217122559, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(1556805.5000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  10 training error:  tensor(122369.2422, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
[W 2024-03-12 14:44:02,538] Trial 67 failed with parameters: {'log_learning_rate': -3.2541995477443817, 'log_learning_rate_D': -3.7411500962150415, 'log_learning_rate_D_dagger': -1.7106414217122559, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 5} because of the following error: The value nan is not acceptable.
[W 2024-03-12 14:44:02,539] Trial 67 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  37.66179299354553
Memory status after this trial: 
Memory allocated:  4.2509765625
Memory cached:  6.0
--------------------  Trial  68   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.5186081533132114, 'log_learning_rate_D': -3.829115481868729, 'log_learning_rate_D_dagger': -1.7312436254849468, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 5}

	 epoch  0 training error:  tensor(2035241.2500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  10 training error:  tensor(180593.1719, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  20 training error:  tensor(11572.9707, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  30 training error:  tensor(4669.4268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  40 training error:  tensor(5540.6245, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  50 training error:  tensor(3980.6223, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  60 training error:  tensor(3607.6348, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  70 training error:  tensor(3095.1450, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  80 training error:  tensor(6552.9800, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  90 training error:  tensor(2277.3103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
[I 2024-03-12 14:47:39,131] Trial 68 finished with value: 3695.76025390625 and parameters: {'log_learning_rate': -3.5186081533132114, 'log_learning_rate_D': -3.829115481868729, 'log_learning_rate_D_dagger': -1.7312436254849468, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 5}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  216.33296012878418
Memory status after this trial: 
Memory allocated:  4.2509765625
Memory cached:  6.0
--------------------  Trial  69   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.563880261154812, 'log_learning_rate_D': -3.748270103665082, 'log_learning_rate_D_dagger': -1.2818013003273963, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(1292.5394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(7.2288, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.1212, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.7857, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.7390, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.6497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5925, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.6033, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5017, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4943, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
[I 2024-03-12 14:54:59,471] Trial 69 finished with value: 0.4393816888332367 and parameters: {'log_learning_rate': -2.563880261154812, 'log_learning_rate_D': -3.748270103665082, 'log_learning_rate_D_dagger': -1.2818013003273963, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  440.06965732574463
Memory status after this trial: 
Memory allocated:  3.3203125
Memory cached:  6.0
--------------------  Trial  70   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.8147069201286694, 'log_learning_rate_D': -4.070107980191897, 'log_learning_rate_D_dagger': -1.515966221311437, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(3914.7168, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(16.7465, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(3.0438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(2.4283, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.9622, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(1.5173, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(1.1571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.8839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.7204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.6699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
[I 2024-03-12 15:02:19,760] Trial 70 finished with value: 0.4326835572719574 and parameters: {'log_learning_rate': -2.8147069201286694, 'log_learning_rate_D': -4.070107980191897, 'log_learning_rate_D_dagger': -1.515966221311437, 'training_batch_size': 6, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  440.02499747276306
Memory status after this trial: 
Memory allocated:  3.3203125
Memory cached:  6.0
--------------------  Trial  71   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.534922117777683, 'log_learning_rate_D': -3.531492025896563, 'log_learning_rate_D_dagger': -1.21433462082052, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(22504.2812, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  10 training error:  tensor(913.6657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  20 training error:  tensor(45.0601, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  30 training error:  tensor(5.2038, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  40 training error:  tensor(2.2270, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  50 training error:  tensor(1.8571, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  60 training error:  tensor(1.5735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  70 training error:  tensor(1.2928, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  80 training error:  tensor(1.0246, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.7874, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
[I 2024-03-12 15:06:51,755] Trial 71 finished with value: 0.5156042575836182 and parameters: {'log_learning_rate': -2.534922117777683, 'log_learning_rate_D': -3.531492025896563, 'log_learning_rate_D_dagger': -1.21433462082052, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  271.74625635147095
Memory status after this trial: 
Memory allocated:  3.318359375
Memory cached:  6.0
--------------------  Trial  72   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2871045093036044, 'log_learning_rate_D': -3.2828443277829003, 'log_learning_rate_D_dagger': -1.8972105574191107, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(37410.5977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(355.7213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(46.9982, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(32.8203, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(12.0394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(84.2301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(61.0467, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(14.8831, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(29.8478, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(32.2288, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
[I 2024-03-12 15:14:11,481] Trial 72 finished with value: 4.995856761932373 and parameters: {'log_learning_rate': -2.2871045093036044, 'log_learning_rate_D': -3.2828443277829003, 'log_learning_rate_D_dagger': -1.8972105574191107, 'training_batch_size': 6, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  439.46144795417786
Memory status after this trial: 
Memory allocated:  3.3203125
Memory cached:  6.0
--------------------  Trial  73   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.96179120284203, 'log_learning_rate_D': -2.418981074975957, 'log_learning_rate_D_dagger': -1.5709647027605154, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(3263.6118, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  10 training error:  tensor(120.0474, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  20 training error:  tensor(3.3091, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.8942, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.3452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.7963, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.6795, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  70 training error:  tensor(1.0838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  80 training error:  tensor(1.1438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
	 epoch  90 training error:  tensor(1.2059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69091796875
Memory cached:  6.0
[I 2024-03-12 15:18:43,466] Trial 73 finished with value: 0.4580211639404297 and parameters: {'log_learning_rate': -2.96179120284203, 'log_learning_rate_D': -2.418981074975957, 'log_learning_rate_D_dagger': -1.5709647027605154, 'training_batch_size': 7, 'training_p': 6, 'p_order_W': 2, 'p_order_D': 2, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  271.73650217056274
Memory status after this trial: 
Memory allocated:  3.318359375
Memory cached:  6.0
--------------------  Trial  74   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.6325304724585923, 'log_learning_rate_D': -3.001283569914257, 'log_learning_rate_D_dagger': -1.421532652873012, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(114.1204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.7704, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5182, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5047, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4989, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4924, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4882, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4836, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4786, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[I 2024-03-12 15:26:15,676] Trial 74 finished with value: 0.37690088152885437 and parameters: {'log_learning_rate': -2.6325304724585923, 'log_learning_rate_D': -3.001283569914257, 'log_learning_rate_D_dagger': -1.421532652873012, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  451.9411745071411
Memory status after this trial: 
Memory allocated:  3.5693359375
Memory cached:  6.0
--------------------  Trial  75   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -3.1413368862817554, 'log_learning_rate_D': -3.1636250120261846, 'log_learning_rate_D_dagger': -1.7300938910779127, 'training_batch_size': 10, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(36.3129, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  10 training error:  tensor(7.7718, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.9046, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.7352, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.9324, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.6213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.6089, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.5607, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.5522, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05712890625
Memory cached:  8.0
[I 2024-03-12 15:28:01,576] Trial 75 finished with value: 0.47013336420059204 and parameters: {'log_learning_rate': -3.1413368862817554, 'log_learning_rate_D': -3.1636250120261846, 'log_learning_rate_D_dagger': -1.7300938910779127, 'training_batch_size': 10, 'training_p': 6, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  105.6580708026886
Memory status after this trial: 
Memory allocated:  3.4697265625
Memory cached:  6.0
--------------------  Trial  76   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.7969882498221583, 'log_learning_rate_D': -3.644819633960756, 'log_learning_rate_D_dagger': -1.169504332402532, 'training_batch_size': 6, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(0.8682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.5257, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5279, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5257, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5272, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[I 2024-03-12 15:35:02,800] Trial 76 finished with value: 0.4146614074707031 and parameters: {'log_learning_rate': -2.7969882498221583, 'log_learning_rate_D': -3.644819633960756, 'log_learning_rate_D_dagger': -1.169504332402532, 'training_batch_size': 6, 'training_p': 7, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 2}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  420.9555995464325
Memory status after this trial: 
Memory allocated:  3.1787109375
Memory cached:  6.0
--------------------  Trial  77   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1404994309194962, 'log_learning_rate_D': -3.892256911893535, 'log_learning_rate_D_dagger': -2.21207004305381, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 15:35:06,876] Trial 77 failed with parameters: {'log_learning_rate': -2.1404994309194962, 'log_learning_rate_D': -3.892256911893535, 'log_learning_rate_D_dagger': -2.21207004305381, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 15:35:06,877] Trial 77 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.829066753387451
Memory status after this trial: 
Memory allocated:  3.6640625
Memory cached:  6.0
--------------------  Trial  78   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0949138574889234, 'log_learning_rate_D': -3.450105692958054, 'log_learning_rate_D_dagger': -1.9462622525687976, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(50.5594, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.4829, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.8133, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.0405, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.8775, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.6436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.7401, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.7042, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.7885, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
[I 2024-03-12 15:40:05,554] Trial 78 finished with value: 0.5009347796440125 and parameters: {'log_learning_rate': -2.0949138574889234, 'log_learning_rate_D': -3.450105692958054, 'log_learning_rate_D_dagger': -1.9462622525687976, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  298.43035197257996
Memory status after this trial: 
Memory allocated:  3.6640625
Memory cached:  6.0
--------------------  Trial  79   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.328882838570804, 'log_learning_rate_D': -2.703263527999759, 'log_learning_rate_D_dagger': -1.4183294855013284, 'training_batch_size': 6, 'training_p': 8, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(219.8778, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.3364, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5787, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5493, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5433, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5532, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5418, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5411, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5415, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[I 2024-03-12 15:47:39,791] Trial 79 finished with value: 0.4101165235042572 and parameters: {'log_learning_rate': -2.328882838570804, 'log_learning_rate_D': -2.703263527999759, 'log_learning_rate_D_dagger': -1.4183294855013284, 'training_batch_size': 6, 'training_p': 8, 'p_order_W': 3, 'p_order_D': 5, 'p_order_D_dagger': 3}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  453.90501141548157
Memory status after this trial: 
Memory allocated:  3.5693359375
Memory cached:  6.0
--------------------  Trial  80   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.6623281192239423, 'log_learning_rate_D': -2.9640338593898923, 'log_learning_rate_D_dagger': -1.4070436162723345, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(11966.7256, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[W 2024-03-12 15:48:14,748] Trial 80 failed with parameters: {'log_learning_rate': -2.6623281192239423, 'log_learning_rate_D': -2.9640338593898923, 'log_learning_rate_D_dagger': -1.4070436162723345, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 15:48:14,749] Trial 80 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  34.649954319000244
Memory status after this trial: 
Memory allocated:  3.9599609375
Memory cached:  6.0
--------------------  Trial  81   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.592947659153025, 'log_learning_rate_D': -3.037297133241821, 'log_learning_rate_D_dagger': -1.4209255653039072, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(11692.5850, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[W 2024-03-12 15:48:59,303] Trial 81 failed with parameters: {'log_learning_rate': -2.592947659153025, 'log_learning_rate_D': -3.037297133241821, 'log_learning_rate_D_dagger': -1.4209255653039072, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 4} because of the following error: The value nan is not acceptable.
[W 2024-03-12 15:48:59,303] Trial 81 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  44.28207612037659
Memory status after this trial: 
Memory allocated:  3.9599609375
Memory cached:  6.0
--------------------  Trial  82   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.6040929993028086, 'log_learning_rate_D': -3.0024157808579677, 'log_learning_rate_D_dagger': -1.4274022668453017, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 4}

	 epoch  0 training error:  tensor(12287.1045, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  10 training error:  tensor(151.4887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  20 training error:  tensor(29.6366, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  30 training error:  tensor(18.5505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.2144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5810, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5710, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5656, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5618, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.05908203125
Memory cached:  6.0
[I 2024-03-12 15:57:05,973] Trial 82 finished with value: 0.5202650427818298 and parameters: {'log_learning_rate': -2.6040929993028086, 'log_learning_rate_D': -3.0024157808579677, 'log_learning_rate_D_dagger': -1.4274022668453017, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 3, 'p_order_D_dagger': 4}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  486.37790298461914
Memory status after this trial: 
Memory allocated:  3.9599609375
Memory cached:  6.0
--------------------  Trial  83   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.526626342118677, 'log_learning_rate_D': -2.95235109841524, 'log_learning_rate_D_dagger': -1.1401538612454707, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(29.9923, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.6689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5556, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5287, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5014, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4821, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4792, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4742, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4713, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.69287109375
Memory cached:  6.0
[I 2024-03-12 16:03:52,373] Trial 83 finished with value: 0.3643087148666382 and parameters: {'log_learning_rate': -2.526626342118677, 'log_learning_rate_D': -2.95235109841524, 'log_learning_rate_D_dagger': -1.1401538612454707, 'training_batch_size': 6, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 63 with value: 0.36037781834602356.
Time for this trial:  406.1394159793854
Memory status after this trial: 
Memory allocated:  2.9296875
Memory cached:  6.0
--------------------  Trial  84   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3978271425471243, 'log_learning_rate_D': -3.2001578587703277, 'log_learning_rate_D_dagger': -1.113889031133318, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(5.4423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.9292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5627, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4704, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4306, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4328, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4327, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4342, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.95947265625
Memory cached:  6.0
[I 2024-03-12 16:08:32,015] Trial 84 finished with value: 0.34929484128952026 and parameters: {'log_learning_rate': -2.3978271425471243, 'log_learning_rate_D': -3.2001578587703277, 'log_learning_rate_D_dagger': -1.113889031133318, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
res:  tensor(0.3493, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3604, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  279.38091802597046
Memory status after this trial: 
Memory allocated:  1.81201171875
Memory cached:  6.0
--------------------  Trial  85   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3458855003810553, 'log_learning_rate_D': -2.8400458888791755, 'log_learning_rate_D_dagger': -1.1358811324227653, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(292.1268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  10 training error:  tensor(5.3860, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[W 2024-03-12 16:09:30,240] Trial 85 failed with parameters: {'log_learning_rate': -2.3458855003810553, 'log_learning_rate_D': -2.8400458888791755, 'log_learning_rate_D_dagger': -1.1358811324227653, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 16:09:30,241] Trial 85 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  57.97369384765625
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  86   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.396827133115109, 'log_learning_rate_D': -2.7481803836177976, 'log_learning_rate_D_dagger': -1.1558358933995678, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(109.8576, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.9398, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4870, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4761, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4591, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4551, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4539, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-12 16:14:08,607] Trial 86 finished with value: 0.39612680673599243 and parameters: {'log_learning_rate': -2.396827133115109, 'log_learning_rate_D': -2.7481803836177976, 'log_learning_rate_D_dagger': -1.1558358933995678, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  278.10650086402893
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  87   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2043746340152532, 'log_learning_rate_D': -3.147276337593964, 'log_learning_rate_D_dagger': -1.1342790376447034, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(11.8088, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.6313, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5001, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4898, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4778, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4727, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4730, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.84228515625
Memory cached:  6.0
[I 2024-03-12 16:18:46,252] Trial 87 finished with value: 0.35275799036026 and parameters: {'log_learning_rate': -2.2043746340152532, 'log_learning_rate_D': -3.147276337593964, 'log_learning_rate_D_dagger': -1.1342790376447034, 'training_batch_size': 7, 'training_p': 5, 'p_order_W': 3, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  277.3607699871063
Memory status after this trial: 
Memory allocated:  3.3525390625
Memory cached:  6.0
--------------------  Trial  88   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9064508701084604, 'log_learning_rate_D': -3.1410266728269995, 'log_learning_rate_D_dagger': -1.0740872180184573, 'training_batch_size': 8, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(149.8296, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(5.2816, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.3164, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5998, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4829, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4725, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4699, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4704, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:21:40,072] Trial 88 finished with value: 0.3595300018787384 and parameters: {'log_learning_rate': -1.9064508701084604, 'log_learning_rate_D': -3.1410266728269995, 'log_learning_rate_D_dagger': -1.0740872180184573, 'training_batch_size': 8, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  173.57150864601135
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  89   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9143234544841468, 'log_learning_rate_D': -3.8931743076390797, 'log_learning_rate_D_dagger': -1.106587373901775, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(176.1926, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(7.1877, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.5682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6360, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5397, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5050, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4855, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4725, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4681, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:24:33,759] Trial 89 finished with value: 0.40566569566726685 and parameters: {'log_learning_rate': -1.9143234544841468, 'log_learning_rate_D': -3.8931743076390797, 'log_learning_rate_D_dagger': -1.106587373901775, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  173.44997549057007
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  90   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7567483681295264, 'log_learning_rate_D': -3.4022327481617314, 'log_learning_rate_D_dagger': -1.0112733941306888, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(300.2253, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(27.5498, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(3.3069, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.7396, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5264, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4853, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4751, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4642, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4546, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4462, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:27:27,768] Trial 90 finished with value: 0.37512627243995667 and parameters: {'log_learning_rate': -1.7567483681295264, 'log_learning_rate_D': -3.4022327481617314, 'log_learning_rate_D_dagger': -1.0112733941306888, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  173.77061867713928
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  91   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.154449977204581, 'log_learning_rate_D': -3.155498685234896, 'log_learning_rate_D_dagger': -1.1718438816885257, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(26.4643, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.1452, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.6096, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6764, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5929, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5193, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4864, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4773, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4741, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
[I 2024-03-12 16:29:04,181] Trial 91 finished with value: 0.36823901534080505 and parameters: {'log_learning_rate': -2.154449977204581, 'log_learning_rate_D': -3.155498685234896, 'log_learning_rate_D_dagger': -1.1718438816885257, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  96.18910622596741
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  92   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1259565580531468, 'log_learning_rate_D': -3.632863506512289, 'log_learning_rate_D_dagger': -1.144636852515045, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(351.3933, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  10 training error:  tensor(2.0173, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  20 training error:  tensor(22.0092, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  30 training error:  tensor(9.1351, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  40 training error:  tensor(3.3012, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.9838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.7198, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5990, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5430, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
[I 2024-03-12 16:30:40,790] Trial 92 finished with value: 0.4560225009918213 and parameters: {'log_learning_rate': -2.1259565580531468, 'log_learning_rate_D': -3.632863506512289, 'log_learning_rate_D_dagger': -1.144636852515045, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  96.38212990760803
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  93   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.035298824034181, 'log_learning_rate_D': -3.199740427568885, 'log_learning_rate_D_dagger': -1.0193241197474037, 'training_batch_size': 8, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(69.1706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.9670, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5266, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5002, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4721, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4685, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:33:33,907] Trial 93 finished with value: 0.3594290316104889 and parameters: {'log_learning_rate': -2.035298824034181, 'log_learning_rate_D': -3.199740427568885, 'log_learning_rate_D_dagger': -1.0193241197474037, 'training_batch_size': 8, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  172.88169598579407
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  94   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.1749097316686696, 'log_learning_rate_D': -3.31428768165074, 'log_learning_rate_D_dagger': -1.1204280422920367, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(253.4456, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  10 training error:  tensor(24.8281, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  20 training error:  tensor(15.4951, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  30 training error:  tensor(4.7986, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  40 training error:  tensor(1.2830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.7415, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.6744, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5344, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5163, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4992, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.67138671875
Memory cached:  6.0
[I 2024-03-12 16:35:10,695] Trial 94 finished with value: 0.39721736311912537 and parameters: {'log_learning_rate': -2.1749097316686696, 'log_learning_rate_D': -3.31428768165074, 'log_learning_rate_D_dagger': -1.1204280422920367, 'training_batch_size': 9, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  96.54734516143799
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  95   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.992839233695512, 'log_learning_rate_D': -3.1487067079787834, 'log_learning_rate_D_dagger': -1.001236950842107, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(80.8022, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(5.6706, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.8637, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5032, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4908, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4725, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4625, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4548, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4506, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:38:03,539] Trial 95 finished with value: 0.38997602462768555 and parameters: {'log_learning_rate': -1.992839233695512, 'log_learning_rate_D': -3.1487067079787834, 'log_learning_rate_D_dagger': -1.001236950842107, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  172.58214807510376
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  96   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.302880942962819, 'log_learning_rate_D': -3.181222488288442, 'log_learning_rate_D_dagger': -1.420634309953756, 'training_batch_size': 8, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(3.2041, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.5128, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4945, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4867, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4705, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4694, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4694, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4721, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:40:57,873] Trial 96 finished with value: 0.360015332698822 and parameters: {'log_learning_rate': -2.302880942962819, 'log_learning_rate_D': -3.181222488288442, 'log_learning_rate_D_dagger': -1.420634309953756, 'training_batch_size': 8, 'training_p': 5, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 84 with value: 0.34929484128952026.
Time for this trial:  174.0854549407959
Memory status after this trial: 
Memory allocated:  2.810546875
Memory cached:  6.0
--------------------  Trial  97   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3688675901409155, 'log_learning_rate_D': -3.50376837879741, 'log_learning_rate_D_dagger': -1.412866327555049, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(126.3072, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  10 training error:  tensor(7.9093, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.5762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4906, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4632, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4471, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4388, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4338, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4315, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4304, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.57373046875
Memory cached:  6.0
[I 2024-03-12 16:43:52,240] Trial 97 finished with value: 0.34043747186660767 and parameters: {'log_learning_rate': -2.3688675901409155, 'log_learning_rate_D': -3.50376837879741, 'log_learning_rate_D_dagger': -1.412866327555049, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 97 with value: 0.34043747186660767.
res:  tensor(0.3404, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3493, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  174.12443566322327
Memory status after this trial: 
Memory allocated:  1.27001953125
Memory cached:  4.0
--------------------  Trial  98   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3122876648930673, 'log_learning_rate_D': -3.270813209667066, 'log_learning_rate_D_dagger': -1.549090529771769, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(111.1416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  10 training error:  tensor(3.7729, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.3903, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.8349, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.6075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5147, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4806, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4493, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4385, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4336, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
[I 2024-03-12 16:46:43,675] Trial 98 finished with value: 0.3268338143825531 and parameters: {'log_learning_rate': -2.3122876648930673, 'log_learning_rate_D': -3.270813209667066, 'log_learning_rate_D_dagger': -1.549090529771769, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
res:  tensor(0.3268, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3404, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  171.218514919281
Memory status after this trial: 
Memory allocated:  1.27001953125
Memory cached:  4.0
--------------------  Trial  99   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7084793705951522, 'log_learning_rate_D': -3.2699334015660715, 'log_learning_rate_D_dagger': -1.3662753440092357, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(1.0776, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4753, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4596, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
[I 2024-03-12 16:49:23,817] Trial 99 finished with value: 0.39830026030540466 and parameters: {'log_learning_rate': -1.7084793705951522, 'log_learning_rate_D': -3.2699334015660715, 'log_learning_rate_D_dagger': -1.3662753440092357, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 2}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  159.91711711883545
Memory status after this trial: 
Memory allocated:  2.146484375
Memory cached:  4.0
--------------------  Trial  100   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3042084681923356, 'log_learning_rate_D': -3.476853135089991, 'log_learning_rate_D_dagger': -1.5091855298879535, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(217.0004, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.6695, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.4418, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4836, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4498, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4329, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4316, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4312, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4311, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
[I 2024-03-12 16:52:16,098] Trial 100 finished with value: 0.3538583517074585 and parameters: {'log_learning_rate': -2.3042084681923356, 'log_learning_rate_D': -3.476853135089991, 'log_learning_rate_D_dagger': -1.5091855298879535, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  172.0416133403778
Memory status after this trial: 
Memory allocated:  2.537109375
Memory cached:  4.0
--------------------  Trial  101   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8869649213637207, 'log_learning_rate_D': -3.4722949913843646, 'log_learning_rate_D_dagger': -1.4955352054518856, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(29.2974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.5242, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.7344, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5296, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4952, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4723, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4537, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4316, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4315, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
[I 2024-03-12 16:55:54,276] Trial 101 finished with value: 0.34367749094963074 and parameters: {'log_learning_rate': -1.8869649213637207, 'log_learning_rate_D': -3.4722949913843646, 'log_learning_rate_D_dagger': -1.4955352054518856, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  217.91995072364807
Memory status after this trial: 
Memory allocated:  3.4677734375
Memory cached:  4.0
--------------------  Trial  102   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.029749768996494, 'log_learning_rate_D': -3.4323573324066685, 'log_learning_rate_D_dagger': -1.4974159390871338, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(78.1847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.9152, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.4132, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5240, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4421, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4383, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4335, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4275, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4215, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4156, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
[I 2024-03-12 16:59:31,893] Trial 102 finished with value: 0.37992164492607117 and parameters: {'log_learning_rate': -2.029749768996494, 'log_learning_rate_D': -3.4323573324066685, 'log_learning_rate_D_dagger': -1.4974159390871338, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  217.3412048816681
Memory status after this trial: 
Memory allocated:  3.4677734375
Memory cached:  4.0
--------------------  Trial  103   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8415523124519577, 'log_learning_rate_D': -3.5069667056544453, 'log_learning_rate_D_dagger': -1.3534011124849137, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(402.9144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  10 training error:  tensor(5.0373, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
	 epoch  20 training error:  tensor(4.3131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  30 training error:  tensor(1.3638, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.9770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.7682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.6345, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5511, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5145, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.5057, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
[I 2024-03-12 17:03:09,996] Trial 103 finished with value: 0.45954981446266174 and parameters: {'log_learning_rate': -1.8415523124519577, 'log_learning_rate_D': -3.5069667056544453, 'log_learning_rate_D_dagger': -1.3534011124849137, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  217.8440501689911
Memory status after this trial: 
Memory allocated:  3.4677734375
Memory cached:  4.0
--------------------  Trial  104   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.420996532796362, 'log_learning_rate_D': -3.3764886893808024, 'log_learning_rate_D_dagger': -1.503727250359543, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(333.6473, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  10 training error:  tensor(18.7870, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
	 epoch  20 training error:  tensor(5.9352, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.7927, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.7151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.5615, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.5251, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.5130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.5048, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4988, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  6.0
[I 2024-03-12 17:06:48,208] Trial 104 finished with value: 0.4522092044353485 and parameters: {'log_learning_rate': -2.420996532796362, 'log_learning_rate_D': -3.3764886893808024, 'log_learning_rate_D_dagger': -1.503727250359543, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  217.93915057182312
Memory status after this trial: 
Memory allocated:  3.4677734375
Memory cached:  4.0
--------------------  Trial  105   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6191079133700692, 'log_learning_rate_D': -3.553409303641471, 'log_learning_rate_D_dagger': -1.2397546780745579, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(518.9807, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  10 training error:  tensor(11.8977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  20 training error:  tensor(1.2304, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.5471, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.5058, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4766, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4672, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4584, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4504, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.30029296875
Memory cached:  4.0
[I 2024-03-12 17:10:57,828] Trial 105 finished with value: 0.38375863432884216 and parameters: {'log_learning_rate': -1.6191079133700692, 'log_learning_rate_D': -3.553409303641471, 'log_learning_rate_D_dagger': -1.2397546780745579, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 2, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 98 with value: 0.3268338143825531.
Time for this trial:  249.35767936706543
Memory status after this trial: 
Memory allocated:  2.537109375
Memory cached:  4.0
--------------------  Trial  106   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.258762232930978, 'log_learning_rate_D': -3.656407353844863, 'log_learning_rate_D_dagger': -1.0098042003823973, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 17:11:00,892] Trial 106 failed with parameters: {'log_learning_rate': -2.258762232930978, 'log_learning_rate_D': -3.656407353844863, 'log_learning_rate_D_dagger': -1.0098042003823973, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:11:00,893] Trial 106 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.830822229385376
Memory status after this trial: 
Memory allocated:  3.2734375
Memory cached:  4.0
--------------------  Trial  107   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2353730292316314, 'log_learning_rate_D': -3.668745503909372, 'log_learning_rate_D_dagger': -1.0195073617468213, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 17:11:04,047] Trial 107 failed with parameters: {'log_learning_rate': -2.2353730292316314, 'log_learning_rate_D': -3.668745503909372, 'log_learning_rate_D_dagger': -1.0195073617468213, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:11:04,048] Trial 107 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.908267021179199
Memory status after this trial: 
Memory allocated:  3.2734375
Memory cached:  4.0
--------------------  Trial  108   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.216241931148286, 'log_learning_rate_D': -3.669361962098125, 'log_learning_rate_D_dagger': -1.0248670401984894, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 17:11:07,057] Trial 108 failed with parameters: {'log_learning_rate': -2.216241931148286, 'log_learning_rate_D': -3.669361962098125, 'log_learning_rate_D_dagger': -1.0248670401984894, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:11:07,058] Trial 108 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.778545618057251
Memory status after this trial: 
Memory allocated:  3.2734375
Memory cached:  4.0
--------------------  Trial  109   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9224182831219174, 'log_learning_rate_D': -3.6534245853486613, 'log_learning_rate_D_dagger': -1.0608193336107536, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(33.3148, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.8193, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4444, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4072, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.3961, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.3886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.3841, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.3800, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.3769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  1.56884765625
Memory cached:  4.0
[I 2024-03-12 17:14:30,865] Trial 109 finished with value: 0.3218425512313843 and parameters: {'log_learning_rate': -1.9224182831219174, 'log_learning_rate_D': -3.6534245853486613, 'log_learning_rate_D_dagger': -1.0608193336107536, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
res:  tensor(0.3218, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3268, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  203.5760519504547
Memory status after this trial: 
Memory allocated:  2.00634765625
Memory cached:  4.0
--------------------  Trial  110   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.23077723794615, 'log_learning_rate_D': -3.6956009214593672, 'log_learning_rate_D_dagger': -1.3105841869150683, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 17:14:33,963] Trial 110 failed with parameters: {'log_learning_rate': -2.23077723794615, 'log_learning_rate_D': -3.6956009214593672, 'log_learning_rate_D_dagger': -1.3105841869150683, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:14:33,963] Trial 110 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8731577396392822
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  4.0
--------------------  Trial  111   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.175186603318816, 'log_learning_rate_D': -3.7231730588922605, 'log_learning_rate_D_dagger': -1.592412850997118, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 17:14:37,035] Trial 111 failed with parameters: {'log_learning_rate': -2.175186603318816, 'log_learning_rate_D': -3.7231730588922605, 'log_learning_rate_D_dagger': -1.592412850997118, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:14:37,036] Trial 111 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.828707218170166
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  4.0
--------------------  Trial  112   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2083086627697184, 'log_learning_rate_D': -3.6891457360048934, 'log_learning_rate_D_dagger': -1.7976675861309988, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(96.6956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(15.2291, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(3.2756, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.6932, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.9625, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.6324, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5704, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5819, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.6610, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:18:02,082] Trial 112 finished with value: 0.4793102443218231 and parameters: {'log_learning_rate': -2.2083086627697184, 'log_learning_rate_D': -3.6891457360048934, 'log_learning_rate_D_dagger': -1.7976675861309988, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  204.79595112800598
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  113   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.0446783639240502, 'log_learning_rate_D': -3.7748408428090765, 'log_learning_rate_D_dagger': -1.5777578674849315, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(73.0934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
[W 2024-03-12 17:18:05,320] Trial 113 failed with parameters: {'log_learning_rate': -2.0446783639240502, 'log_learning_rate_D': -3.7748408428090765, 'log_learning_rate_D_dagger': -1.5777578674849315, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:18:05,320] Trial 113 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.005619525909424
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  8.0
--------------------  Trial  114   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.061239013757473, 'log_learning_rate_D': -3.587790134701721, 'log_learning_rate_D_dagger': -1.3303441685108073, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(188.0656, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  10 training error:  tensor(22.3538, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  20 training error:  tensor(5.7585, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.4983, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.8780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5042, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.6142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4149, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4296, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
[I 2024-03-12 17:19:56,891] Trial 114 finished with value: 0.3764594495296478 and parameters: {'log_learning_rate': -2.061239013757473, 'log_learning_rate_D': -3.587790134701721, 'log_learning_rate_D_dagger': -1.3303441685108073, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  111.31806325912476
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  8.0
--------------------  Trial  115   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.280814297584786, 'log_learning_rate_D': -3.7953741004691475, 'log_learning_rate_D_dagger': -1.6330610756331065, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(1.2624, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(0.4801, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.4585, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  30 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  40 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  70 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  90 training error:  tensor(0.4569, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
[I 2024-03-12 17:24:55,245] Trial 115 finished with value: 0.39755532145500183 and parameters: {'log_learning_rate': -2.280814297584786, 'log_learning_rate_D': -3.7953741004691475, 'log_learning_rate_D_dagger': -1.6330610756331065, 'training_batch_size': 7, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 2}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  298.1014130115509
Memory status after this trial: 
Memory allocated:  3.544921875
Memory cached:  4.0
--------------------  Trial  116   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4706549800713449, 'log_learning_rate_D': -3.3093482754911836, 'log_learning_rate_D_dagger': -1.5427256916719905, 'training_batch_size': 7, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(140.0181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.6561, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  20 training error:  tensor(0.5567, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4671, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  50 training error:  tensor(0.4403, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  60 training error:  tensor(0.4309, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4236, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  80 training error:  tensor(0.4192, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4166, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:29:53,607] Trial 116 finished with value: 0.38227954506874084 and parameters: {'log_learning_rate': -1.4706549800713449, 'log_learning_rate_D': -3.3093482754911836, 'log_learning_rate_D_dagger': -1.5427256916719905, 'training_batch_size': 7, 'training_p': 3, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  298.1033902168274
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  4.0
--------------------  Trial  117   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.3951313495596342, 'log_learning_rate_D': -3.6845407362781963, 'log_learning_rate_D_dagger': -1.2291950279513502, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(94.2869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.6055, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.9975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6557, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4603, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4319, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4294, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:33:32,592] Trial 117 finished with value: 0.3469022810459137 and parameters: {'log_learning_rate': -2.3951313495596342, 'log_learning_rate_D': -3.6845407362781963, 'log_learning_rate_D_dagger': -1.2291950279513502, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  218.70555520057678
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  118   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.4043303208634152, 'log_learning_rate_D': -3.6992717436437634, 'log_learning_rate_D_dagger': -1.2684467006122684, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 17:33:35,701] Trial 118 failed with parameters: {'log_learning_rate': -2.4043303208634152, 'log_learning_rate_D': -3.6992717436437634, 'log_learning_rate_D_dagger': -1.2684467006122684, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:33:35,702] Trial 118 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8675777912139893
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  4.0
--------------------  Trial  119   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.38541660502602, 'log_learning_rate_D': -3.974513843680898, 'log_learning_rate_D_dagger': -1.223852894285634, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(46.6395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.9308, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4890, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4884, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4642, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4573, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.5024, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4685, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:37:13,418] Trial 119 finished with value: 0.39805102348327637 and parameters: {'log_learning_rate': -2.38541660502602, 'log_learning_rate_D': -3.974513843680898, 'log_learning_rate_D_dagger': -1.223852894285634, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  217.44063258171082
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  120   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9917288938980748, 'log_learning_rate_D': -3.466800216657962, 'log_learning_rate_D_dagger': -1.3341471171198291, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(6.5005, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.2000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5739, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4858, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4698, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4585, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4467, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4337, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4331, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:40:51,457] Trial 120 finished with value: 0.3489050567150116 and parameters: {'log_learning_rate': -1.9917288938980748, 'log_learning_rate_D': -3.466800216657962, 'log_learning_rate_D_dagger': -1.3341471171198291, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  217.7707760334015
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  121   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.2280984614208497, 'log_learning_rate_D': -3.6914845535498326, 'log_learning_rate_D_dagger': -1.346023051839851, 'training_batch_size': 9, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(31.8428, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  10 training error:  tensor(7.7131, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.7394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.8151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.5617, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4933, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4772, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4717, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
[I 2024-03-12 17:42:50,062] Trial 121 finished with value: 0.41461801528930664 and parameters: {'log_learning_rate': -2.2280984614208497, 'log_learning_rate_D': -3.6914845535498326, 'log_learning_rate_D_dagger': -1.346023051839851, 'training_batch_size': 9, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  118.3636908531189
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  122   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7879033409179985, 'log_learning_rate_D': -3.460487809775854, 'log_learning_rate_D_dagger': -1.4738649106496062, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(271.6436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(11.3663, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(2.2056, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.8059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.6636, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.6126, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.5767, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5521, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.5341, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.5205, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:46:28,176] Trial 122 finished with value: 0.48727813363075256 and parameters: {'log_learning_rate': -1.7879033409179985, 'log_learning_rate_D': -3.460487809775854, 'log_learning_rate_D_dagger': -1.4738649106496062, 'training_batch_size': 8, 'training_p': 4, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  217.846049785614
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  123   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9587661168526656, 'log_learning_rate_D': -3.5315649905647053, 'log_learning_rate_D_dagger': -1.7039863404256823, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(51.4103, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(6.7144, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.2078, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.8329, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4816, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3839, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3515, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3417, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3327, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:50:05,321] Trial 123 finished with value: 0.34334173798561096 and parameters: {'log_learning_rate': -1.9587661168526656, 'log_learning_rate_D': -3.5315649905647053, 'log_learning_rate_D_dagger': -1.7039863404256823, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  216.9026596546173
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  124   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9228129943155334, 'log_learning_rate_D': -3.7938963433827237, 'log_learning_rate_D_dagger': -1.695133862163071, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(286.9130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(32.0135, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[W 2024-03-12 17:51:03,286] Trial 124 failed with parameters: {'log_learning_rate': -1.9228129943155334, 'log_learning_rate_D': -3.7938963433827237, 'log_learning_rate_D_dagger': -1.695133862163071, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 17:51:03,287] Trial 124 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  57.70282483100891
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  125   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.934536930751909, 'log_learning_rate_D': -3.8002601948975885, 'log_learning_rate_D_dagger': -1.6901123388959516, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(29.8505, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(1.0662, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4471, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4299, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4219, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4129, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3970, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3770, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3655, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3593, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 17:56:21,759] Trial 125 finished with value: 0.3623882830142975 and parameters: {'log_learning_rate': -1.934536930751909, 'log_learning_rate_D': -3.8002601948975885, 'log_learning_rate_D_dagger': -1.6901123388959516, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  318.1824321746826
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  126   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.8154301278679787, 'log_learning_rate_D': -3.619523391066708, 'log_learning_rate_D_dagger': -1.2679530883758, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(252.9869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(11.1632, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.7500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6644, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4396, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4229, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4160, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4119, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4093, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4070, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 18:00:00,298] Trial 126 finished with value: 0.3720444142818451 and parameters: {'log_learning_rate': -1.8154301278679787, 'log_learning_rate_D': -3.619523391066708, 'log_learning_rate_D_dagger': -1.2679530883758, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  218.2296712398529
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  127   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.116773061085954, 'log_learning_rate_D': -3.5508353783746287, 'log_learning_rate_D_dagger': -1.82173198938058, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(163.7029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  10 training error:  tensor(86.2811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  20 training error:  tensor(31.9655, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  30 training error:  tensor(4.8760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  40 training error:  tensor(8.3292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  50 training error:  tensor(2.8716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  60 training error:  tensor(2.0359, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  70 training error:  tensor(1.7993, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  80 training error:  tensor(1.6599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
	 epoch  90 training error:  tensor(1.5258, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.13427734375
Memory cached:  8.0
[I 2024-03-12 18:01:58,528] Trial 127 finished with value: 1.4920578002929688 and parameters: {'log_learning_rate': -2.116773061085954, 'log_learning_rate_D': -3.5508353783746287, 'log_learning_rate_D_dagger': -1.82173198938058, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  117.9782440662384
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  128   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.4540715748735726, 'log_learning_rate_D': -3.7304118840834586, 'log_learning_rate_D_dagger': -1.5982572042797856, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(128.0542, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
[W 2024-03-12 18:02:24,821] Trial 128 failed with parameters: {'log_learning_rate': -2.4540715748735726, 'log_learning_rate_D': -3.7304118840834586, 'log_learning_rate_D_dagger': -1.5982572042797856, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 18:02:24,822] Trial 128 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  26.022417068481445
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  129   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.9821113718580452, 'log_learning_rate_D': -3.357196886622028, 'log_learning_rate_D_dagger': -1.6300677378818311, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(181.0537, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.5609, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4967, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4431, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4042, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3950, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3873, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 18:07:43,128] Trial 129 finished with value: 0.3854953944683075 and parameters: {'log_learning_rate': -1.9821113718580452, 'log_learning_rate_D': -3.357196886622028, 'log_learning_rate_D_dagger': -1.6300677378818311, 'training_batch_size': 7, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  318.04216027259827
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  130   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -2.4765126004579963, 'log_learning_rate_D': -3.691731770142145, 'log_learning_rate_D_dagger': -1.2170233571873725, 'training_batch_size': 7, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 5, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(171.3760, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  4.0
	 epoch  10 training error:  tensor(2.1993, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.2297, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5680, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(1.0151, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4758, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4604, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4456, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 18:13:01,422] Trial 130 finished with value: 0.41704875230789185 and parameters: {'log_learning_rate': -2.4765126004579963, 'log_learning_rate_D': -3.691731770142145, 'log_learning_rate_D_dagger': -1.2170233571873725, 'training_batch_size': 7, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 5, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  318.01020646095276
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  131   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6893865997851294, 'log_learning_rate_D': -3.0713724280612995, 'log_learning_rate_D_dagger': -1.0932110914291706, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(103.7653, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[W 2024-03-12 18:13:26,406] Trial 131 failed with parameters: {'log_learning_rate': -1.6893865997851294, 'log_learning_rate_D': -3.0713724280612995, 'log_learning_rate_D_dagger': -1.0932110914291706, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 18:13:26,407] Trial 131 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  24.72488498687744
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  132   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6403132480647657, 'log_learning_rate_D': -3.0672567496585703, 'log_learning_rate_D_dagger': -1.3803685143911968, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(31.2599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.8165, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.8492, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5628, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5037, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4630, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4333, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4100, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3907, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 18:17:04,360] Trial 132 finished with value: 0.3367488980293274 and parameters: {'log_learning_rate': -1.6403132480647657, 'log_learning_rate_D': -3.0672567496585703, 'log_learning_rate_D_dagger': -1.3803685143911968, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  217.69077968597412
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  133   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6715043210363691, 'log_learning_rate_D': -3.081550335288208, 'log_learning_rate_D_dagger': -1.3554320114676437, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(77.8244, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(4.8690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.2022, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4797, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3865, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3648, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3328, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3399, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 18:20:42,706] Trial 133 finished with value: 0.3373829424381256 and parameters: {'log_learning_rate': -1.6715043210363691, 'log_learning_rate_D': -3.081550335288208, 'log_learning_rate_D_dagger': -1.3554320114676437, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 109 with value: 0.3218425512313843.
Time for this trial:  218.0600094795227
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  134   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6994389681838937, 'log_learning_rate_D': -3.2864304254207912, 'log_learning_rate_D_dagger': -1.3617142492290282, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(26.8427, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.7758, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4794, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3589, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3400, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3230, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3032, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.2955, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.2965, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.2938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 18:24:21,097] Trial 134 finished with value: 0.3144383430480957 and parameters: {'log_learning_rate': -1.6994389681838937, 'log_learning_rate_D': -3.2864304254207912, 'log_learning_rate_D_dagger': -1.3617142492290282, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
res:  tensor(0.3144, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3218, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  218.13197374343872
Memory status after this trial: 
Memory allocated:  2.20068359375
Memory cached:  6.0
--------------------  Trial  135   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6315754377039284, 'log_learning_rate_D': -3.289834165638223, 'log_learning_rate_D_dagger': -1.40723320093869, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(14.8762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.7114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5476, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4394, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.4062, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.3917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3822, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3751, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3696, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3662, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:27:59,216] Trial 135 finished with value: 0.37973496317863464 and parameters: {'log_learning_rate': -1.6315754377039284, 'log_learning_rate_D': -3.289834165638223, 'log_learning_rate_D_dagger': -1.40723320093869, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  217.86041927337646
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  136   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4990854611845594, 'log_learning_rate_D': -3.0732320618936475, 'log_learning_rate_D_dagger': -1.3591172974238361, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(93.7020, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(4.4391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.8907, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4455, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.4260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.4157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3761, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3558, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:31:33,298] Trial 136 finished with value: 0.3499160408973694 and parameters: {'log_learning_rate': -1.4990854611845594, 'log_learning_rate_D': -3.0732320618936475, 'log_learning_rate_D_dagger': -1.3591172974238361, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  213.82281136512756
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  137   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7582419330056955, 'log_learning_rate_D': -3.4794361223361827, 'log_learning_rate_D_dagger': -1.6076261433228163, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(74.2115, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(7.7302, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(3.0931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.3099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.7875, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.5536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4188, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3626, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3322, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:35:11,884] Trial 137 finished with value: 0.34204304218292236 and parameters: {'log_learning_rate': -1.7582419330056955, 'log_learning_rate_D': -3.4794361223361827, 'log_learning_rate_D_dagger': -1.6076261433228163, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  218.34826707839966
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  138   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7338240185118976, 'log_learning_rate_D': -3.8694345840676614, 'log_learning_rate_D_dagger': -1.597907975285331, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(95.5684, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  10 training error:  tensor(6.2824, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  10.0
	 epoch  20 training error:  tensor(10.4083, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  30 training error:  tensor(5.0059, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  40 training error:  tensor(2.2904, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  50 training error:  tensor(1.1685, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  10.0
	 epoch  60 training error:  tensor(0.8010, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.5887, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4391, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
[I 2024-03-12 18:37:10,165] Trial 138 finished with value: 0.3882600665092468 and parameters: {'log_learning_rate': -1.7338240185118976, 'log_learning_rate_D': -3.8694345840676614, 'log_learning_rate_D_dagger': -1.597907975285331, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  118.04397630691528
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  139   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6594004868406587, 'log_learning_rate_D': -3.38483836321187, 'log_learning_rate_D_dagger': -1.6985848401132013, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(190.7486, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(34.7939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(8.4662, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.3320, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(1.2492, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.8719, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.6926, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.5570, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4437, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3641, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:40:48,086] Trial 139 finished with value: 0.36416172981262207 and parameters: {'log_learning_rate': -1.6594004868406587, 'log_learning_rate_D': -3.38483836321187, 'log_learning_rate_D_dagger': -1.6985848401132013, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  217.65032172203064
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  140   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3260734312779638, 'log_learning_rate_D': -2.9260841593842146, 'log_learning_rate_D_dagger': -1.5232174261723561, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(10.1523, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.2926, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5916, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4006, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3781, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.3475, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3275, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3260, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3324, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3214, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:44:26,417] Trial 140 finished with value: 0.3420712947845459 and parameters: {'log_learning_rate': -1.3260734312779638, 'log_learning_rate_D': -2.9260841593842146, 'log_learning_rate_D_dagger': -1.5232174261723561, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  218.06458139419556
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  141   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.279933571834894, 'log_learning_rate_D': -2.942813541437791, 'log_learning_rate_D_dagger': -1.8575044741231146, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(79.5388, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 18:44:31,886] Trial 141 failed with parameters: {'log_learning_rate': -1.279933571834894, 'log_learning_rate_D': -2.942813541437791, 'log_learning_rate_D_dagger': -1.8575044741231146, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 18:44:31,886] Trial 141 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  5.1927080154418945
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  142   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.335597030365341, 'log_learning_rate_D': -3.065562270370533, 'log_learning_rate_D_dagger': -1.7792530552437764, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(7.1301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.6308, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5486, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4701, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.4354, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.4072, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3830, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3618, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.3430, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3277, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:48:10,056] Trial 142 finished with value: 0.3360769748687744 and parameters: {'log_learning_rate': -1.335597030365341, 'log_learning_rate_D': -3.065562270370533, 'log_learning_rate_D_dagger': -1.7792530552437764, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  217.89963936805725
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  143   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2617139417570298, 'log_learning_rate_D': -2.840426057144616, 'log_learning_rate_D_dagger': -1.8062624266194403, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 5, 'p_order_D_dagger': 2}

	 epoch  0 training error:  tensor(4.1757, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.6352, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.0652, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6969, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4988, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3975, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3802, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3801, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3800, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:51:35,313] Trial 143 finished with value: 0.390499085187912 and parameters: {'log_learning_rate': -1.2617139417570298, 'log_learning_rate_D': -2.840426057144616, 'log_learning_rate_D_dagger': -1.8062624266194403, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 5, 'p_order_D_dagger': 2}. Best is trial 134 with value: 0.3144383430480957.
Time for this trial:  204.99419021606445
Memory status after this trial: 
Memory allocated:  3.7392578125
Memory cached:  6.0
--------------------  Trial  144   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4875606261157333, 'log_learning_rate_D': -3.052707051239465, 'log_learning_rate_D_dagger': -1.5904252019152312, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(224.9921, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(35.3315, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 18:52:15,468] Trial 144 failed with parameters: {'log_learning_rate': -1.4875606261157333, 'log_learning_rate_D': -3.052707051239465, 'log_learning_rate_D_dagger': -1.5904252019152312, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 18:52:15,468] Trial 144 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  39.91417169570923
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  8.0
--------------------  Trial  145   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.529206775317216, 'log_learning_rate_D': -2.927696162099371, 'log_learning_rate_D_dagger': -1.5468332901504658, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(25.7082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 18:52:25,341] Trial 145 failed with parameters: {'log_learning_rate': -1.529206775317216, 'log_learning_rate_D': -2.927696162099371, 'log_learning_rate_D_dagger': -1.5468332901504658, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 18:52:25,341] Trial 145 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  9.619088411331177
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  146   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7210833877934546, 'log_learning_rate_D': -2.9137344863697288, 'log_learning_rate_D_dagger': -1.5455976805138925, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(65.7233, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 18:52:43,691] Trial 146 failed with parameters: {'log_learning_rate': -1.7210833877934546, 'log_learning_rate_D': -2.9137344863697288, 'log_learning_rate_D_dagger': -1.5455976805138925, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 18:52:43,691] Trial 146 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  18.090694665908813
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  8.0
--------------------  Trial  147   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5683340164313546, 'log_learning_rate_D': -2.9317851554434573, 'log_learning_rate_D_dagger': -1.5814017967817204, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(6.7266, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.6367, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4108, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3734, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3416, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.3190, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3039, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3317, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.2945, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.2951, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 18:56:21,083] Trial 147 finished with value: 0.31300362944602966 and parameters: {'log_learning_rate': -1.5683340164313546, 'log_learning_rate_D': -2.9317851554434573, 'log_learning_rate_D_dagger': -1.5814017967817204, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
res:  tensor(0.3130, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3144, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  217.13491988182068
Memory status after this trial: 
Memory allocated:  2.20068359375
Memory cached:  6.0
--------------------  Trial  148   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3449655921921233, 'log_learning_rate_D': -2.91110344323451, 'log_learning_rate_D_dagger': -1.5545667388341025, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(79.2501, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.8894, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.8838, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3691, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3300, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3073, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.2984, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3079, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.2959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 18:59:57,068] Trial 148 finished with value: 0.3139536678791046 and parameters: {'log_learning_rate': -1.3449655921921233, 'log_learning_rate_D': -2.91110344323451, 'log_learning_rate_D_dagger': -1.5545667388341025, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  215.7464141845703
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  149   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3147973515589617, 'log_learning_rate_D': -2.931224813221373, 'log_learning_rate_D_dagger': -1.5455609149336418, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(17.8072, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.9497, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.7439, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4668, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3522, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:01:42,571] Trial 149 failed with parameters: {'log_learning_rate': -1.3147973515589617, 'log_learning_rate_D': -2.931224813221373, 'log_learning_rate_D_dagger': -1.5455609149336418, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:01:42,571] Trial 149 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  105.23799228668213
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  150   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2733987943870502, 'log_learning_rate_D': -3.0813570488655944, 'log_learning_rate_D_dagger': -1.5509635351837368, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(28.7917, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(2.3889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.6479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4907, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.4057, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3805, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3596, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3408, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:04:31,187] Trial 150 failed with parameters: {'log_learning_rate': -1.2733987943870502, 'log_learning_rate_D': -3.0813570488655944, 'log_learning_rate_D_dagger': -1.5509635351837368, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:04:31,187] Trial 150 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  168.34923696517944
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  8.0
--------------------  Trial  151   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3757590935847603, 'log_learning_rate_D': -2.9385033896936408, 'log_learning_rate_D_dagger': -1.6144357760716168, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(45.8438, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.9956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.7136, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4477, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3805, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3751, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3710, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3668, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3625, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:08:09,809] Trial 151 finished with value: 0.3705059587955475 and parameters: {'log_learning_rate': -1.3757590935847603, 'log_learning_rate_D': -2.9385033896936408, 'log_learning_rate_D_dagger': -1.6144357760716168, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  218.3576581478119
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  152   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2869235063980204, 'log_learning_rate_D': -2.9242542230417055, 'log_learning_rate_D_dagger': -1.9007348329354776, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(42.4392, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  10 training error:  tensor(4.3385, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  20 training error:  tensor(6.8813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  10.0
	 epoch  30 training error:  tensor(3.5850, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  40 training error:  tensor(2.4956, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  50 training error:  tensor(1.8386, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  60 training error:  tensor(1.5647, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  10.0
	 epoch  70 training error:  tensor(1.2811, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  80 training error:  tensor(1.0563, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.8564, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
[I 2024-03-12 19:10:09,046] Trial 152 finished with value: 0.7373525500297546 and parameters: {'log_learning_rate': -1.2869235063980204, 'log_learning_rate_D': -2.9242542230417055, 'log_learning_rate_D_dagger': -1.9007348329354776, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  118.96786761283875
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  10.0
--------------------  Trial  153   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5641760270699798, 'log_learning_rate_D': -2.788732561520372, 'log_learning_rate_D_dagger': -1.5461544900693305, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(78.3575, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(5.7746, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(2.1605, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5123, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.5304, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4735, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4507, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4453, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.5075, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4226, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:13:45,927] Trial 153 finished with value: 0.43948110938072205 and parameters: {'log_learning_rate': -1.5641760270699798, 'log_learning_rate_D': -2.788732561520372, 'log_learning_rate_D_dagger': -1.5461544900693305, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  216.61783838272095
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  154   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6931241794314114, 'log_learning_rate_D': -3.04873395082805, 'log_learning_rate_D_dagger': -1.4368626323092542, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(209.0069, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(12.0498, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.9848, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:14:51,849] Trial 154 failed with parameters: {'log_learning_rate': -1.6931241794314114, 'log_learning_rate_D': -3.04873395082805, 'log_learning_rate_D_dagger': -1.4368626323092542, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:14:51,850] Trial 154 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  65.67297410964966
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  155   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5354325069453387, 'log_learning_rate_D': -3.0545662948131747, 'log_learning_rate_D_dagger': -1.429671576767179, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(30.0223, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.6502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.4233, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3888, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3802, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3682, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3623, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3552, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:18:30,998] Trial 155 finished with value: 0.3609250485897064 and parameters: {'log_learning_rate': -1.5354325069453387, 'log_learning_rate_D': -3.0545662948131747, 'log_learning_rate_D_dagger': -1.429671576767179, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  218.86383819580078
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  156   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.7064236140883344, 'log_learning_rate_D': -2.8806347048288528, 'log_learning_rate_D_dagger': -2.0045138771848925, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(73.8122, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(20.8060, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(3.8454, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.1347, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.6847, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.5342, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5178, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.5056, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4939, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4840, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:22:09,365] Trial 156 finished with value: 0.47616568207740784 and parameters: {'log_learning_rate': -1.7064236140883344, 'log_learning_rate_D': -2.8806347048288528, 'log_learning_rate_D_dagger': -2.0045138771848925, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  218.10795664787292
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  157   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1117101440748125, 'log_learning_rate_D': -3.058940662004087, 'log_learning_rate_D_dagger': -1.770601516451892, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(157.6755, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(42.2702, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(4.2076, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.4437, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.6292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4878, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4689, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4524, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4393, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4276, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:25:48,495] Trial 157 finished with value: 0.4470013678073883 and parameters: {'log_learning_rate': -1.1117101440748125, 'log_learning_rate_D': -3.058940662004087, 'log_learning_rate_D_dagger': -1.770601516451892, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  218.86122274398804
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  158   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4837329914207391, 'log_learning_rate_D': -3.0556172142401663, 'log_learning_rate_D_dagger': -1.5615734982530616, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(233.5613, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  10 training error:  tensor(59.4950, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  20 training error:  tensor(19.9502, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  10.0
	 epoch  30 training error:  tensor(8.1889, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  40 training error:  tensor(5.6351, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  50 training error:  tensor(2.5268, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  60 training error:  tensor(1.4999, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  10.0
	 epoch  70 training error:  tensor(1.0197, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.7479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.7012, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
[I 2024-03-12 19:27:47,281] Trial 158 finished with value: 0.6660181879997253 and parameters: {'log_learning_rate': -1.4837329914207391, 'log_learning_rate_D': -3.0556172142401663, 'log_learning_rate_D_dagger': -1.5615734982530616, 'training_batch_size': 9, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  118.52751135826111
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  10.0
--------------------  Trial  159   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.429626029609902, 'log_learning_rate_D': -3.22965129810045, 'log_learning_rate_D_dagger': -1.4545532232829312, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(13.2599, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.7948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.4677, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4143, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3615, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3478, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3330, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3189, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3114, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:31:25,362] Trial 159 finished with value: 0.337595134973526 and parameters: {'log_learning_rate': -1.429626029609902, 'log_learning_rate_D': -3.22965129810045, 'log_learning_rate_D_dagger': -1.4545532232829312, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  217.7765839099884
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  160   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.406067492692514, 'log_learning_rate_D': -3.2391025536482214, 'log_learning_rate_D_dagger': -1.451949346418319, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(23.9202, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.8578, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(0.5440, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3435, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3141, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.2974, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.2948, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.2926, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.2940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.2931, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:35:04,373] Trial 160 finished with value: 0.3184262216091156 and parameters: {'log_learning_rate': -1.406067492692514, 'log_learning_rate_D': -3.2391025536482214, 'log_learning_rate_D_dagger': -1.451949346418319, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  218.7293345928192
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  161   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6092645269254358, 'log_learning_rate_D': -3.1947301677584927, 'log_learning_rate_D_dagger': -1.4403680418060374, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(85.4813, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.8293, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.5241, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5008, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.3683, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3539, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3436, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3161, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:38:42,307] Trial 161 finished with value: 0.331807941198349 and parameters: {'log_learning_rate': -1.6092645269254358, 'log_learning_rate_D': -3.1947301677584927, 'log_learning_rate_D_dagger': -1.4403680418060374, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  217.65319633483887
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  162   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4626461382767368, 'log_learning_rate_D': -3.1154566006872417, 'log_learning_rate_D_dagger': -1.290489707334034, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(216.4305, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(9.4765, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.7908, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4421, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.4208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4063, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3940, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3804, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3648, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3476, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[W 2024-03-12 19:42:16,497] Trial 162 failed with parameters: {'log_learning_rate': -1.4626461382767368, 'log_learning_rate_D': -3.1154566006872417, 'log_learning_rate_D_dagger': -1.290489707334034, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:42:16,497] Trial 162 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  213.90232348442078
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  163   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4296714482812578, 'log_learning_rate_D': -3.270871212969435, 'log_learning_rate_D_dagger': -1.3007474457757482, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(248.6213, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.5295, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(2.3870, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.4500, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.5966, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.4619, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4007, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3913, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3863, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:45:54,845] Trial 163 finished with value: 0.4045356810092926 and parameters: {'log_learning_rate': -1.4296714482812578, 'log_learning_rate_D': -3.270871212969435, 'log_learning_rate_D_dagger': -1.3007474457757482, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 147 with value: 0.31300362944602966.
Time for this trial:  218.0672950744629
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  164   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5597658938872063, 'log_learning_rate_D': -3.2188513080880936, 'log_learning_rate_D_dagger': -1.4390290222149564, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(67.7066, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(6.0657, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  20 training error:  tensor(1.2356, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4429, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.4284, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3733, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.3419, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.3217, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.3022, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  8.0
[I 2024-03-12 19:49:33,410] Trial 164 finished with value: 0.31252455711364746 and parameters: {'log_learning_rate': -1.5597658938872063, 'log_learning_rate_D': -3.2188513080880936, 'log_learning_rate_D_dagger': -1.4390290222149564, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
res:  tensor(0.3125, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3130, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  218.3098373413086
Memory status after this trial: 
Memory allocated:  2.20068359375
Memory cached:  6.0
--------------------  Trial  165   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.568870971702689, 'log_learning_rate_D': -3.230673398729352, 'log_learning_rate_D_dagger': -1.4650612222322634, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(85.6886, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:49:47,590] Trial 165 failed with parameters: {'log_learning_rate': -1.568870971702689, 'log_learning_rate_D': -3.230673398729352, 'log_learning_rate_D_dagger': -1.4650612222322634, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:49:47,590] Trial 165 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  13.9500093460083
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  166   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5784978658783082, 'log_learning_rate_D': -3.1033728334392965, 'log_learning_rate_D_dagger': -1.4477699325036488, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 19:49:50,973] Trial 166 failed with parameters: {'log_learning_rate': -1.5784978658783082, 'log_learning_rate_D': -3.1033728334392965, 'log_learning_rate_D_dagger': -1.4477699325036488, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:49:50,974] Trial 166 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  3.107524871826172
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  167   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5348084852631934, 'log_learning_rate_D': -3.222616345076259, 'log_learning_rate_D_dagger': -1.0909069001041376, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(24.7799, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.3027, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4791, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4301, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3922, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3510, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3082, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:52:11,241] Trial 167 failed with parameters: {'log_learning_rate': -1.5348084852631934, 'log_learning_rate_D': -3.222616345076259, 'log_learning_rate_D_dagger': -1.0909069001041376, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:52:11,241] Trial 167 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  140.00911164283752
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  168   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5898102996430887, 'log_learning_rate_D': -3.2243363070145175, 'log_learning_rate_D_dagger': -1.42271194014613, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(258.9325, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(10.3185, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(2.0740, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.8762, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4637, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4290, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4140, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3955, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4004, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 19:55:50,068] Trial 168 finished with value: 0.3967897593975067 and parameters: {'log_learning_rate': -1.5898102996430887, 'log_learning_rate_D': -3.2243363070145175, 'log_learning_rate_D_dagger': -1.42271194014613, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
Time for this trial:  218.55471515655518
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  169   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.400286228828243, 'log_learning_rate_D': -3.111610990376894, 'log_learning_rate_D_dagger': -1.3106985956971244, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(107.8232, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.5382, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.6591, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4519, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4303, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4099, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3896, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3693, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3472, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:58:57,746] Trial 169 failed with parameters: {'log_learning_rate': -1.400286228828243, 'log_learning_rate_D': -3.111610990376894, 'log_learning_rate_D_dagger': -1.3106985956971244, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:58:57,747] Trial 169 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  187.4298152923584
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  170   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4050424160083734, 'log_learning_rate_D': -3.123581542952417, 'log_learning_rate_D_dagger': -1.0776353768655524, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(68.0697, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 19:59:20,513] Trial 170 failed with parameters: {'log_learning_rate': -1.4050424160083734, 'log_learning_rate_D': -3.123581542952417, 'log_learning_rate_D_dagger': -1.0776353768655524, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 19:59:20,514] Trial 170 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  22.51278328895569
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  171   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3896258099360521, 'log_learning_rate_D': -3.1112876210793523, 'log_learning_rate_D_dagger': -1.6851037764708368, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(43.8324, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(4.4869, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.4359, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.6716, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4630, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3988, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3809, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3645, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3484, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3338, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 20:02:55,906] Trial 171 failed with parameters: {'log_learning_rate': -1.3896258099360521, 'log_learning_rate_D': -3.1112876210793523, 'log_learning_rate_D_dagger': -1.6851037764708368, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:02:55,907] Trial 171 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  215.13488745689392
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  172   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4269668841780871, 'log_learning_rate_D': -3.066474091054038, 'log_learning_rate_D_dagger': -1.3033246257037105, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(21.3232, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.0856, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.5085, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3977, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3520, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3156, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3101, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.2958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.2959, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.2937, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 20:06:34,814] Trial 172 finished with value: 0.3147816061973572 and parameters: {'log_learning_rate': -1.4269668841780871, 'log_learning_rate_D': -3.066474091054038, 'log_learning_rate_D_dagger': -1.3033246257037105, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
Time for this trial:  218.63544082641602
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  173   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5362395306745982, 'log_learning_rate_D': -3.082010888721748, 'log_learning_rate_D_dagger': -1.0777876984570087, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(17.0875, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[W 2024-03-12 20:06:51,175] Trial 173 failed with parameters: {'log_learning_rate': -1.5362395306745982, 'log_learning_rate_D': -3.082010888721748, 'log_learning_rate_D_dagger': -1.0777876984570087, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:06:51,175] Trial 173 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  16.083189725875854
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  174   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.6564296399680996, 'log_learning_rate_D': -3.1113038890248013, 'log_learning_rate_D_dagger': -1.2984732095608085, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(162.2157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(6.7249, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.1617, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.5536, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.5053, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4833, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.4654, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.4510, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.4392, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.4292, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 20:10:29,878] Trial 174 finished with value: 0.3889490067958832 and parameters: {'log_learning_rate': -1.6564296399680996, 'log_learning_rate_D': -3.1113038890248013, 'log_learning_rate_D_dagger': -1.2984732095608085, 'training_batch_size': 8, 'training_p': 3, 'p_order_W': 5, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
Time for this trial:  218.3875069618225
Memory status after this trial: 
Memory allocated:  4.1298828125
Memory cached:  6.0
--------------------  Trial  175   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1978538754633226, 'log_learning_rate_D': -2.994237986789355, 'log_learning_rate_D_dagger': -1.195701624510315, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:10:33,035] Trial 175 failed with parameters: {'log_learning_rate': -1.1978538754633226, 'log_learning_rate_D': -2.994237986789355, 'log_learning_rate_D_dagger': -1.195701624510315, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:10:33,036] Trial 175 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.9131011962890625
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  176   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5487377557994915, 'log_learning_rate_D': -2.692297627092372, 'log_learning_rate_D_dagger': -1.0878047628000962, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(118.1479, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(8.0856, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.6859, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4891, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3120, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3116, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3110, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3125, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3128, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 20:13:58,743] Trial 176 finished with value: 0.3356712758541107 and parameters: {'log_learning_rate': -1.5487377557994915, 'log_learning_rate_D': -2.692297627092372, 'log_learning_rate_D_dagger': -1.0878047628000962, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
Time for this trial:  205.41746711730957
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  177   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.5194606528951502, 'log_learning_rate_D': -2.721449767891966, 'log_learning_rate_D_dagger': -1.098534476460571, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(109.3660, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  10 training error:  tensor(8.4016, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  20 training error:  tensor(8.7934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  30 training error:  tensor(0.9204, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  40 training error:  tensor(0.8517, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  50 training error:  tensor(0.7423, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  60 training error:  tensor(0.5335, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  70 training error:  tensor(0.4555, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  80 training error:  tensor(0.4216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
	 epoch  90 training error:  tensor(0.4112, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.32861328125
Memory cached:  8.0
[I 2024-03-12 20:15:50,899] Trial 177 finished with value: 0.40742501616477966 and parameters: {'log_learning_rate': -1.5194606528951502, 'log_learning_rate_D': -2.721449767891966, 'log_learning_rate_D_dagger': -1.098534476460571, 'training_batch_size': 9, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
Time for this trial:  111.85593461990356
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  178   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.217232177738762, 'log_learning_rate_D': -2.9910880949732865, 'log_learning_rate_D_dagger': -1.192655308372676, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:15:54,089] Trial 178 failed with parameters: {'log_learning_rate': -1.217232177738762, 'log_learning_rate_D': -2.9910880949732865, 'log_learning_rate_D_dagger': -1.192655308372676, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:15:54,089] Trial 178 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8988468647003174
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  179   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3860643987108312, 'log_learning_rate_D': -2.9930925975490124, 'log_learning_rate_D_dagger': -1.1946406467998483, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(7.8876, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(0.7904, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3826, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3518, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3282, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3130, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3208, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3231, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3187, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 20:19:19,649] Trial 179 finished with value: 0.3298530578613281 and parameters: {'log_learning_rate': -1.3860643987108312, 'log_learning_rate_D': -2.9930925975490124, 'log_learning_rate_D_dagger': -1.1946406467998483, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 164 with value: 0.31252455711364746.
Time for this trial:  205.26191878318787
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  180   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2150837883853027, 'log_learning_rate_D': -2.985980383380731, 'log_learning_rate_D_dagger': -1.073539540670195, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:19:22,863] Trial 180 failed with parameters: {'log_learning_rate': -1.2150837883853027, 'log_learning_rate_D': -2.985980383380731, 'log_learning_rate_D_dagger': -1.073539540670195, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:19:22,863] Trial 180 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.928276777267456
Memory status after this trial: 
Memory allocated:  3.935546875
Memory cached:  6.0
--------------------  Trial  181   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3824057517542583, 'log_learning_rate_D': -2.807100737949289, 'log_learning_rate_D_dagger': -1.199586301039924, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(78.5140, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  10 training error:  tensor(4.4820, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.0815, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4359, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3938, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3640, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3380, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3134, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3008, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.2934, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.23095703125
Memory cached:  6.0
[I 2024-03-12 20:22:43,914] Trial 181 finished with value: 0.3123883605003357 and parameters: {'log_learning_rate': -1.3824057517542583, 'log_learning_rate_D': -2.807100737949289, 'log_learning_rate_D_dagger': -1.199586301039924, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 181 with value: 0.3123883605003357.
res:  tensor(0.3124, grad_fn=<ToCopyBackward0>)
self.bestValue:  tensor(0.3125, grad_fn=<ToCopyBackward0>)
Save this model!
Time for this trial:  200.79318165779114
Memory status after this trial: 
Memory allocated:  2.00634765625
Memory cached:  6.0
--------------------  Trial  182   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3623489767787669, 'log_learning_rate_D': -2.6453994112896027, 'log_learning_rate_D_dagger': -1.1824991106214953, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(46.1379, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.5986, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4255, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.3690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3320, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3064, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.2953, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.2969, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.2973, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.2985, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 20:26:08,040] Trial 182 finished with value: 0.31342440843582153 and parameters: {'log_learning_rate': -1.3623489767787669, 'log_learning_rate_D': -2.6453994112896027, 'log_learning_rate_D_dagger': -1.1824991106214953, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 181 with value: 0.3123883605003357.
Time for this trial:  203.87458872795105
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  183   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.229013928640919, 'log_learning_rate_D': -2.8044984256422354, 'log_learning_rate_D_dagger': -1.1929188339572567, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:11,146] Trial 183 failed with parameters: {'log_learning_rate': -1.229013928640919, 'log_learning_rate_D': -2.8044984256422354, 'log_learning_rate_D_dagger': -1.1929188339572567, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:11,146] Trial 183 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8449108600616455
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  184   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2010923874519512, 'log_learning_rate_D': -2.604394170557451, 'log_learning_rate_D_dagger': -1.0560229271615116, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:14,244] Trial 184 failed with parameters: {'log_learning_rate': -1.2010923874519512, 'log_learning_rate_D': -2.604394170557451, 'log_learning_rate_D_dagger': -1.0560229271615116, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:14,244] Trial 184 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8602731227874756
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  185   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4272454476271625, 'log_learning_rate_D': -2.6263371920451952, 'log_learning_rate_D_dagger': -1.1666711530898617, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:17,446] Trial 185 failed with parameters: {'log_learning_rate': -1.4272454476271625, 'log_learning_rate_D': -2.6263371920451952, 'log_learning_rate_D_dagger': -1.1666711530898617, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:17,447] Trial 185 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.9352822303771973
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  186   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2393479923101933, 'log_learning_rate_D': -2.6471646019564514, 'log_learning_rate_D_dagger': -1.0669276987538343, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:20,610] Trial 186 failed with parameters: {'log_learning_rate': -1.2393479923101933, 'log_learning_rate_D': -2.6471646019564514, 'log_learning_rate_D_dagger': -1.0669276987538343, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:20,611] Trial 186 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.907998561859131
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  187   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4014932102579485, 'log_learning_rate_D': -2.6371785984128704, 'log_learning_rate_D_dagger': -1.078131173057713, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:23,796] Trial 187 failed with parameters: {'log_learning_rate': -1.4014932102579485, 'log_learning_rate_D': -2.6371785984128704, 'log_learning_rate_D_dagger': -1.078131173057713, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:23,796] Trial 187 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.9048705101013184
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  188   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.220021675692017, 'log_learning_rate_D': -2.646802687267637, 'log_learning_rate_D_dagger': -1.059680100068211, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:26,969] Trial 188 failed with parameters: {'log_learning_rate': -1.220021675692017, 'log_learning_rate_D': -2.646802687267637, 'log_learning_rate_D_dagger': -1.059680100068211, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:26,969] Trial 188 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8821306228637695
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  189   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2350063993813782, 'log_learning_rate_D': -2.6481310808171363, 'log_learning_rate_D_dagger': -1.0669743463595778, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:26:30,068] Trial 189 failed with parameters: {'log_learning_rate': -1.2350063993813782, 'log_learning_rate_D': -2.6481310808171363, 'log_learning_rate_D_dagger': -1.0669743463595778, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:26:30,068] Trial 189 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8550822734832764
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  190   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.1718165794347342, 'log_learning_rate_D': -2.6518996105830777, 'log_learning_rate_D_dagger': -1.086270865502651, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(53.8708, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.3763, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4350, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4000, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3388, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3137, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3189, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3181, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3189, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 20:29:54,784] Trial 190 finished with value: 0.3475225567817688 and parameters: {'log_learning_rate': -1.1718165794347342, 'log_learning_rate_D': -2.6518996105830777, 'log_learning_rate_D_dagger': -1.086270865502651, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 181 with value: 0.3123883605003357.
Time for this trial:  204.46019864082336
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  191   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2295285066446906, 'log_learning_rate_D': -2.8058339334339504, 'log_learning_rate_D_dagger': -1.1797867063763303, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:29:57,947] Trial 191 failed with parameters: {'log_learning_rate': -1.2295285066446906, 'log_learning_rate_D': -2.8058339334339504, 'log_learning_rate_D_dagger': -1.1797867063763303, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:29:57,948] Trial 191 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8900725841522217
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  192   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2332484000704762, 'log_learning_rate_D': -2.7966680221026214, 'log_learning_rate_D_dagger': -1.1744726049310354, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:30:01,088] Trial 192 failed with parameters: {'log_learning_rate': -1.2332484000704762, 'log_learning_rate_D': -2.7966680221026214, 'log_learning_rate_D_dagger': -1.1744726049310354, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:30:01,088] Trial 192 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8551950454711914
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  193   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2406204168936883, 'log_learning_rate_D': -2.8086368637006864, 'log_learning_rate_D_dagger': -1.1812869587475952, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:30:04,187] Trial 193 failed with parameters: {'log_learning_rate': -1.2406204168936883, 'log_learning_rate_D': -2.8086368637006864, 'log_learning_rate_D_dagger': -1.1812869587475952, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:30:04,187] Trial 193 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8412599563598633
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  194   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.4070004390436182, 'log_learning_rate_D': -2.8268660952710785, 'log_learning_rate_D_dagger': -1.1826036918062992, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(27.4769, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.3219, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4678, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4172, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.3472, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3138, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3029, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.2964, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.2943, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.2930, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 20:33:28,920] Trial 194 finished with value: 0.3131937086582184 and parameters: {'log_learning_rate': -1.4070004390436182, 'log_learning_rate_D': -2.8268660952710785, 'log_learning_rate_D_dagger': -1.1826036918062992, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 181 with value: 0.3123883605003357.
Time for this trial:  204.4515724182129
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  195   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2260446822709061, 'log_learning_rate_D': -2.8480738232908918, 'log_learning_rate_D_dagger': -1.2021706915880517, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:33:32,041] Trial 195 failed with parameters: {'log_learning_rate': -1.2260446822709061, 'log_learning_rate_D': -2.8480738232908918, 'log_learning_rate_D_dagger': -1.2021706915880517, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:33:32,041] Trial 195 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.852928638458252
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  196   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.2177453983897666, 'log_learning_rate_D': -2.782497877221909, 'log_learning_rate_D_dagger': -1.1864932564802486, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(35.1425, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(1.7958, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(0.4514, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(0.4404, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.4049, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.3881, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3785, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3690, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3630, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3586, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 20:36:57,496] Trial 196 finished with value: 0.360525906085968 and parameters: {'log_learning_rate': -1.2177453983897666, 'log_learning_rate_D': -2.782497877221909, 'log_learning_rate_D_dagger': -1.1864932564802486, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 181 with value: 0.3123883605003357.
Time for this trial:  205.2033064365387
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  197   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.392635213341677, 'log_learning_rate_D': -2.8640531967983356, 'log_learning_rate_D_dagger': -1.1997883759665602, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:37:00,596] Trial 197 failed with parameters: {'log_learning_rate': -1.392635213341677, 'log_learning_rate_D': -2.8640531967983356, 'log_learning_rate_D_dagger': -1.1997883759665602, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:37:00,597] Trial 197 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8557631969451904
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  198   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3867748122121764, 'log_learning_rate_D': -2.8428525300966063, 'log_learning_rate_D_dagger': -1.1835299721528034, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:37:03,761] Trial 198 failed with parameters: {'log_learning_rate': -1.3867748122121764, 'log_learning_rate_D': -2.8428525300966063, 'log_learning_rate_D_dagger': -1.1835299721528034, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:37:03,762] Trial 198 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8871397972106934
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  199   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.378311143432198, 'log_learning_rate_D': -2.8443420772372052, 'log_learning_rate_D_dagger': -1.2454570546561072, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

[W 2024-03-12 20:37:06,870] Trial 199 failed with parameters: {'log_learning_rate': -1.378311143432198, 'log_learning_rate_D': -2.8443420772372052, 'log_learning_rate_D_dagger': -1.2454570546561072, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3} because of the following error: The value nan is not acceptable.
[W 2024-03-12 20:37:06,871] Trial 199 failed with value tensor(nan, grad_fn=<ToCopyBackward0>).
Time for this trial:  2.8654232025146484
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
--------------------  Trial  200   --------------------
Start timing: 
Parameters: 
{'log_learning_rate': -1.3517288188273702, 'log_learning_rate_D': -2.588033717965602, 'log_learning_rate_D_dagger': -1.1925649550223656, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}

	 epoch  0 training error:  tensor(190.5947, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  10 training error:  tensor(3.9290, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  20 training error:  tensor(1.9598, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  30 training error:  tensor(1.0780, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  40 training error:  tensor(0.7142, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  50 training error:  tensor(0.4357, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  60 training error:  tensor(0.3845, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  70 training error:  tensor(0.3595, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  80 training error:  tensor(0.3541, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
	 epoch  90 training error:  tensor(0.3424, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2.03662109375
Memory cached:  6.0
[I 2024-03-12 20:40:31,836] Trial 200 finished with value: 0.3518691658973694 and parameters: {'log_learning_rate': -1.3517288188273702, 'log_learning_rate_D': -2.588033717965602, 'log_learning_rate_D_dagger': -1.1925649550223656, 'training_batch_size': 8, 'training_p': 2, 'p_order_W': 4, 'p_order_D': 4, 'p_order_D_dagger': 3}. Best is trial 181 with value: 0.3123883605003357.
Time for this trial:  204.69808530807495
Memory status after this trial: 
Memory allocated:  3.7412109375
Memory cached:  6.0
