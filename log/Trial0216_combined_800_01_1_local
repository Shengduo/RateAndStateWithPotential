/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[I 2024-03-01 16:41:30,553] Using an existing study with name 'my_study' instead of creating a new one.
Cuda is available:  True
Device is:  cuda
Memory allocated:  0.0
Memory cached:  0.0
Data file:  ./data/Trial0216_combined_800.pt
Vs.shape:  torch.Size([800, 100])
thetas.shape:  torch.Size([800, 100])
fs.shape:  torch.Size([800, 100])
ts.shape:  torch.Size([800, 100])
Xs.shape:  torch.Size([800, 100])
No pruned database has been founded.
--------------------  Trial  0   --------------------
Start timing: 
Parameters: 
{'W_layers': 2, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 7, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 4, 'log_learning_rate': -1.8502038721062348, 'log_learning_rate_D': -4.127253564378144, 'log_learning_rate_D_dagger': -2.9056061961809307, 'training_batch_size': 9, 'training_p': 6}
	 epoch  0 training error:  tensor(9.5733, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  118.0
	 epoch  10 training error:  tensor(1.6746, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  126.0
	 epoch  20 training error:  tensor(0.4572, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  122.0
	 epoch  30 training error:  tensor(0.3833, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  110.0
	 epoch  40 training error:  tensor(0.3515, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  116.0
	 epoch  50 training error:  tensor(0.3544, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  126.0
	 epoch  60 training error:  tensor(0.3554, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  124.0
	 epoch  70 training error:  tensor(0.3506, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  118.0
	 epoch  80 training error:  tensor(0.3466, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  122.0
	 epoch  90 training error:  tensor(0.3464, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  21.45556640625
Memory cached:  126.0
[I 2024-03-01 16:49:13,698] Trial 0 finished with value: 0.2139398604631424 and parameters: {'W_layers': 2, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 7, 'D_layers': 6, 'D_layer_units_exponent_0': 10, 'D_layer_units_exponent_1': 4, 'D_layer_units_exponent_2': 6, 'D_layer_units_exponent_3': 10, 'D_layer_units_exponent_4': 4, 'D_layer_units_exponent_5': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 6, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 10, 'D_dagger_layer_units_exponent_3': 4, 'log_learning_rate': -1.8502038721062348, 'log_learning_rate_D': -4.127253564378144, 'log_learning_rate_D_dagger': -2.9056061961809307, 'training_batch_size': 9, 'training_p': 6}. Best is trial 0 with value: 0.2139398604631424.
res:  tensor(0.2139, grad_fn=<ToCopyBackward0>)
self.bestValue:  10000000.0
Save this model!
Time for this trial:  462.8534276485443
Memory status after this trial: 
Memory allocated:  2334.318359375
Memory cached:  2360.0
--------------------  Trial  1   --------------------
Start timing: 
Parameters: 
{'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'log_learning_rate': -1.655178639143462, 'log_learning_rate_D': -4.297844924877448, 'log_learning_rate_D_dagger': -1.4340703711091916, 'training_batch_size': 12, 'training_p': 4}
	 epoch  0 training error:  tensor(1.1068, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2366.0
	 epoch  10 training error:  tensor(10.1157, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  20 training error:  tensor(1.0216, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  30 training error:  tensor(0.6395, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  40 training error:  tensor(0.3600, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  50 training error:  tensor(0.3306, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  60 training error:  tensor(0.3468, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  70 training error:  tensor(0.3355, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  80 training error:  tensor(0.3269, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
	 epoch  90 training error:  tensor(0.3107, grad_fn=<ToCopyBackward0>)
Memory status after this epoch: 
Memory allocated:  2342.32373046875
Memory cached:  2370.0
[I 2024-03-01 16:53:24,484] Trial 1 finished with value: 0.3455421030521393 and parameters: {'W_layers': 4, 'W_layer_units_exponent_0': 9, 'W_layer_units_exponent_1': 8, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 5, 'D_layers': 2, 'D_layer_units_exponent_0': 7, 'D_layer_units_exponent_1': 6, 'D_dagger_layers': 4, 'D_dagger_layer_units_exponent_0': 5, 'D_dagger_layer_units_exponent_1': 10, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'log_learning_rate': -1.655178639143462, 'log_learning_rate_D': -4.297844924877448, 'log_learning_rate_D_dagger': -1.4340703711091916, 'training_batch_size': 12, 'training_p': 4}. Best is trial 0 with value: 0.2139398604631424.
Time for this trial:  250.51350688934326
Memory status after this trial: 
Memory allocated:  3417.2509765625
Memory cached:  3448.0
--------------------  Trial  2   --------------------
Start timing: 
Parameters: 
{'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 5, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -3.801959585501915, 'log_learning_rate_D': -2.932602187927802, 'log_learning_rate_D_dagger': -1.3166528816826912, 'training_batch_size': 12, 'training_p': 8}
[W 2024-03-01 16:53:26,833] Trial 2 failed with parameters: {'W_layers': 6, 'W_layer_units_exponent_0': 10, 'W_layer_units_exponent_1': 10, 'W_layer_units_exponent_2': 4, 'W_layer_units_exponent_3': 10, 'W_layer_units_exponent_4': 9, 'W_layer_units_exponent_5': 7, 'D_layers': 5, 'D_layer_units_exponent_0': 9, 'D_layer_units_exponent_1': 6, 'D_layer_units_exponent_2': 8, 'D_layer_units_exponent_3': 5, 'D_layer_units_exponent_4': 5, 'D_dagger_layers': 6, 'D_dagger_layer_units_exponent_0': 7, 'D_dagger_layer_units_exponent_1': 8, 'D_dagger_layer_units_exponent_2': 8, 'D_dagger_layer_units_exponent_3': 8, 'D_dagger_layer_units_exponent_4': 8, 'D_dagger_layer_units_exponent_5': 7, 'log_learning_rate': -3.801959585501915, 'log_learning_rate_D': -2.932602187927802, 'log_learning_rate_D_dagger': -1.3166528816826912, 'training_batch_size': 12, 'training_p': 8} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 10.38 GiB already allocated; 69.44 MiB free; 10.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/home/shengduo/RateAndStateWithPotential/TuneDimXi_logV_WDsep_deltaTSqed_combinedSet_local.py", line 227, in objective
    avg_training_loss = train1Epoch(trainDataLoader, Loss, myWD, params['training_p'], 0.)
  File "/home/shengduo/RateAndStateWithPotential/FrictionNNModels.py", line 358, in train1Epoch
    myPot.calf(Xs, XDots, ts)
  File "/home/shengduo/RateAndStateWithPotential/FrictionNNModels.py", line 120, in calf
    dWdX = torch.autograd.grad(outputs=W, inputs=X_W, create_graph=True)[0]
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 276, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 10.38 GiB already allocated; 69.44 MiB free; 10.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2024-03-01 16:53:26,834] Trial 2 failed with value None.
Traceback (most recent call last):
  File "/home/shengduo/RateAndStateWithPotential/TuneDimXi_logV_WDsep_deltaTSqed_combinedSet_local.py", line 308, in <module>
    this_study.optimize(myOpt.objective, n_trials=10)
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/home/shengduo/RateAndStateWithPotential/TuneDimXi_logV_WDsep_deltaTSqed_combinedSet_local.py", line 227, in objective
    avg_training_loss = train1Epoch(trainDataLoader, Loss, myWD, params['training_p'], 0.)
  File "/home/shengduo/RateAndStateWithPotential/FrictionNNModels.py", line 358, in train1Epoch
    myPot.calf(Xs, XDots, ts)
  File "/home/shengduo/RateAndStateWithPotential/FrictionNNModels.py", line 120, in calf
    dWdX = torch.autograd.grad(outputs=W, inputs=X_W, create_graph=True)[0]
  File "/home/shengduo/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 276, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 10.38 GiB already allocated; 69.44 MiB free; 10.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
